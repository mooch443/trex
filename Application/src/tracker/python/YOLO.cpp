#include "YOLO.h"
#include <misc/PixelTree.h>
#include <misc/PythonWrapper.h>
#include <grabber/misc/default_config.h>
#include <video/Video.h>
#include <misc/Timer.h>
#include <misc/ThreadPool.h>
#include <misc/TrackingSettings.h>
#include <python/GPURecognition.h>
#include <gui/GuiTypes.h>

namespace track {

using namespace cmn;

struct AcceptanceSettings {
    Float2_t sqcm;
    SizeFilters min_max;
    
    bool is_acceptable(const pv::Blob& blob) const {
        if(min_max.empty())
            return true;
        return min_max.in_range_of_one(blob.num_pixels() * sqcm);
    }
    
    static AcceptanceSettings Make() {
        const auto cm_per_pixel = SETTING(cm_per_pixel).value<Settings::cm_per_pixel_t>();
        return AcceptanceSettings{
            .sqcm = SQR(cm_per_pixel),
            .min_max = SETTING(detect_size_filter).value<SizeFilters>()
        };
    }
};

std::mutex running_mutex;
std::shared_future<void> running_prediction;
std::promise<void> running_promise;

std::mutex init_mutex;
std::future<void> init_future;

std::atomic<bool> yolo_initialized{false};
std::atomic<double> _network_fps{0.0};
std::atomic<size_t> _network_samples{0u};

std::mutex transfer_done_mutex;
std::future<void> transferred_done;

std::vector<detect::ModelConfig> _loaded_models;
std::unique_ptr<GenericThreadPool> _pool;

struct YOLO::Data {
    std::atomic<bool> _background_required;
    std::atomic<bool> _background_set;
    
    Data() {
        reset();
    }
    void reset() {
        _background_required = GlobalSettings::has("track_background_subtraction") ? SETTING(track_background_subtraction).value<bool>() : false;
        _background_set = false;
    }
    
    bool has_background() const {
        return not _background_required.load() || _background_set.load();
    }
    void set_background(const Image::Ptr& background) {
        _background_set = background != nullptr;
    }
};

YOLO::Data& YOLO::data() {
    static Data _data;
    return _data;
}

void YOLO::set_background(const Image::Ptr &image) {
    data().set_background(image);
    if(data().has_background())
        Detection::manager().set_paused(false);
}

void YOLO::reinit(ModuleProxy& proxy) {
    proxy.set_variable("model_type", detect::detection_type().toStr());
    
    if(SETTING(detect_model).value<file::Path>().empty()) {
        Print("You can provide a model for object detection using the command-line argument -m <path>. Otherwise, we will assume YOLOv8n-pose");
        SETTING(detect_model) = file::Path("yolov8n-pose");
    }

    using namespace track::detect;
    _loaded_models.clear();
    data().reset();

    // caching here since it can be modified above
    auto path = SETTING(detect_model).value<file::Path>();
    if(detect::yolo::valid_model(path)) {
        if(not path.has_extension()) {
            path = path.add_extension("pt"); // pytorch model
        }
        
        _loaded_models.emplace_back(
            ModelTaskType::detect,
            SETTING(yolo_tracking_enabled).value<bool>(),
            path.str(),
            SETTING(detect_resolution).value<DetectResolution>()
        );
        
    } else
        throw U_EXCEPTION("This does not seem like a valid model to use: ", path,". Either we cannot find it, or it is not in a valid format. Expected is a pytorch .pt saved model file (as generated by training e.g. YOLOv8).");

    if(SETTING(region_model).value<file::Path>().exists())
        _loaded_models.emplace_back(
            ModelTaskType::region,
            SETTING(yolo_region_tracking_enabled).value<bool>(), // region models dont have tracking
            SETTING(region_model).value<file::Path>().str(),
            SETTING(region_resolution).value<DetectResolution>()
        );

    if(_loaded_models.empty()) {
        if(not path.empty())
            throw U_EXCEPTION("Cannot find model ", path);
        
        throw U_EXCEPTION("Please provide at least one model to use for segmentation.");
    }
    
    _loaded_models = PythonIntegration::set_models(_loaded_models, proxy.m);
    
    for(auto &config : _loaded_models) {
        if(config.task == ModelTaskType::detect) {
            SETTING(detect_format) = ObjectDetectionFormat_t(config.output_format);
            SETTING(detect_resolution) = config.trained_resolution;
            if(auto detect_classes = SETTING(detect_classes).value<cmn::blob::MaybeObjectClass_t>();
               not detect_classes.has_value()
               || detect_classes->empty())
            {
                Print("// Loading classes from model: ", config.classes);
                SETTING(detect_classes) = cmn::blob::MaybeObjectClass_t{config.classes};
            }
            
            if(config.output_format == ObjectDetectionFormat::poses)
            {
                SETTING(detect_keypoint_format) = config.keypoint_format ? *config.keypoint_format : KeypointFormat{};
            }
            
        } else if(config.task == ModelTaskType::region) {
            SETTING(region_resolution) = config.trained_resolution;
        }
    }
    
    /*if(auto detect_format = SETTING(detect_format).value<ObjectDetectionFormat_t>();
       detect_format == ObjectDetectionFormat::boxes)
    {
        if(SETTING(calculate_posture).value<bool>()) {
            FormatWarning("Disabling posture for now, since pure detection models cannot produce useful posture (everything will be rectangles).");
            SETTING(calculate_posture) = false;
        }
    }*/
}

void YOLO::init() {
    bool expected = false;
    if(yolo_initialized.compare_exchange_strong(expected, true)) {
        data().reset();
        
        _network_fps = _network_samples = 0;
        _pool = std::make_unique<GenericThreadPool>(3, "Yolo");

        std::unique_lock guard(init_mutex);
        if(init_future.valid())
            init_future.get();
        
        Python::schedule([](){
            ModuleProxy proxy{
                ThrowAlways{},
                "bbx_saved_model",
                YOLO::reinit
            };
        }).get();
        
        //! this will block everything + the GUI
        //! unfortunately currently this is the lazy solution
        //! to the model resolution not being up-to-date with
        //! the actual .pt file.
        //init_future.wait();
    }
}

void YOLO::deinit() {
    bool expected = true;
    if(yolo_initialized.compare_exchange_strong(expected, false)) {
        {
            std::unique_lock guard(transfer_done_mutex);
            if(transferred_done.valid())
                transferred_done.get();
        }
        _pool = nullptr;
        
        std::unique_lock guard(running_mutex);
        {
            if(running_prediction.valid()) {
                Print("Still have an active prediction running, waiting...");
                running_prediction.get();
                Print("Got it.");
            }
            running_promise = {};
            running_prediction = {};
        }
        
        if(not Python::python_initialized())
            throw U_EXCEPTION("Please Yolo::deinit before calling Python::deinit().");
        
        Python::schedule([](){
            track::PythonIntegration::unload_module("bbx_saved_model");
            track::PythonIntegration::unload_module("trex_yolo");
            track::PythonIntegration::unload_module("trex_detection_model");
        }).get();
        
        data().reset();
    }
}

// Function to move outlines to the origin
void normalize_points(std::vector<std::vector<Vec2>>& points) {
    float min_x = std::numeric_limits<float>::max();
    float min_y = std::numeric_limits<float>::max();

    for (const auto& outline : points) {
        for (const auto& point : outline) {
            min_x = min(min_x, point.x);
            min_y = min(min_y, point.y);
        }
    }

    for (auto& outline : points) {
        for (auto& point : outline) {
            point.x -= min_x;
            point.y -= min_y;
        }
    }
}

// Function to find bounding box size
std::pair<int, int> find_bounding_box_size(const std::vector<std::vector<Vec2>>& points) {
    float max_x = 0, max_y = 0;
    for (const auto& outline : points) {
        for (const auto& point : outline) {
            max_x = max(max_x, point.x);
            max_y = max(max_y, point.y);
        }
    }
    return { static_cast<int>(max_x) + 1, static_cast<int>(max_y) + 1 };
}

// Function to draw outlines on an OpenCV matrix
template<typename Vector>
void draw_outlines(const std::vector<Vector>& _points, const std::string& title = "Outlines") {
    std::vector<std::vector<Vec2>> copy;
    for(auto &pts : _points) {
        if constexpr(_is_smart_pointer<std::remove_cvref_t<decltype(pts)>>)
            copy.emplace_back(*pts);
        else
            copy.emplace_back(pts);
    }
    
    normalize_points(copy);
    auto size = find_bounding_box_size(copy);
    
    // Display the image
    cv::Mat image(size.second, size.first, CV_8UC3, cv::Scalar(0, 0, 0));

    cmn::gui::ColorWheel wheel;
    for (const auto& outline : copy) {
        auto color = wheel.next();
        for (size_t i = 0; i < outline.size(); ++i) {
            cv::Point2f start(outline[i].x, outline[i].y);
            cv::Point2f end(outline[(i + 1) % outline.size()].x, outline[(i + 1) % outline.size()].y);
            cv::line(image, start, end, color, 1);
            cv::circle(image, start, 5, color);
        }
    }
    
    tf::imshow(title, image);
}

void YOLO::receive(SegmentationData& data, track::detect::Result&& result) {
    const auto encoding = Background::meta_encoding();
    const auto mode = Background::image_mode();
    data.frame.set_encoding(encoding);
        
    cv::Mat r3;
    if (mode == ImageMode::R3G3B2) {
        if (data.image->dims == 3)
            convert_to_r3g3b2<3>(data.image->get(), r3);
        else if (data.image->dims == 4)
            convert_to_r3g3b2<4>(data.image->get(), r3);
        else
            throw U_EXCEPTION("Invalid number of channels (",data.image->dims,") in input image for the network.");
    }
    else if(mode == ImageMode::RGB) {
        if(data.image->dims == 3) {
            r3 = data.image->get();
        } else if(data.image->dims == 4) {
            cv::cvtColor(data.image->get(), r3, cv::COLOR_BGRA2BGR);
        } else
            throw U_EXCEPTION("Invalid number of channels (",data.image->dims,") in input image for the network.");
    }
    else if (mode == ImageMode::GRAY) {
        if(data.image->dims == 3)
            cv::cvtColor(data.image->get(), r3, cv::COLOR_BGR2GRAY);
        else if(data.image->dims == 4)
            cv::cvtColor(data.image->get(), r3, cv::COLOR_BGRA2GRAY);
        else if(data.image->dims == 1)
            r3 = data.image->get();
        else
            throw U_EXCEPTION("Invalid number of channels (",data.image->dims,") in input image for the network.");
    } else
        throw U_EXCEPTION("Invalid image mode ", mode);

    const auto detect_only_classes = SETTING(detect_only_classes).value<track::detect::PredictionFilter>();
    const coord_t w = max(0, r3.cols - 1);
    const coord_t h = max(0, r3.rows - 1);
    
    const auto settings = AcceptanceSettings::Make();

    //! decide on whether to use masks (if available), or bounding boxes
    //! if masks are not available. for the boxes we simply copy over all
    //! of the pixels in the bounding box, for the masks we copy over only
    //! the pixels that are inside the mask.
    if (not result.masks().empty()) {
        /// yes we have masks!
        process_instance_segmentation(detect_only_classes, w, h, r3, data, result, settings);
    } else if (not result.obbdata().empty()) {
        /// we have obb data, but no masks
        process_obbs(detect_only_classes, w, h, r3, data, result, settings);
    } else {
        /// we had no instance segmentation...
        process_boxes_only(detect_only_classes, w, h, r3, data, result, settings);
    }
}

void YOLO::process_obbs(
       const track::detect::PredictionFilter& detect_only_classes,
       coord_t w,
       coord_t h,
       const cv::Mat& r3,
       SegmentationData &data,
       track::detect::Result &result,
       const AcceptanceSettings &settings)
{
    size_t N_rows = result.obbdata().size();
    auto& obbdata = result.obbdata();
    
    for (size_t i = 0; i < N_rows; ++i) {
        auto row = obbdata[i];
        if (not detect_only_classes.allowed(row.clid)) {
            continue;
        }
        
        auto corners = row.corners();
        Bounds bounds = detect::ICXYWHR::bounding_box(corners);
        //bounds = bounds.mul(scale_factor);
        bounds.restrict_to(Bounds(0, 0, w, h));
        
        cmn::PixelArray_t pixels;
        std::vector<HorizontalLine> lines;
        
        int ymin = bounds.y;
        int ymax = bounds.y + bounds.height;
        
        for(int y = ymin; y<=ymax && y < h; ++y) {
            std::array<float, 4> intersections;
            size_t index = 0;
            
            /// go through all y and collect lines
            /// go through sides:
            for(size_t e=0; e<4; ++e) {
                Vec2 v0 = corners[e];
                Vec2 v1 = corners[(e+1)%4];
                
                // (v1 - v0) * t + v0 = (1 0) * t + (0 yb)
                //  t = (-v0.x yb-v0.y) / ((v1.x-v0.x-1 v1.y-v0.y))
                //  tx = -v0.x / (v1.x - v0.x -1)
                //  ty = (yb - v0.y) / (v1.y - v0.y)
                
                auto dy = (v1.y - v0.y);
                if(dy == 0) {
                    /// the side is parallel to the y-axis and we are on it
                    if(y == v0.y) {
                        auto xmin = min(v0.x, v1.x);
                        auto xmax = max(v0.x, v1.x);
                        intersections[index++] = xmin;
                        intersections[index++] = xmax;
                    }
                    
                } else {
                    auto ty = (y - v0.y) / dy;
                    if(ty >= 0 && ty < 1) {
                        auto xi = (v1.x - v0.x) * ty + v0.x;
                        intersections[index++] = xi;
                    }
                }
            }
            
            
            if(index < 2) {
                if(not lines.empty()
                   && y < ymax)
                {
                    FormatWarning("Invalid intersections: ", intersections, " (", index,") for y=", y, " with corners ", corners);
                    
                    /*HorizontalLine line{
                        saturate(coord_t(y), coord_t(0), coord_t(h)),
                        saturate(coord_t(bounds.x), coord_t(0), coord_t(w)),
                        saturate(coord_t(bounds.x + bounds.width), coord_t(0), coord_t(w))
                    };
                    
                    pixels.insert(pixels.end(), r3.ptr<uchar>(line.y, line.x0), r3.ptr<uchar>(line.y, line.x1 + 1));
                    lines.emplace_back(std::move(line));*/
                    break;
                }
                    
                continue;
            }
            
            // sort the two x‐intersections
            float xf0 = std::min(intersections[0], intersections[1]);
            float xf1 = std::max(intersections[0], intersections[1]);

            // now round/clamp to integer pixel columns:
            int x0 = static_cast<int>(std::ceil(xf0));
            int x1 = static_cast<int>(std::floor(xf1));
            
            // clamp to image bounds [0..w-1]
            x0 = std::clamp(x0, 0, w-1);
            x1 = std::clamp(x1, 0, w-1);
            
            HorizontalLine line{
                saturate(coord_t(y), coord_t(0), coord_t(h)),
                coord_t(x0),
                coord_t(x1)
            };
            
            pixels.insert(pixels.end(), r3.ptr<uchar>(line.y, line.x0), r3.ptr<uchar>(line.y, line.x1 + 1));
            lines.emplace_back(std::move(line));
        }
        
        //cv::Mat crop;
        //r3(bounds).copyTo(crop);

        /*for (int y = bounds.y; y < bounds.y + bounds.height; ++y) {
            HorizontalLine line{
                saturate(coord_t(y), coord_t(0), coord_t(h)),
                saturate(coord_t(bounds.x), coord_t(0), coord_t(w)),
                saturate(coord_t(bounds.x + bounds.width), coord_t(0), coord_t(w))
            };
            pixels.insert(pixels.end(), r3.ptr<uchar>(line.y, line.x0), r3.ptr<uchar>(line.y, line.x1 + 1));
            lines.emplace_back(std::move(line));
        }*/

        if (not lines.empty()) {
            uint8_t flags{0};
            pv::Blob::set_flag(flags, pv::Blob::Flags::is_rgb, r3.channels() == 3);
            pv::Blob::set_flag(flags, pv::Blob::Flags::is_r3g3b2, Background::meta_encoding() == meta_encoding_t::r3g3b2);
            pv::Blob::set_flag(flags, pv::Blob::Flags::is_binary, Background::meta_encoding() == meta_encoding_t::binary);

            pv::Blob blob(lines, flags);

            /// we are not allowed to save this blob to file:
            if(not settings.is_acceptable(blob)) {
                continue;
            }

            data.predictions.push_back({
                .clid = size_t(row.clid),
                .p = float(row.conf)
            });

            blob::Pose pose;
            if(not result.keypoints().empty()) {
                auto p = result.keypoints()[i];
                pose = p.toPose();
                data.keypoints.push_back(std::move(p));
            }

            data.frame.add_object(lines, pixels, flags, blob::Prediction{
                .clid = uint8_t(row.clid),
                .p = uint8_t(float(row.conf) * 255.f),
                .pose = std::move(pose)
            });
        }
    }
}

void YOLO::process_boxes_only(
       const track::detect::PredictionFilter& detect_only_classes,
       coord_t w,
       coord_t h,
       const cv::Mat& r3,
       SegmentationData &data,
       track::detect::Result &result,
       const AcceptanceSettings &settings)
{
    size_t N_rows = result.boxes().num_rows();
    auto& boxes = result.boxes();
    
    for (size_t i = 0; i < N_rows; ++i) {
        auto& row = boxes[i];
        if (not detect_only_classes.allowed(row.clid)) {
            continue;
        }
        
        Bounds bounds = row.box;
        //bounds = bounds.mul(scale_factor);
        bounds.restrict_to(Bounds(0, 0, w, h));
        
        cmn::PixelArray_t pixels;
        std::vector<HorizontalLine> lines;

        for (int y = bounds.y; y < bounds.y + bounds.height; ++y) {
            // integer overflow deals with this, lol
            //assert(uint(y) < data.image->rows);

            HorizontalLine line{
                saturate(coord_t(y), coord_t(0), coord_t(h)),
                saturate(coord_t(bounds.x), coord_t(0), coord_t(w)),
                saturate(coord_t(bounds.x + bounds.width), coord_t(0), coord_t(w))
            };
            pixels.insert(pixels.end(), r3.ptr<uchar>(line.y, line.x0), r3.ptr<uchar>(line.y, line.x1 + 1));
            lines.emplace_back(std::move(line));
        }

        if (not lines.empty()) {
            uint8_t flags{0};
            pv::Blob::set_flag(flags, pv::Blob::Flags::is_rgb, r3.channels() == 3);
            /// TODO: this might be a bit unsafe?
            pv::Blob::set_flag(flags, pv::Blob::Flags::is_r3g3b2, Background::meta_encoding() == meta_encoding_t::r3g3b2);
            pv::Blob::set_flag(flags, pv::Blob::Flags::is_binary, Background::meta_encoding() == meta_encoding_t::binary);
            
            pv::Blob blob(lines, flags);
            
            /// we are not allowed to save this blob to file:
            if(not settings.is_acceptable(blob)) {
                continue;
            }
            
            data.predictions.push_back({
                .clid = size_t(row.clid),
                .p = float(row.conf)
            });
            
            blob::Pose pose;
            if(not result.keypoints().empty()) {
                auto p = result.keypoints()[i];
                pose = p.toPose();
                data.keypoints.push_back(std::move(p));
            }
            
            data.frame.add_object(lines, pixels, flags, blob::Prediction{
                .clid = uint8_t(row.clid),
                .p = uint8_t(float(row.conf) * 255.f),
                .pose = std::move(pose)
            });
        }
    }
}

void YOLO::process_instance_segmentation(
      const track::detect::PredictionFilter& detect_only_classes,
      coord_t w,
      coord_t h,
      const cv::Mat& r3,
      SegmentationData &data,
      track::detect::Result &result,
      const AcceptanceSettings &settings)
{
    size_t N_rows = result.boxes().num_rows();
    auto& boxes = result.boxes();
    
    /// controls access to the data property
    std::mutex mutex;
    
    /// this function processes individual entries into the boxes
    /// array delivered in the result. go row by row and calculate
    /// the outlines from instance segmentation...
    auto fn = [&](auto, size_t start, size_t end, auto) {
        cmn::CPULabeling::DLList list;
        
        for(size_t i=start; i!=end; ++i) {
            auto& row = boxes[i];
            
            /// filter using *detect_only_classes*
            if (not detect_only_classes.allowed(row.clid))
            {
                continue;
            }
            
            auto& mask = result.masks()[i];
            
            auto r = process_instance(list, w, h, r3, row, mask, settings);
            if(r) {
                auto &&[assign, pair] = r.value();
                
                /// synchronized access to the global data property
                std::unique_lock guard(mutex);
                data.predictions.emplace_back(std::move(assign));
                data.frame.add_object(std::move(pair));
            }
        }
            
    };
    
    if(N_rows > 1) {
        distribute_indexes(fn, *_pool, size_t(0), N_rows);
    } else {
        fn(0, 0, N_rows, 0);
    }
}

std::optional<std::tuple<SegmentationData::Assignment, blob::Pair>> YOLO::process_instance(
     cmn::CPULabeling::DLList& list,
     coord_t w,
     coord_t h,
     const cv::Mat &r3,
     const track::detect::Row &row,
     const track::detect::MaskData &mask,
     const AcceptanceSettings& settings)
{
    // Extract bounding box from the detection row
    Bounds bounds = row.box;

    // Perform CPU-based connected-component labeling on the mask
    auto blobs = CPULabeling::run(list, mask.mat);
    if(blobs.empty())
        // If no blobs found, skip this instance
        return std::nullopt;
    
    // Identify the largest blob by pixel count
    size_t msize = 0, midx = 0;
    for (size_t j = 0; j < blobs.size(); ++j) {
        if (blobs.at(j).pixels->size() > msize) {
            msize = blobs.at(j).pixels->size();
            midx = j;
        }
    }

    // Select the blob with the maximum pixel count for further processing
    auto&& pair = blobs.at(midx);
    //size_t num_pixels{ 0u };
    // Adjust each horizontal line by bounding-box offset and clamp to image dimensions
    for (auto& line : *pair.lines) {
        line.x0 = saturate(coord_t(line.x0 + bounds.x), coord_t(0), w);
        line.x1 = saturate(coord_t(line.x1 + bounds.x), line.x0, w);
        line.y = saturate(coord_t(line.y + bounds.y), coord_t(0), h);
        //num_pixels += ptr_safe_t(line.x1) - ptr_safe_t(line.x0) + ptr_safe_t(1);
        
        if (line.x0 >= r3.cols
            || line.x1 >= r3.cols
            || line.y >= r3.rows)
            throw U_EXCEPTION("Coordinates of line ", line, " are invalid for image ", r3.cols, "x", r3.rows);
        // Now each line coordinate lies within valid image bounds
    }

    // Assign class ID and confidence to this blob prediction
    pair.pred = blob::Prediction{
        .clid = static_cast<uint8_t>(row.clid),
        .p = uint8_t(float(row.conf) * 255.f)
    };
    // Mark blob as instance segmentation and set encoding-based flags
    pair.extra_flags |= pv::Blob::flag(pv::Blob::Flags::is_instance_segmentation);
    
    const auto meta_encoding = Background::meta_encoding();
    if(meta_encoding == meta_encoding_t::r3g3b2) {
        assert(r3.channels() == 1);
        pv::Blob::set_flag(pair.extra_flags, pv::Blob::Flags::is_r3g3b2, true);
    }
    pv::Blob::set_flag(pair.extra_flags, pv::Blob::Flags::is_rgb, meta_encoding == meta_encoding_t::rgb8);
    pv::Blob::set_flag(pair.extra_flags, pv::Blob::Flags::is_binary, meta_encoding == meta_encoding_t::binary);
    assert(pv::Blob::is_flag(pair.extra_flags, pv::Blob::Flags::is_rgb) == (meta_encoding == meta_encoding_t::rgb8));

    // Build a Blob object from the labeled lines for acceptance testing and pixel extraction
    pv::Blob blob(std::make_unique<std::vector<HorizontalLine>>(*pair.lines), nullptr, uint8_t(pair.extra_flags), blob::Prediction{pair.pred});
    
    /// Check whether the given object is acceptable regarding the current
    /// segmentation settings or not:
    if(not settings.is_acceptable(blob)) {
        return std::nullopt;
    }
    
    //pv::Blob blob(*pair.lines, *pair.pixels, pair.extra_flags, pair.pred);
    // Convert the blob outline into actual pixel values from the image
    auto [o, px] = blob.calculate_pixels(r3);
    pair.pixels = std::move(px);

    // Extract the outer contour points from the blob for outline construction
    auto points = pixel::find_outer_points(&blob, 0);
    // Remove any invalid or empty contour point sets
    for(auto it = points.begin(); it != points.end(); ) {
        if(not *it || (*it)->empty())
            it = points.erase(it);
        else
            ++it;
    }
    
    // Prepare assignment structure with class and probability for this detection
    SegmentationData::Assignment assign{
        .clid = size_t(row.clid),
        .p = float(row.conf)
    };
    
    // If there are contour points, process outlines and optionally compress
    if (not points.empty()) {
        // here we should likely make sure that we collect all possible lines
        // not just the outer lines?
        //Print("We have detected ", points.size(), " outlines here but only use the first one.");
        
        // Retrieve outline compression setting to reduce vertex count if needed
        /// we may have to downsample outlines
        const auto outline_compression = FAST_SETTING(outline_compression);
        
        // Containers for storing original and compressed outlines
        std::vector<std::vector<Vec2>> all;
        std::vector<Vec2> reduced;
        // If compression is enabled and the outline is large, perform downsampling
        if(outline_compression > 0
           && points.front()->size() > 1000)
        {
            reduced.reserve(points.front()->size());
            gui::reduce_vertex_line(*points.front(), reduced, 0.5);
            Print(points.front()->size(), " reduced to ", reduced.size());
            all.emplace_back(reduced);
            
            // Store the compressed outline as the primary outline
            //data.outlines.emplace_back(*points.front());
            pair.pred.outlines.set_original(std::move(reduced));
            
            // Visualization: draw full outlines for debugging
            draw_outlines(points);
            
        } else {
            // No compression: store original outline directly
            pair.pred.outlines.set_original(std::move(*points.front()));
        }
        
        // Remove the used first outline from the list
        points.erase(points.begin());
        
        if(outline_compression > 0) {
            // Process any remaining outlines after the first
            for(auto& pts : points) {
                reduced.clear();
                reduced.reserve(pts->size());
                
                gui::reduce_vertex_line(*pts, reduced, 0.5);
                Print("* ",pts->size(), " reduced to ", reduced.size());
                all.emplace_back(reduced);
                
                // Append additional outlines to the prediction object
                pair.pred.outlines.add(std::move(reduced));
            }
            
            draw_outlines(all, "Reduced");
            
        } else {
            // Process any remaining outlines after the first
            for(auto& pts : points)
                // Append additional outlines to the prediction object
                pair.pred.outlines.add(std::move(*pts));
        }
    }
    
    return std::make_tuple(
        std::move(assign),
        std::move(pair)
    );
}

bool YOLO::is_initializing() {
    std::unique_lock guard(init_mutex);
    return init_future.valid();
}

double YOLO::fps() {
    if(_network_samples.load() == 0u)
        return 0.0;
    return _network_fps.load() / double(_network_samples.load());
}

struct YOLO::TransferData {
    std::vector<Image::Ptr> images;
    //std::vector<Image::Ptr> oimages;
    std::vector<SegmentationData> datas;
    std::vector<Vec2> scales;
    std::vector<Vec2> offsets;
    std::vector<size_t> orig_id;
    std::vector<std::promise<SegmentationData>> promises;
    std::vector<std::function<void()>> callbacks;

    TransferData() = default;
    TransferData(const TransferData&) = delete;
    TransferData(TransferData&&) = default;
    TransferData& operator=(TransferData&&) = default;
    TransferData& operator=(const TransferData&) = delete;

    ~TransferData() {
        for (auto&& img : images) {
            TileImage::move_back(std::move(img));
        }
        //thread_print("** deleting ", (uint64_t)this);
    }
};

void YOLO::StartPythonProcess(TransferData&& transfer) {
    if (not yolo_initialized) {
        // probably shutting down at the moment
        throw U_EXCEPTION("Cannot start a python process because we are shutting down.");
        /*for (size_t i = 0; i < transfer.datas.size(); ++i) {
            transfer.promises.at(i).set_exception(nullptr);

            try {
                transfer.callbacks.at(i)();
            }
            catch (...) {
                FormatExcept("Exception in callback of element ", i, " in python results.");
            }
        }
        FormatExcept("System shutting down.");
        return;*/
    }

    Timer timer;
    using py = track::PythonIntegration;
    //thread_print("** transfer of ", (uint64_t)& transfer);

    bool force = false;
    const size_t _N = transfer.datas.size();
    {
        [[maybe_unused]] ModuleProxy yolo("trex_yolo", [&force](ModuleProxy&) {
            force = true;
        }, true);
        [[maybe_unused]] ModuleProxy detection_model("trex_detection_model", [&force](ModuleProxy&){
            force = true;
        }, true);
    }
    
    if(force) {
        try {
            py::unload_module("bbx_saved_model");
        } catch(...) {
            FormatWarning("Was unable to unload the module.");
        }
    }
    ModuleProxy bbx("bbx_saved_model", YOLO::reinit, true);
    //bbx.set_variable("offsets", std::move(transfer.offsets));
    //bbx.set_variable("image", transfer.images);
    //bbx.set_variable("oimages", transfer.oimages);

    std::vector<uint64_t> mask_Ns;
    std::vector<float> mask_points;

    try {
        track::detect::YoloInput input{
            std::move(transfer.images),
            (transfer.offsets),
            (transfer.scales),
            (transfer.orig_id),
            [](std::vector<Image::Ptr>&& images)
            {
                for (auto&& image : images)
                    TileImage::move_back(std::move(image));
            }
        };

        //auto results = py::predict(std::move(input), bbx.m);
        //Print("C++ results = ", results);
        auto results = py::predict(std::move(input), bbx.m);
        double elapsed = timer.elapsed();
        timer.reset();
        ReceivePackage(std::move(transfer), std::move(results));
        //bbx.run("apply");
        //double cpp_elapsed = timer.elapsed();

        auto samples = _network_samples.load();
        auto fps = _network_fps.load();
        if (samples > 10u) {
            fps = fps / double(samples);
            samples = 1;
        }
        _network_fps = fps + (double(_N) / elapsed);
        _network_samples = samples + 1;
        //Print("[py] network: ", elapsed);
        //Print("[cpp] network: ", cpp_elapsed);
    }
    catch (const std::exception& ex) {
        FormatError("Exception: ", ex.what());
        for(auto &t : transfer.promises) {
            try {
                throw SoftException(no_quotes((std::string)ex.what()));
            } catch(...) {
                t.set_exception(std::current_exception());
            }
        }
        
        transfer.promises.clear();
        ReceivePackage(std::move(transfer), {});
        
    }
    catch (...) {
        FormatWarning("Continue after exception...");

        throw;
    }
}

void YOLO::ReceivePackage(TransferData&& transfer, std::vector<track::detect::Result>&& results) {
    //size_t elements{0};
    //size_t outline_elements{0};
    //thread_print("Received a number of results: ", results.size());
    //thread_print("For elements: ", datas);
    //for(auto &t : transfer.oimages)
    //    TileImage::buffers.move_back(std::move(t));

    if (results.empty()) {
#ifndef NDEBUG
        if (not transfer.images.empty())
            tf::imshow("ma", transfer.images.front()->get());
#endif
        if(not transfer.promises.empty()) {
            for (size_t i = 0; i < transfer.datas.size(); ++i) {
                try {
                    transfer.promises.at(i).set_value(std::move(transfer.datas.at(i)));
                }
                catch (...) {
                    FormatExcept("A promise failed for ", transfer.datas.at(i));
                    transfer.promises.at(i).set_exception(std::current_exception());
                }
                
                try {
                    transfer.callbacks.at(i)();
                }
                catch (...) {
                    FormatExcept("Exception in callback of element ", i, " in python results.");
                }
            }
        }
        FormatExcept("Empty data for ", transfer.datas, " image=", transfer.orig_id);
        return;
    }
    
    std::unique_lock guard(transfer_done_mutex);
    if(transferred_done.valid())
        transferred_done.get();

    /// pack the function and move it into the pool
    /// (we have non-copyable stuff in there so we need to pack)
    /// this will move all the post-processing into a different
    /// thread:
    auto p = pack<void()>([transfer = std::move(transfer), results = std::move(results)]() mutable {
        for (size_t i = 0; i < transfer.datas.size(); ++i) {
            auto&& result = results.at(i);
            auto& data = transfer.datas.at(i);
            
            try {
                receive(data, std::move(result));
                transfer.promises.at(i).set_value(std::move(data));
            }
            catch (...) {
                FormatExcept("A promise failed for ", transfer.datas.at(i));
                transfer.promises.at(i).set_exception(std::current_exception());
            }

            try {
                transfer.callbacks.at(i)();
            }
            catch (...) {
                FormatExcept("Exception in callback of element ", i, " in python results.");
            }
        }
    });
    
    transferred_done = _pool ? _pool->enqueue([p = std::move(p)](){
        p();
    }) : std::future<void>{};
}

void YOLO::apply(std::vector<TileImage>&& tiles) {
    while(true) {
        if(std::unique_lock guard(init_mutex);
           init_future.valid())
        {
            if(init_future.wait_for(std::chrono::milliseconds(1)) == std::future_status::ready) {
                init_future.get();
                break;
            }
        } else
            break;
    }
    
    namespace py = Python;
    TransferData transfer;

    size_t i = 0;
    for(auto&& tiled : tiles) {
        transfer.images.insert(transfer.images.end(), std::make_move_iterator(tiled.images.begin()), std::make_move_iterator(tiled.images.end()));
        
        if(not tiled.promise)
            throw U_EXCEPTION("Promise was not set.");
        transfer.promises.emplace_back(std::move(*tiled.promise));
        tiled.promise = nullptr;
        
        //Print("Image scale: ", scale, " with tile source=", tiled.source_size, " image=", data.image->dimensions()," output_size=", SETTING(output_size).value<Size2>(), " original=", tiled.original_size);
        
        for(auto p : tiled.offsets()) {
            transfer.orig_id.push_back(i);
            transfer.scales.push_back( //SETTING(output_size).value<Size2>()
                                      tiled.original_size.div(tiled.source_size) );
            tiled.data.tiles.push_back(Bounds(p.x, p.y, tiled.tile_size.width, tiled.tile_size.height).mul(transfer.scales.back()));
        }
        
        auto o = tiled.offsets();
        transfer.offsets.insert(transfer.offsets.end(), o.begin(), o.end());
        transfer.datas.emplace_back(std::move(tiled.data));
        transfer.callbacks.emplace_back(tiled.callback);
        
        ++i;
    }

    tiles.clear();
    
    try {
        {
            std::unique_lock guard(running_mutex);
            if(running_prediction.valid())
                running_prediction.get();
            running_promise = {};
            running_prediction = running_promise.get_future().share();
        }

        py::schedule([&transfer]() mutable {
            StartPythonProcess(std::move(transfer));
        }).get();
        
        running_promise.set_value();
        
    } catch(...) {
        running_promise.set_value();
        for(auto &t : transfer.promises) {
            t.set_exception(std::current_exception());
        }
        //throw;
    }
}

}
