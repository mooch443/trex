{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from matplotlib import backends\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiameseNetworkBase(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, image_size, color_channels, embedding_size):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_once(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, input1, input2):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_embedding(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model(self, filepath):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self, filepath):\n",
    "        pass\n",
    "\n",
    "class SiameseNetworkPyTorch(SiameseNetworkBase, nn.Module):\n",
    "    def __init__(self, image_size=(105, 105), color_channels=1, embedding_size=128):\n",
    "        nn.Module.__init__(self)\n",
    "        SiameseNetworkBase.__init__(self, image_size, color_channels, embedding_size)\n",
    "        self.image_size = image_size\n",
    "        self.color_channels = color_channels\n",
    "        self.embedding_size = embedding_size\n",
    "        self.transform = None\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.color_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # Calculate the size after convolutional layers to define the first linear layer\n",
    "        self._to_linear = self._calculate_conv_output_size()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, self.embedding_size)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _calculate_conv_output_size(self):\n",
    "        # Dummy input to calculate the size\n",
    "        with torch.no_grad():\n",
    "            #print(f\"Calculating the size after convolutional layers... (input size: {self.color_channels}x{self.image_size[0]}x{self.image_size[1]})\")\n",
    "            x = torch.zeros(1, self.color_channels, *self.image_size)\n",
    "            x = self.conv_layers(x)\n",
    "            return x.view(1, -1).size(1)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size()[0], -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n",
    "    @staticmethod\n",
    "    def load_trained_model(filepath, image_size=(105, 105), color_channels=3, embedding_size=128):\n",
    "        \"\"\"\n",
    "        Loads a trained Siamese network from a saved state dict.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the saved model file.\n",
    "            image_size (tuple): Size of the input images.\n",
    "            color_channels (int): Number of color channels.\n",
    "            embedding_size (int): Size of the embedding vector.\n",
    "\n",
    "        Returns:\n",
    "            SiameseNetworkPyTorch: An instance of the loaded model.\n",
    "        \"\"\"\n",
    "        model = SiameseNetworkPyTorch(\n",
    "            image_size=image_size,\n",
    "            color_channels=color_channels,\n",
    "            embedding_size=embedding_size\n",
    "        )\n",
    "        model.load_model(filepath)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def predict(self, img1, img2, transform, threshold):\n",
    "        \"\"\"\n",
    "        Predicts whether two images are similar or dissimilar.\n",
    "\n",
    "        Args:\n",
    "            img1 (PIL.Image or str): First image or path to the image.\n",
    "            img2 (PIL.Image or str): Second image or path to the image.\n",
    "            transform (callable): Transformations to apply to the images.\n",
    "            threshold (float): Threshold for determining similarity.\n",
    "\n",
    "        Returns:\n",
    "            float: The Euclidean distance between the embeddings.\n",
    "            int: Prediction label (0 for similar, 1 for dissimilar).\n",
    "        \"\"\"\n",
    "        # Load images if paths are provided\n",
    "        if isinstance(img1, str):\n",
    "            img1 = Image.open(img1).convert('RGB')\n",
    "        if isinstance(img2, str):\n",
    "            img2 = Image.open(img2).convert('RGB')\n",
    "\n",
    "        # Apply transformations\n",
    "        img1 = transform(img1).unsqueeze(0)  # Add batch dimension\n",
    "        img2 = transform(img2).unsqueeze(0)\n",
    "\n",
    "        # Move to device\n",
    "        device = next(self.parameters()).device\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            output1 = self.forward_once(img1)\n",
    "            output2 = self.forward_once(img2)\n",
    "\n",
    "        # Compute Euclidean distance\n",
    "        #euclidean_distance = F.pairwise_distance(output1, output2).item()\n",
    "        cosine_similarity = F.cosine_similarity(output1, output2).item()\n",
    "\n",
    "        # Make prediction based on threshold\n",
    "        #prediction = 1 if euclidean_distance > threshold else -1\n",
    "        prediction = 1.0 if cosine_similarity >= threshold else -1.0\n",
    "\n",
    "        #return euclidean_distance, prediction\n",
    "        return cosine_similarity, prediction\n",
    "\n",
    "    def compute_embedding(self, img, transform):\n",
    "        \"\"\"\n",
    "        Computes the embedding for a single image.\n",
    "\n",
    "        Args:\n",
    "            img (PIL.Image or str): Image or path to the image.\n",
    "            transform (callable): Transformations to apply to the image.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Embedding vector.\n",
    "        \"\"\"\n",
    "        # Load image if path is provided\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img).convert('RGB')\n",
    "\n",
    "        # Apply transformations\n",
    "        img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Move to device\n",
    "        device = next(self.parameters()).device\n",
    "        img = img.to(device)\n",
    "\n",
    "        # Compute embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = self.forward_once(img)\n",
    "\n",
    "        return embedding.cpu().numpy().flatten()\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        self.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights using Xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SiameseDatasetBase(Dataset, ABC):\n",
    "    def __init__(self, transform=None, callbacks=None):\n",
    "        \"\"\"\n",
    "        Base class for Siamese datasets.\n",
    "\n",
    "        Args:\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            callbacks (dict, optional): Dictionary of callback functions for data retrieval.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.callbacks = callbacks or {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class SiameseDatasetFromPaths(SiameseDatasetBase):\n",
    "    def __init__(self, image_folder_dataset, transform=None, num_pairs=10000):\n",
    "        super().__init__(transform=transform)\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.classes = list(self.image_folder_dataset.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "        # Precompute pairs\n",
    "        self.pairs = []\n",
    "        for _ in range(num_pairs):\n",
    "            should_get_same_class = random.randint(0, 1)\n",
    "            class1 = random.choice(self.classes)\n",
    "            img1_path = random.choice(self.image_folder_dataset[class1])\n",
    "\n",
    "            if should_get_same_class or len(self.classes) < 2:\n",
    "                # Positive pair\n",
    "                class2 = class1\n",
    "                label = 1.0\n",
    "            else:\n",
    "                # Negative pair\n",
    "                class2 = random.choice(self.classes)\n",
    "                while class1 == class2:\n",
    "                    class2 = random.choice(self.classes)\n",
    "                label = -1.0\n",
    "\n",
    "            img2_path = random.choice(self.image_folder_dataset[class2])\n",
    "            self.pairs.append((img1_path, img2_path, label))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1_path, img2_path, label = self.pairs[index]\n",
    "\n",
    "        img1 = Image.open(img1_path).convert('L')\n",
    "        img2 = Image.open(img2_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "def build_image_dataset_from_directory(root_dir):\n",
    "    \"\"\"\n",
    "    Builds a dictionary where each key is a class label and the value is a list of image paths.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Path to the root directory ('train' or 'val').\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with class labels as keys and lists of image paths as values.\n",
    "    \"\"\"\n",
    "    image_dataset = {}\n",
    "    classes = os.listdir(root_dir)\n",
    "    for cls in classes:\n",
    "        cls_path = os.path.join(root_dir, cls)\n",
    "        if os.path.isdir(cls_path):\n",
    "            images = [os.path.join(cls_path, img) for img in os.listdir(cls_path)\n",
    "                      if img.lower().endswith(('jpg', 'png', 'jpeg'))]\n",
    "            if images:\n",
    "                image_dataset[cls] = images\n",
    "    return image_dataset\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class SiameseDatasetFromCpp(SiameseDatasetBase):\n",
    "    def __init__(self, transform=None, callbacks=None):\n",
    "        super().__init__(transform=transform, callbacks=callbacks)\n",
    "\n",
    "        if 'get_data' not in self.callbacks:\n",
    "            raise ValueError(\"Callback 'get_data' must be provided for SiameseDatasetFromCpp\")\n",
    "\n",
    "        # Optionally, initialize any required state or connection here\n",
    "        # For example, establish a connection to the C++ application if needed\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves a data sample from the C++ application via the 'get_data' callback.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the sample. Since data is provided via callback, index can be ignored.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, Tensor]: Tuple containing the two images and the label.\n",
    "        \"\"\"\n",
    "        # Fetch image data and label from the C++ application via the 'get_data' callback\n",
    "        img1_data, img2_data, label = self.callbacks['get_data']()\n",
    "\n",
    "        # Convert raw data to PIL Images\n",
    "        img1 = Image.fromarray(img1_data).convert('RGB')\n",
    "        img2 = Image.fromarray(img2_data).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        label_tensor = torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "        return img1, img2, label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Since data is provided on-demand from the C++ application, we can set an arbitrary large number\n",
    "        or manage the length via a callback if needed.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        if 'get_length' in self.callbacks:\n",
    "            return self.callbacks['get_length']()\n",
    "        else:\n",
    "            # Return a large number to simulate an infinite dataset\n",
    "            return 1000000\n",
    "    \n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "    \n",
    "# siamese_trainer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import or define your model and dataset classes here\n",
    "# from siamese_network import SiameseNetworkPyTorch\n",
    "# from contrastive_loss import ContrastiveLoss\n",
    "# from siamese_dataset import SiameseDatasetFromPaths, SiameseDatasetFromCpp\n",
    "\n",
    "class SiameseTrainer:\n",
    "    @staticmethod\n",
    "    def train_model(\n",
    "        model_params,\n",
    "        train_dataset_params,\n",
    "        val_dataset_params,\n",
    "        training_params,\n",
    "        callbacks\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Static method to train the Siamese network.\n",
    "\n",
    "        Args:\n",
    "            model_params (dict): Parameters for model initialization.\n",
    "            train_dataset_params (dict): Parameters for the training dataset.\n",
    "            val_dataset_params (dict): Parameters for the validation dataset.\n",
    "            training_params (dict): Training parameters (epochs, batch_size, learning_rate, etc.).\n",
    "            callbacks (dict): Dictionary of callback functions.\n",
    "        \"\"\"\n",
    "        # Unpack model parameters\n",
    "        image_size = model_params.get('image_size', (105, 105))\n",
    "        color_channels = model_params.get('color_channels', 3)\n",
    "        embedding_size = model_params.get('embedding_size', 128)\n",
    "\n",
    "        # Unpack training parameters\n",
    "        num_epochs = training_params.get('num_epochs', 10)\n",
    "        batch_size = training_params.get('batch_size', 32)\n",
    "        learning_rate = training_params.get('learning_rate', 0.0005)\n",
    "        max_batches = training_params.get('max_batches', None)\n",
    "        margin = training_params.get('margin', 2.0)\n",
    "        threshold = training_params.get('threshold', 0.5) #margin / 2.0)\n",
    "        patience = training_params.get('patience', 5)  # For early stopping\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "        # Define transformations\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5]*color_channels, std=[0.5]*color_channels)\n",
    "        ])\n",
    "\n",
    "        # Initialize the training dataset\n",
    "        train_dataset_class = train_dataset_params.get('dataset_class', 'paths')\n",
    "        if train_dataset_class == 'dataset':\n",
    "            train_dataset = train_dataset_params.get('data')\n",
    "        elif train_dataset_class == 'paths':\n",
    "            train_dataset = SiameseDatasetFromPaths(train_dataset_params.get('data'), transform=transform)\n",
    "        elif train_dataset_class == 'cpp':\n",
    "            train_dataset = SiameseDatasetFromCpp(transform=transform, callbacks=callbacks)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid train dataset_class specified.\")\n",
    "\n",
    "        # Initialize the validation dataset\n",
    "        val_dataset_class = val_dataset_params.get('dataset_class', 'paths')\n",
    "        if val_dataset_class == 'dataset':\n",
    "            val_dataset = val_dataset_params.get('data')\n",
    "        elif val_dataset_class == 'paths':\n",
    "            val_dataset = SiameseDatasetFromPaths(val_dataset_params.get('data'), transform=transform)\n",
    "        elif val_dataset_class == 'cpp':\n",
    "            val_dataset = SiameseDatasetFromCpp(transform=transform, callbacks=callbacks)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid val dataset_class specified.\")\n",
    "\n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "        # Initialize the model\n",
    "        print(f\"Training on {device}, with {color_channels} color channels\")\n",
    "        model = SiameseNetworkPyTorch(\n",
    "            image_size=image_size,\n",
    "            color_channels=color_channels,\n",
    "            embedding_size=embedding_size\n",
    "        )\n",
    "        model.transform = transform  # Store the transform for later use\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        #criterion = ContrastiveLoss(margin=margin)\n",
    "        criterion = torch.nn.CosineEmbeddingLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Learning Rate Scheduler\n",
    "        from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "        # Early stopping parameters\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        early_stop = False\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            if early_stop:\n",
    "                print(\"Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            print(f\"Starting epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "            for batch_idx, (img1, img2, label) in enumerate(tqdm(train_loader)):\n",
    "                if max_batches and batch_idx >= max_batches:\n",
    "                    break  # Limit the number of batches if desired\n",
    "\n",
    "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                output1, output2 = model(img1, img2)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(output1, output2, label)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # Compute distances\n",
    "                #euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                cosine_similarity = F.cosine_similarity(output1, output2)\n",
    "\n",
    "                # Make predictions based on threshold\n",
    "                pred = torch.where(cosine_similarity >= threshold, torch.tensor(1.0).to(device), torch.tensor(-1.0).to(device))\n",
    "\n",
    "                # Make predictions based on threshold\n",
    "                #pred = (euclidean_distance > threshold).float()\n",
    "                \n",
    "                # Compare predictions with labels\n",
    "                correct += (pred == label).sum().item()\n",
    "                total += label.size(0)\n",
    "\n",
    "                # Call the 'on_batch_end' callback\n",
    "                if 'on_batch_end' in callbacks:\n",
    "                    callbacks['on_batch_end'](epoch, batch_idx, loss.item())\n",
    "\n",
    "            avg_loss = epoch_loss / (batch_idx + 1)\n",
    "            train_acc = correct / total\n",
    "\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] completed. Average Training Loss: {avg_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "            # Perform validation\n",
    "            val_loss, val_acc = SiameseTrainer.validate_model(model, val_loader, criterion, device, threshold)\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "                # Save the best model weights\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print(f\"Validation loss decreased ({best_val_loss:.4f}). Saving model ...\")\n",
    "                # Optionally, save the model to a file\n",
    "                if 'save_model' in callbacks:\n",
    "                    callbacks['save_model'](model, epoch)\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f'Epochs without improvement: {epochs_no_improve}/{patience}')\n",
    "\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print('Early stopping triggered.')\n",
    "                    early_stop = True\n",
    "                    break  # Break out of the epoch loop\n",
    "\n",
    "            # Call the 'on_epoch_end' callback\n",
    "            if 'on_epoch_end' in callbacks:\n",
    "                callbacks['on_epoch_end'](epoch, avg_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "        # Load the best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(\"Best model restored.\")\n",
    "\n",
    "        # Call the 'on_training_end' callback\n",
    "        if 'on_training_end' in callbacks:\n",
    "            callbacks['on_training_end'](model)\n",
    "\n",
    "        return model  # Return the trained model\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_model(model, val_loader, criterion, device, threshold):\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (img1, img2, label) in enumerate(tqdm(val_loader)):\n",
    "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output1, output2 = model(img1, img2)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(output1, output2, label)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Compute distances\n",
    "                #euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                cosine_similarity = F.cosine_similarity(output1, output2)\n",
    "\n",
    "                # Make predictions based on threshold\n",
    "                #pred = (euclidean_distance > threshold).float()\n",
    "                pred = torch.where(cosine_similarity >= threshold, torch.tensor(1.0).to(device), torch.tensor(-1.0).to(device))\n",
    "\n",
    "                # Compare predictions with labels\n",
    "                correct += (pred == label).sum().item()\n",
    "                total += label.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / (batch_idx + 1)\n",
    "        val_acc = correct / total\n",
    "\n",
    "        return avg_val_loss, val_acc\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define dummy callbacks\n",
    "def on_batch_end(epoch, batch_idx, loss):\n",
    "    pass\n",
    "    #print(f\"[Callback] Epoch {epoch}, Batch {batch_idx}, Loss: {loss:.4f}\")\n",
    "\n",
    "def on_epoch_end(epoch, avg_train_loss, train_acc, avg_val_loss, val_acc):\n",
    "    print(f\"[Callback] Epoch {epoch} ended.\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "def save_model(model, epoch):\n",
    "    # For testing, we won't actually save the model\n",
    "    print(f\"[Callback] Model saved at epoch {epoch}\")\n",
    "\n",
    "def on_training_end(model):\n",
    "    print(\"[Callback] Training completed.\")\n",
    "\n",
    "# Assemble the callbacks into a dictionary\n",
    "callbacks = {\n",
    "    'on_batch_end': on_batch_end,\n",
    "    'on_epoch_end': on_epoch_end,\n",
    "    'save_model': save_model,\n",
    "    'on_training_end': on_training_end\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in training set: 8\n",
      "Number of classes in validation set: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/157 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m\n\u001b[1;32m     70\u001b[0m val_dataset_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: val_image_dataset  \u001b[38;5;66;03m# The image dataset built earlier\u001b[39;00m\n\u001b[1;32m     73\u001b[0m }\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#'''\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mSiameseTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataset_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 520\u001b[0m, in \u001b[0;36mSiameseTrainer.train_model\u001b[0;34m(model_params, train_dataset_params, val_dataset_params, training_params, callbacks)\u001b[0m\n\u001b[1;32m    517\u001b[0m pred \u001b[38;5;241m=\u001b[39m (euclidean_distance \u001b[38;5;241m>\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Compare predictions with labels\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    521\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Call the 'on_batch_end' callback\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define model parameters\n",
    "model_params = {\n",
    "    'image_size': (80, 80),\n",
    "    'color_channels': 1,\n",
    "    'embedding_size': 128\n",
    "}\n",
    "\n",
    "# Define training parameters\n",
    "training_params = {\n",
    "    'num_epochs': 25,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.0005,\n",
    "    'margin': 2.0,\n",
    "    'max_batches': 50  # Limit the number of batches for testing\n",
    "}\n",
    "\n",
    "# For c++ usage\n",
    "import numpy as np\n",
    "\n",
    "def get_data():\n",
    "    # Generate dummy image data (random noise)\n",
    "    height, width, channels = 105, 105, 3\n",
    "    img1_data = np.random.randint(0, 256, (height, width, channels), dtype=np.uint8)\n",
    "    img2_data = np.random.randint(0, 256, (height, width, channels), dtype=np.uint8)\n",
    "    label = np.random.choice([0, 1])  # Randomly choose 0 or 1\n",
    "\n",
    "    return img1_data, img2_data, label\n",
    "\n",
    "# /// Update callbacks\n",
    "'''\n",
    "callbacks['get_data'] = get_data\n",
    "\n",
    "train_dataset_params = {\n",
    "    'dataset_class': 'cpp',  # Indicate that we're using the dataset that interfaces with C++\n",
    "}\n",
    "\n",
    "# Run the training\n",
    "trained_model = SiameseTrainer.train_model(\n",
    "    model_params,\n",
    "    train_dataset_params,\n",
    "    training_params,\n",
    "    callbacks\n",
    ")\n",
    "#'''\n",
    "# ///\n",
    "\n",
    "#'''\n",
    "# Build the image dataset\n",
    "root_dir = 'id_dataset'  # Replace with your dataset path\n",
    "\n",
    "# Paths to the 'train' and 'val' directories\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "val_dir = os.path.join(root_dir, 'val')\n",
    "\n",
    "# Build the image datasets\n",
    "train_image_dataset = build_image_dataset_from_directory(train_dir)\n",
    "val_image_dataset = build_image_dataset_from_directory(val_dir)\n",
    "\n",
    "# Check the number of classes and images\n",
    "print(f\"Number of classes in training set: {len(train_image_dataset)}\")\n",
    "print(f\"Number of classes in validation set: {len(val_image_dataset)}\")\n",
    "\n",
    "# Define training dataset parameters\n",
    "train_dataset_params = {\n",
    "    'dataset_class': 'paths',\n",
    "    'data': train_image_dataset  # The image dataset built earlier\n",
    "}\n",
    "\n",
    "# Define validation dataset parameters\n",
    "val_dataset_params = {\n",
    "    'dataset_class': 'paths',\n",
    "    'data': val_image_dataset  # The image dataset built earlier\n",
    "}\n",
    "#'''\n",
    "\n",
    "# Run the training\n",
    "trained_model = SiameseTrainer.train_model(\n",
    "    model_params,\n",
    "    train_dataset_params,\n",
    "    val_dataset_params,\n",
    "    training_params,\n",
    "    callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class SiamesePredictor:\n",
    "    def __init__(self, model_path, model_params, threshold):\n",
    "        \"\"\"\n",
    "        Initializes the predictor with a trained model.\n",
    "\n",
    "        Args:\n",
    "            model_path (str or nn.Module): Path to the saved model file or an instance of the model.\n",
    "            model_params (dict): Parameters used to initialize the model.\n",
    "            threshold (float): Threshold for determining similarity.\n",
    "        \"\"\"\n",
    "        # Extract model parameters\n",
    "        self.image_size = model_params.get('image_size', (105, 105))\n",
    "        self.color_channels = model_params.get('color_channels', 1)\n",
    "        self.embedding_size = model_params.get('embedding_size', 128)\n",
    "\n",
    "        # Device configuration\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "        # Load the model\n",
    "        if isinstance(model_path, str):\n",
    "            self.model = SiameseNetworkPyTorch.load_trained_model(\n",
    "                filepath=model_path,\n",
    "                image_size=self.image_size,\n",
    "                color_channels=self.color_channels,\n",
    "                embedding_size=self.embedding_size\n",
    "            )\n",
    "        elif isinstance(model_path, nn.Module):\n",
    "            self.model = model_path\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_path provided.\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Define transformations based on color channels\n",
    "        self.transform = self._get_transform()\n",
    "\n",
    "    def _get_transform(self):\n",
    "        \"\"\"\n",
    "        Defines the transformations to apply to input images.\n",
    "        Adjusts for grayscale or RGB images based on the model's color_channels.\n",
    "        \"\"\"\n",
    "        if self.color_channels == 1:\n",
    "            # Grayscale transformations\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "        else:\n",
    "            # RGB transformations\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5]*self.color_channels, std=[0.5]*self.color_channels)\n",
    "            ])\n",
    "        return transform\n",
    "\n",
    "    def predict(self, input1, input2):\n",
    "        \"\"\"\n",
    "        Predicts similarity between inputs. Automatically handles different input types.\n",
    "\n",
    "        Args:\n",
    "            input1: Single image, list of images, or set of images.\n",
    "            input2: Single image, list of images, set of images, or dict of sets.\n",
    "\n",
    "        Returns:\n",
    "            See the method's docstring in previous answers for possible returns.\n",
    "        \"\"\"\n",
    "        # Handle single image vs single image\n",
    "        if self._is_single_image(input1) and self._is_single_image(input2):\n",
    "            similarity, prediction = self._predict_single_pair(input1, input2)\n",
    "            return similarity, prediction\n",
    "\n",
    "        # Handle single image vs list of images\n",
    "        elif self._is_single_image(input1) and self._is_list_of_images(input2):\n",
    "            results = self._predict_image_to_images(input1, input2)\n",
    "            return results\n",
    "\n",
    "        # Handle list of images vs list of images\n",
    "        elif self._is_list_of_images(input1) and self._is_list_of_images(input2):\n",
    "            results = self._predict_images_to_images(input1, input2)\n",
    "            return results\n",
    "\n",
    "        # Handle set comparison: input2 is a dict of sets\n",
    "        elif self._is_list_of_images(input1) and isinstance(input2, dict):\n",
    "            closest_set, distances = self._compare_set_to_sets(input1, input2)\n",
    "            return closest_set, distances\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input types for prediction.\")\n",
    "\n",
    "    # Helper methods to determine input types\n",
    "    def _is_single_image(self, input):\n",
    "        if isinstance(input, (str, Image.Image)):\n",
    "            return True\n",
    "        if isinstance(input, np.ndarray) and (len(input.shape) == 3 or (len(input.shape) == 4 and input.shape[0] == 1)):\n",
    "            return True\n",
    "        elif isinstance(input, torch.Tensor) and input.dim() == 4 and input.size(0) == 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _is_list_of_images(self, input):\n",
    "        if isinstance(input, np.ndarray) and len(input.shape) == 4 and input.shape[0] > 1:\n",
    "            return True\n",
    "        return isinstance(input, list) and all(isinstance(i, (str, Image.Image, np.ndarray, torch.Tensor)) for i in input)\n",
    "\n",
    "    # Prediction methods\n",
    "    def _predict_single_pair(self, img1, img2):\n",
    "        # Load and preprocess images\n",
    "        img1 = self._prepare_input(img1)\n",
    "        img2 = self._prepare_input(img2)\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            output1 = self.model.forward_once(img1)\n",
    "            output2 = self.model.forward_once(img2)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        cosine_similarity = F.cosine_similarity(output1, output2).item()\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = 1.0 if cosine_similarity >= self.threshold else -1.0\n",
    "\n",
    "        return cosine_similarity, prediction\n",
    "\n",
    "    def _predict_image_to_images(self, img1, images):\n",
    "        # Load and preprocess images\n",
    "        img1 = self._prepare_input(img1)\n",
    "        img2_batch = self._prepare_batch_input(images)\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            output1 = self.model.forward_once(img1)\n",
    "            output2 = self.model.forward_once(img2_batch)\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        cosine_similarities = F.cosine_similarity(output1, output2)\n",
    "\n",
    "        # Make predictions\n",
    "        preds = torch.where(cosine_similarities >= self.threshold, torch.tensor(1.0).to(self.device), torch.tensor(-1.0).to(self.device))\n",
    "        similarities = cosine_similarities.cpu().numpy()\n",
    "        preds = preds.cpu().numpy()\n",
    "\n",
    "        # Pair image identifiers with results\n",
    "        results = list(zip(images, similarities, preds))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _predict_images_to_images(self, images1, images2):\n",
    "        # Load and preprocess images\n",
    "        img1_batch = self._prepare_batch_input(images1)\n",
    "        img2_batch = self._prepare_batch_input(images2)\n",
    "\n",
    "        # Ensure same batch size\n",
    "        if len(images1) != len(images2):\n",
    "            raise ValueError(\"Both image lists must have the same number of images.\")\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            output1 = self.model.forward_once(img1_batch)\n",
    "            output2 = self.model.forward_once(img2_batch)\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        cosine_similarities = F.cosine_similarity(output1, output2)\n",
    "\n",
    "        # Make predictions\n",
    "        preds = torch.where(cosine_similarities >= self.threshold, torch.tensor(1.0).to(self.device), torch.tensor(-1.0).to(self.device))\n",
    "        similarities = cosine_similarities.cpu().numpy()\n",
    "        preds = preds.cpu().numpy()\n",
    "\n",
    "        # Pair image identifiers with results\n",
    "        results = list(zip(images1, images2, similarities, preds))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _compare_set_to_sets(self, comp_images, sets_dict):\n",
    "        \"\"\"\n",
    "        Compares a set of images (comp_images) to multiple sets (sets_dict).\n",
    "\n",
    "        Args:\n",
    "            comp_images (list): List of images in COMP set.\n",
    "            sets_dict (dict): Dictionary with set names as keys and list of images as values.\n",
    "\n",
    "        Returns:\n",
    "            str: Name of the closest set.\n",
    "            dict: Average similarities to each set.\n",
    "        \"\"\"\n",
    "        comp_embeddings = self._compute_embeddings(comp_images)\n",
    "\n",
    "        set_similarities = {}\n",
    "        for set_name, images in sets_dict.items():\n",
    "            set_embeddings = self._compute_embeddings(images)\n",
    "            # Compute similarities between every pair of embeddings\n",
    "            similarities = self._compute_similarity_matrix(comp_embeddings, set_embeddings)\n",
    "            # Compute average similarity\n",
    "            avg_similarity = similarities.mean()\n",
    "            set_similarities[set_name] = avg_similarity\n",
    "\n",
    "        # Determine the closest set (highest average similarity)\n",
    "        closest_set = max(set_similarities, key=set_similarities.get)\n",
    "\n",
    "        return closest_set, set_similarities\n",
    "\n",
    "    def _prepare_input(self, img):\n",
    "        \"\"\"\n",
    "        Prepares a single image input, handling different data types efficiently.\n",
    "\n",
    "        Args:\n",
    "            img: Image input, can be a path, PIL Image, NumPy array, or tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Preprocessed image tensor ready for model input.\n",
    "        \"\"\"\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # Assume tensor is already preprocessed\n",
    "            img = img.to(self.device)\n",
    "            if img.dim() == 3:\n",
    "                img = img.unsqueeze(0)  # Add batch dimension\n",
    "            return img\n",
    "\n",
    "        else:\n",
    "            # Load image if path is provided\n",
    "            if isinstance(img, str):\n",
    "                img = Image.open(img)\n",
    "            elif isinstance(img, np.ndarray):\n",
    "                # Convert NumPy array to PIL Image\n",
    "                if img.ndim == 3:\n",
    "                    if img.shape[2] == 1:\n",
    "                        # Grayscale image\n",
    "                        img = Image.fromarray(np.squeeze(img, axis=2).astype('uint8'), mode='L')\n",
    "                    elif img.shape[2] == 3:\n",
    "                        # RGB image\n",
    "                        img = Image.fromarray(img.astype('uint8'), mode='RGB')\n",
    "                    else:\n",
    "                        # Handle images with other numbers of channels\n",
    "                        img = Image.fromarray(img[:, :, :3].astype('uint8'), mode='RGB')\n",
    "                elif img.ndim == 2:\n",
    "                    # Grayscale image\n",
    "                    img = Image.fromarray(img.astype('uint8'), mode='L')\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported NumPy array shape: {img.shape}\")\n",
    "            elif isinstance(img, Image.Image):\n",
    "                # If it's already a PIL Image, do nothing\n",
    "                pass\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported image type: {type(img)}\")\n",
    "\n",
    "            # Convert image to the appropriate mode\n",
    "            img = self._convert_image_mode(img)\n",
    "\n",
    "            # Apply transformations\n",
    "            img = self.transform(img).unsqueeze(0).to(self.device)\n",
    "            return img\n",
    "\n",
    "    def _prepare_batch_input(self, images):\n",
    "        \"\"\"\n",
    "        Prepares a batch of images efficiently.\n",
    "\n",
    "        Args:\n",
    "            images: List of image inputs.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Batch of preprocessed images.\n",
    "        \"\"\"\n",
    "        tensors = []\n",
    "        for img in images:\n",
    "            tensors.append(self._prepare_input(img))\n",
    "        # Concatenate tensors along the batch dimension\n",
    "        batch_tensor = torch.cat(tensors, dim=0)\n",
    "        return batch_tensor\n",
    "\n",
    "    def _convert_image_mode(self, img):\n",
    "        \"\"\"\n",
    "        Converts the image to the appropriate mode based on color_channels.\n",
    "\n",
    "        Args:\n",
    "            img (PIL.Image): Input image.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: Image converted to the correct mode.\n",
    "        \"\"\"\n",
    "        if self.color_channels == 1:\n",
    "            # Convert to grayscale if necessary\n",
    "            if img.mode != 'L':\n",
    "                img = img.convert('L')\n",
    "        else:\n",
    "            # Convert to RGB if necessary\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "        return img\n",
    "\n",
    "    def _compute_embeddings(self, images):\n",
    "        # Prepare batch input\n",
    "        img_batch = self._prepare_batch_input(images)\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model.forward_once(img_batch)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _compute_similarity_matrix(self, embeddings1, embeddings2):\n",
    "        \"\"\"\n",
    "        Computes the pairwise cosine similarity matrix between two sets of embeddings.\n",
    "\n",
    "        Args:\n",
    "            embeddings1 (Tensor): Embeddings of the first set (N x D).\n",
    "            embeddings2 (Tensor): Embeddings of the second set (M x D).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Similarity matrix of size N x M.\n",
    "        \"\"\"\n",
    "        # Normalize embeddings\n",
    "        embeddings1_norm = F.normalize(embeddings1, p=2, dim=1)\n",
    "        embeddings2_norm = F.normalize(embeddings2, p=2, dim=1)\n",
    "\n",
    "        # Compute cosine similarity matrix\n",
    "        similarities = torch.mm(embeddings1_norm, embeddings2_norm.t())\n",
    "\n",
    "        # Flatten to a single vector\n",
    "        similarities = similarities.view(-1).cpu().numpy()\n",
    "\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part0.npz', '/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part1.npz', '/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part2.npz', '/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part3.npz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:01<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288614, 80, 80, 3) (288614,) (288614,) (288614,) (12253, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:05<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80)\n",
      "Color channels: 3\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from sys import dont_write_bytecode\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class SiameseDatasetFromArrays(Dataset):\n",
    "    def __init__(self, positives, negatives, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with positive and negative examples.\n",
    "\n",
    "        Args:\n",
    "            positives (list): List of positive examples.\n",
    "            negatives (list): List of negative examples.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.data = positives + negatives  # Combine positive and negative examples\n",
    "        random.shuffle(self.data)  # Shuffle the combined data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        label, id1, frame1, image1, id2, frame2, image2 = item\n",
    "\n",
    "        # Process images\n",
    "        img1 = self._process_image(image1)\n",
    "        img2 = self._process_image(image2)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return img1, img2, label\n",
    "\n",
    "    def _process_image(self, image):\n",
    "        #print(image.shape, image.dtype, image.min(), image.max())\n",
    "\n",
    "        # Convert the NumPy array to a tensor\n",
    "        # Expected shape: (H, W, C) or (H, W)\n",
    "        '''if image.ndim == 3:\n",
    "            # Image with channels\n",
    "            image = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n",
    "        elif image.ndim == 2:\n",
    "            # Grayscale image, add channel dimension\n",
    "            image = image[np.newaxis, :, :]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image shape: {image.shape}\")'''\n",
    "\n",
    "        # Convert to float and normalize to [0, 1]\n",
    "        #image = image.astype(np.float32) / 255.0\n",
    "        #print(image.shape)\n",
    "\n",
    "        # Convert to tensor\n",
    "        #img_tensor = torch.from_numpy(image)\n",
    "        if image.shape[-1] == 3:\n",
    "            img_tensor = Image.fromarray(image, 'RGB')\n",
    "        else:\n",
    "            img_tensor = Image.fromarray(image[..., 0], 'L')\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "        else:\n",
    "            # Normalize manually if no transform is provided\n",
    "            raise ValueError(\"A transform function is required.\")\n",
    "            img_tensor = (img_tensor - 0.5) / 0.5  # Assuming mean=0.5, std=0.5\n",
    "\n",
    "        return img_tensor\n",
    "\n",
    "root = '/Users/tristan/trex/docs/notebooks/id_dataset'\n",
    "\n",
    "def concatenate_npzs(base, verbose=False):\n",
    "    paths = sorted(glob(base+\"_tracklet_images_single_part*.npz\"))\n",
    "    print(paths)\n",
    "\n",
    "    all_images = []\n",
    "    all_frames = []\n",
    "    all_ids = []\n",
    "    all_frame_segment_indexes = []\n",
    "    all_frame_segments = []\n",
    "\n",
    "    number_of_segments = 0\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        with np.load(path) as npz:\n",
    "            if verbose:\n",
    "                print(npz.files)\n",
    "                print(npz['ids'].shape) # (192762,)\n",
    "                print(npz['images'].shape) # (192762, 80, 80, 1)\n",
    "\n",
    "            _images = npz['images']\n",
    "            _frames = npz['frames']\n",
    "            _ids = npz['ids']\n",
    "            _frame_segment_indexes = npz['frame_segment_indexes']\n",
    "\n",
    "            for i in range(_frame_segment_indexes.shape[0]):\n",
    "                _frame_segment_indexes[i] += number_of_segments\n",
    "            #_frame_segment_indexes = _frame_segment_indexes + number_of_segments\n",
    "            _frame_segments = npz['frame_segments']\n",
    "            number_of_segments += _frame_segments.shape[0]\n",
    "\n",
    "            all_images.append(_images)\n",
    "            all_frames.append(_frames)\n",
    "            all_ids.append(_ids)\n",
    "            \n",
    "            all_frame_segment_indexes.append(_frame_segment_indexes)\n",
    "            all_frame_segments.append(_frame_segments)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"loading {path}: {npz['images'].shape} {_frame_segment_indexes.min()} {_frame_segment_indexes.shape}\")\n",
    "\n",
    "    all_images = np.concatenate(all_images, axis=0)\n",
    "    all_frames = np.concatenate(all_frames, axis=0)\n",
    "    all_ids = np.concatenate(all_ids, axis=0)\n",
    "    all_frame_segment_indexes = np.concatenate(all_frame_segment_indexes, axis=0)\n",
    "    # assert that all segment indexes are unique\n",
    "    #assert len(np.unique(all_frame_segment_indexes)) == all_frame_segment_indexes.shape[0]\n",
    "    all_frame_segments = np.concatenate(all_frame_segments, axis=0)\n",
    "\n",
    "    return all_images, all_frames, all_ids, all_frame_segment_indexes, all_frame_segments\n",
    "\n",
    "def load_npz_as_dataset(path, dont_use_these_ids = [], verbose=False, split=0.8):\n",
    "    all_images, all_frames, all_ids, all_frame_segment_indexes, all_frame_segments = concatenate_npzs(path, verbose=verbose)\n",
    "\n",
    "    # print shapes\n",
    "    print(all_images.shape, all_frames.shape, all_ids.shape, all_frame_segment_indexes.shape, all_frame_segments.shape)\n",
    "\n",
    "    all_positives = []\n",
    "    all_negatives = []\n",
    "\n",
    "    # save these images (sorted by id) to folders.\n",
    "    # the root folder is given, then below that we create a folder / id and enumerate\n",
    "    # the images in that folder.\n",
    "\n",
    "    # create a folder for each id\n",
    "    for i, id in enumerate(tqdm(np.unique(all_ids))):\n",
    "        if id in dont_use_these_ids:\n",
    "            continue\n",
    "        folder = os.path.join(root, str(id))\n",
    "        #os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        mask = all_ids == id\n",
    "        if verbose:\n",
    "            print(f\"id: {all_ids.shape}, images: {all_images[mask].shape}, segments: {all_frame_segments.shape} mask: {mask.shape}\")\n",
    "        _used_indexes = np.unique(all_frame_segment_indexes[mask])\n",
    "\n",
    "        used_indexes = []\n",
    "        for s in _used_indexes:\n",
    "            if s < 0 or s >= all_frame_segments.shape[0]:\n",
    "                continue\n",
    "            used_indexes.append(s)\n",
    "            assert s >= 0 and s < all_frame_segments.shape[0], f\"{s} not in {all_frame_segments.shape}\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"id: {id}, images: {all_images[mask].shape}, segments: {all_frame_segments[used_indexes]}\")\n",
    "\n",
    "        # get number of images / frame segment\n",
    "        for s in used_indexes:\n",
    "            mask = all_frame_segment_indexes == s\n",
    "            N = mask.sum()\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"segment: {s}, images: {N} frame_segments: {all_frame_segments.shape}\")\n",
    "            segment = all_frame_segments[s]\n",
    "            L = segment[-1] - segment[0] + 1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"segment: {segment}, images: {N}, L: {L}\")\n",
    "\n",
    "            if N < 100:\n",
    "                continue\n",
    "\n",
    "            my_frames = all_frames[mask]\n",
    "            my_images = all_images[mask]\n",
    "\n",
    "            # find data of other ids that are in the same frame segment\n",
    "            current_frame_mask = (all_frames == my_frames[len(my_frames) // 2]) & (all_ids != id) & (np.isin(all_ids, dont_use_these_ids, invert=True))\n",
    "            _filtered_ids = np.unique(all_ids[current_frame_mask])\n",
    "            if verbose:\n",
    "                print(_filtered_ids, \"not in\", dont_use_these_ids, np.isin(_filtered_ids, dont_use_these_ids).any())\n",
    "            assert np.isin(id, _filtered_ids).sum() == 0\n",
    "            assert np.isin(_filtered_ids, dont_use_these_ids).any() == False\n",
    "\n",
    "            other_indexes = np.unique(all_frame_segment_indexes[current_frame_mask])\n",
    "            #other_segments = np.array([npz[\"frame_segments\"][idx] for idx in other_indexes])\n",
    "\n",
    "            # calculate the maximum number of positive examples we can get from within\n",
    "            # the same segment and the current individual (while maintaining a min_distance\n",
    "            # between the images):\n",
    "            min_distance = 10\n",
    "            n_positives = max(1, int(L / min_distance / 2))\n",
    "\n",
    "            # sample positives\n",
    "            _frames = my_frames\n",
    "            chosen_frames = np.random.choice(_frames, size=n_positives, replace=False)\n",
    "            chosen_frames = np.sort(chosen_frames)\n",
    "            # remove the chosen frames from the list of all frames\n",
    "            _frames = np.setdiff1d(_frames, chosen_frames)\n",
    "\n",
    "            #print(f\"chosen_frames: {chosen_frames} frames: {_frames}\")\n",
    "\n",
    "            # now sample the positives, removing a used paired image from the list of all frames\n",
    "            positives = []\n",
    "            for frame in chosen_frames:\n",
    "                if len(_frames) == 0:\n",
    "                    print(\"***> no more frames to choose from\")\n",
    "                    break\n",
    "\n",
    "                my_image = my_images[my_frames == frame]\n",
    "                assert len(my_image) == 1\n",
    "                my_image = my_image[0]\n",
    "\n",
    "                # now choose the index of the paired image.\n",
    "                # prefer to choose an image as far away as possible within the same segment\n",
    "                paired_frame = _frames[np.abs(_frames - frame).argmax()]\n",
    "                _frames = np.setdiff1d(_frames, [paired_frame])\n",
    "\n",
    "                paired_image = my_images[my_frames == paired_frame]\n",
    "                assert len(paired_image) == 1\n",
    "\n",
    "                assert not id in dont_use_these_ids\n",
    "\n",
    "                paired_image = paired_image[0]\n",
    "                positives.append((1.0, id, frame, my_image, id, paired_frame, paired_image))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\tpositives: {len(positives)}\")\n",
    "\n",
    "            # sample images from other segments to get negative examples (dissimilar)\n",
    "            # we can assume that since the frame segments overlap, both individuals which\n",
    "            # are visible in the same frame segment are dissimilar. lets sample 10% of the\n",
    "            # length of the current individuals segment as negative examples.\n",
    "            n_negatives = max(1, int(1.0 * len(positives)))\n",
    "\n",
    "            # cache the frames for all other indexes\n",
    "            other_index_masks = {}\n",
    "            for idx in other_indexes:\n",
    "                other_frame_mask = (all_frame_segment_indexes == idx)\n",
    "                ids = np.unique(all_ids[other_frame_mask])\n",
    "                assert len(ids) == 1, f\"more than one id in frame segment when searching for idx {idx}: {ids}\"\n",
    "                assert not ids[0] in dont_use_these_ids\n",
    "                other_index_masks[idx] = (other_frame_mask, ids[0], all_frames[other_frame_mask], all_images[other_frame_mask])\n",
    "\n",
    "            # sample negatives\n",
    "            #print(f\"other_indexes: {len(other_indexes)}\")\n",
    "            if len(other_indexes) == 0:\n",
    "                continue\n",
    "            \n",
    "            chosen_negatives = np.random.randint(0, len(other_indexes), size=n_negatives)\n",
    "            chosen_indexes = other_indexes[chosen_negatives]\n",
    "\n",
    "            negatives = []\n",
    "\n",
    "            for other_index in chosen_indexes:\n",
    "                # sample a random index from that segment\n",
    "                other_frame_mask, other_id, _frames, _images = other_index_masks[other_index]\n",
    "\n",
    "                found = False\n",
    "                for _ in range(5):\n",
    "                    index = np.random.randint(0, len(_images))\n",
    "                    frame = _frames[index]\n",
    "\n",
    "                    my_image = my_images[my_frames == frame]\n",
    "                    if len(my_image) == 0:\n",
    "                        if verbose:\n",
    "                            print(f\"\\t XX frame: {frame} no image found\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        my_image = my_image[0]\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"\\t => frame: {frame}, image: {len(my_image)} my_images: {my_images.shape}, other_images: {_images.shape}\")\n",
    "                            print(frame, \"in\", my_frames)\n",
    "\n",
    "                            print(f\"\\t\\t my_image: {my_image.shape} other_images: {_images[index].shape}\")\n",
    "\n",
    "                        assert not id in dont_use_these_ids\n",
    "                        assert not other_id in dont_use_these_ids\n",
    "                        \n",
    "                        other_image = (-1.0, id, frame, my_image, other_id, frame, _images[index])\n",
    "                        negatives.append(other_image)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                if not found and verbose:\n",
    "                    print(f\"\\t ** frame: {frame} no image found at all\")\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\tnegatives: {len(negatives)}\")\n",
    "\n",
    "            all_positives.extend(positives)\n",
    "            all_negatives.extend(negatives)\n",
    "\n",
    "            \n",
    "        #break\n",
    "\n",
    "    for label, id1, frame1, image1, id2, frame2, image2 in all_positives:\n",
    "        assert np.isin([id1, id2], list(dont_use_these_ids)).sum() == 0\n",
    "    \n",
    "    for label, id1, frame1, image1, id2, frame2, image2 in all_negatives:\n",
    "        assert np.isin([id1, id2], list(dont_use_these_ids)).sum() == 0\n",
    "\n",
    "    # Define model parameters\n",
    "    image_size = all_images.shape[1:-1]\n",
    "    print(image_size)\n",
    "\n",
    "    color_channels = all_images.shape[-1]\n",
    "    print(f\"Color channels: {color_channels}\")\n",
    "\n",
    "    # split the data into training and validation sets\n",
    "    split_ratio = split\n",
    "    split_index = int(split_ratio * len(all_positives))\n",
    "\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(all_positives)\n",
    "    np.random.shuffle(all_negatives)\n",
    "\n",
    "    train_positives = all_positives[:split_index]\n",
    "    val_positives = all_positives[split_index:]\n",
    "\n",
    "    train_negatives = all_negatives[:split_index]\n",
    "    val_negatives = all_negatives[split_index:]\n",
    "\n",
    "    # Define the model parameters\n",
    "    model_params = {\n",
    "        'image_size': image_size,\n",
    "        'color_channels': color_channels,\n",
    "        'embedding_size': 128\n",
    "    }\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.RandomAffine(degrees=0.05, translate=(0.05, 0.1), scale=None), #(0.99, 1.01)),\n",
    "        #transforms.RandomAutocontrast(),\n",
    "        #transforms.RandAugment(2, 9),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5] * model_params[\"color_channels\"], std=[0.5] * model_params[\"color_channels\"])\n",
    "    ])\n",
    "\n",
    "    val_dataset = SiameseDatasetFromArrays(\n",
    "        val_positives,\n",
    "        val_negatives,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_dataset = SiameseDatasetFromArrays(\n",
    "        train_positives,\n",
    "        train_negatives,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    return val_dataset, train_dataset, model_params\n",
    "\n",
    "#val_dataset, train_dataset = load_npz_as_dataset('/Users/tristan/Videos/data/group_1_tracklet_images_single_part0.npz', dont_use_these_ids=[5,3,1])\n",
    "val_dataset, train_dataset, model_params = load_npz_as_dataset('/Users/tristan/Downloads/For Tristan/data/ForTristan', dont_use_these_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in training set: 4764\n",
      "Number of classes in validation set: 1018\n",
      "Training on mps, with 3 color channels\n",
      "Starting epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] completed. Average Training Loss: 0.4820, Training Accuracy: 0.5248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 23.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Validation Loss: 0.4135, Validation Accuracy: 0.5855\n",
      "Validation loss decreased (0.4135). Saving model ...\n",
      "[Callback] Model saved at epoch 1\n",
      "[Callback] Epoch 1 ended.\n",
      "Training Loss: 0.4820, Training Accuracy: 0.5248\n",
      "Validation Loss: 0.4135, Validation Accuracy: 0.5855\n",
      "Starting epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] completed. Average Training Loss: 0.4989, Training Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 23.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] Validation Loss: 0.4155, Validation Accuracy: 0.5855\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 2 ended.\n",
      "Training Loss: 0.4989, Training Accuracy: 0.5000\n",
      "Validation Loss: 0.4155, Validation Accuracy: 0.5855\n",
      "Starting epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] completed. Average Training Loss: 0.4775, Training Accuracy: 0.5258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 23.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] Validation Loss: 0.4053, Validation Accuracy: 0.5923\n",
      "Validation loss decreased (0.4053). Saving model ...\n",
      "[Callback] Model saved at epoch 3\n",
      "[Callback] Epoch 3 ended.\n",
      "Training Loss: 0.4775, Training Accuracy: 0.5258\n",
      "Validation Loss: 0.4053, Validation Accuracy: 0.5923\n",
      "Starting epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] completed. Average Training Loss: 0.4741, Training Accuracy: 0.5321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] Validation Loss: 0.4106, Validation Accuracy: 0.5884\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 4 ended.\n",
      "Training Loss: 0.4741, Training Accuracy: 0.5321\n",
      "Validation Loss: 0.4106, Validation Accuracy: 0.5884\n",
      "Starting epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] completed. Average Training Loss: 0.4609, Training Accuracy: 0.5426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] Validation Loss: 0.3916, Validation Accuracy: 0.6169\n",
      "Validation loss decreased (0.3916). Saving model ...\n",
      "[Callback] Model saved at epoch 5\n",
      "[Callback] Epoch 5 ended.\n",
      "Training Loss: 0.4609, Training Accuracy: 0.5426\n",
      "Validation Loss: 0.3916, Validation Accuracy: 0.6169\n",
      "Starting epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] completed. Average Training Loss: 0.4234, Training Accuracy: 0.5854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] Validation Loss: 0.4225, Validation Accuracy: 0.6012\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 6 ended.\n",
      "Training Loss: 0.4234, Training Accuracy: 0.5854\n",
      "Validation Loss: 0.4225, Validation Accuracy: 0.6012\n",
      "Starting epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] completed. Average Training Loss: 0.4013, Training Accuracy: 0.6085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 23.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] Validation Loss: 0.4007, Validation Accuracy: 0.6208\n",
      "Epochs without improvement: 2/15\n",
      "[Callback] Epoch 7 ended.\n",
      "Training Loss: 0.4013, Training Accuracy: 0.6085\n",
      "Validation Loss: 0.4007, Validation Accuracy: 0.6208\n",
      "Starting epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] completed. Average Training Loss: 0.3840, Training Accuracy: 0.6291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] Validation Loss: 0.3842, Validation Accuracy: 0.6238\n",
      "Validation loss decreased (0.3842). Saving model ...\n",
      "[Callback] Model saved at epoch 8\n",
      "[Callback] Epoch 8 ended.\n",
      "Training Loss: 0.3840, Training Accuracy: 0.6291\n",
      "Validation Loss: 0.3842, Validation Accuracy: 0.6238\n",
      "Starting epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] completed. Average Training Loss: 0.3735, Training Accuracy: 0.6404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] Validation Loss: 0.3583, Validation Accuracy: 0.6611\n",
      "Validation loss decreased (0.3583). Saving model ...\n",
      "[Callback] Model saved at epoch 9\n",
      "[Callback] Epoch 9 ended.\n",
      "Training Loss: 0.3735, Training Accuracy: 0.6404\n",
      "Validation Loss: 0.3583, Validation Accuracy: 0.6611\n",
      "Starting epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] completed. Average Training Loss: 0.3648, Training Accuracy: 0.6513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] Validation Loss: 0.3460, Validation Accuracy: 0.6807\n",
      "Validation loss decreased (0.3460). Saving model ...\n",
      "[Callback] Model saved at epoch 10\n",
      "[Callback] Epoch 10 ended.\n",
      "Training Loss: 0.3648, Training Accuracy: 0.6513\n",
      "Validation Loss: 0.3460, Validation Accuracy: 0.6807\n",
      "Starting epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] completed. Average Training Loss: 0.3694, Training Accuracy: 0.6442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] Validation Loss: 0.3556, Validation Accuracy: 0.6572\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 11 ended.\n",
      "Training Loss: 0.3694, Training Accuracy: 0.6442\n",
      "Validation Loss: 0.3556, Validation Accuracy: 0.6572\n",
      "Starting epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] completed. Average Training Loss: 0.3500, Training Accuracy: 0.6669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] Validation Loss: 0.3366, Validation Accuracy: 0.6847\n",
      "Validation loss decreased (0.3366). Saving model ...\n",
      "[Callback] Model saved at epoch 12\n",
      "[Callback] Epoch 12 ended.\n",
      "Training Loss: 0.3500, Training Accuracy: 0.6669\n",
      "Validation Loss: 0.3366, Validation Accuracy: 0.6847\n",
      "Starting epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] completed. Average Training Loss: 0.3489, Training Accuracy: 0.6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] Validation Loss: 0.3117, Validation Accuracy: 0.7161\n",
      "Validation loss decreased (0.3117). Saving model ...\n",
      "[Callback] Model saved at epoch 13\n",
      "[Callback] Epoch 13 ended.\n",
      "Training Loss: 0.3489, Training Accuracy: 0.6700\n",
      "Validation Loss: 0.3117, Validation Accuracy: 0.7161\n",
      "Starting epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100] completed. Average Training Loss: 0.3352, Training Accuracy: 0.6816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 23.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100] Validation Loss: 0.2937, Validation Accuracy: 0.7358\n",
      "Validation loss decreased (0.2937). Saving model ...\n",
      "[Callback] Model saved at epoch 14\n",
      "[Callback] Epoch 14 ended.\n",
      "Training Loss: 0.3352, Training Accuracy: 0.6816\n",
      "Validation Loss: 0.2937, Validation Accuracy: 0.7358\n",
      "Starting epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100] completed. Average Training Loss: 0.3347, Training Accuracy: 0.6835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100] Validation Loss: 0.3422, Validation Accuracy: 0.6768\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 15 ended.\n",
      "Training Loss: 0.3347, Training Accuracy: 0.6835\n",
      "Validation Loss: 0.3422, Validation Accuracy: 0.6768\n",
      "Starting epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100] completed. Average Training Loss: 0.3292, Training Accuracy: 0.6908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100] Validation Loss: 0.3138, Validation Accuracy: 0.7151\n",
      "Epochs without improvement: 2/15\n",
      "[Callback] Epoch 16 ended.\n",
      "Training Loss: 0.3292, Training Accuracy: 0.6908\n",
      "Validation Loss: 0.3138, Validation Accuracy: 0.7151\n",
      "Starting epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] completed. Average Training Loss: 0.3233, Training Accuracy: 0.6979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] Validation Loss: 0.3192, Validation Accuracy: 0.6965\n",
      "Epochs without improvement: 3/15\n",
      "[Callback] Epoch 17 ended.\n",
      "Training Loss: 0.3233, Training Accuracy: 0.6979\n",
      "Validation Loss: 0.3192, Validation Accuracy: 0.6965\n",
      "Starting epoch 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100] completed. Average Training Loss: 0.2998, Training Accuracy: 0.7244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100] Validation Loss: 0.2741, Validation Accuracy: 0.7583\n",
      "Validation loss decreased (0.2741). Saving model ...\n",
      "[Callback] Model saved at epoch 18\n",
      "[Callback] Epoch 18 ended.\n",
      "Training Loss: 0.2998, Training Accuracy: 0.7244\n",
      "Validation Loss: 0.2741, Validation Accuracy: 0.7583\n",
      "Starting epoch 19/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100] completed. Average Training Loss: 0.2932, Training Accuracy: 0.7328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 23.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100] Validation Loss: 0.2744, Validation Accuracy: 0.7623\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 19 ended.\n",
      "Training Loss: 0.2932, Training Accuracy: 0.7328\n",
      "Validation Loss: 0.2744, Validation Accuracy: 0.7623\n",
      "Starting epoch 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100] completed. Average Training Loss: 0.2869, Training Accuracy: 0.7401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100] Validation Loss: 0.2700, Validation Accuracy: 0.7652\n",
      "Validation loss decreased (0.2700). Saving model ...\n",
      "[Callback] Model saved at epoch 20\n",
      "[Callback] Epoch 20 ended.\n",
      "Training Loss: 0.2869, Training Accuracy: 0.7401\n",
      "Validation Loss: 0.2700, Validation Accuracy: 0.7652\n",
      "Starting epoch 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100] completed. Average Training Loss: 0.2831, Training Accuracy: 0.7422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100] Validation Loss: 0.2672, Validation Accuracy: 0.7711\n",
      "Validation loss decreased (0.2672). Saving model ...\n",
      "[Callback] Model saved at epoch 21\n",
      "[Callback] Epoch 21 ended.\n",
      "Training Loss: 0.2831, Training Accuracy: 0.7422\n",
      "Validation Loss: 0.2672, Validation Accuracy: 0.7711\n",
      "Starting epoch 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100] completed. Average Training Loss: 0.2822, Training Accuracy: 0.7427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100] Validation Loss: 0.2774, Validation Accuracy: 0.7711\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 22 ended.\n",
      "Training Loss: 0.2822, Training Accuracy: 0.7427\n",
      "Validation Loss: 0.2774, Validation Accuracy: 0.7711\n",
      "Starting epoch 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100] completed. Average Training Loss: 0.2783, Training Accuracy: 0.7473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 20.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100] Validation Loss: 0.2618, Validation Accuracy: 0.7652\n",
      "Validation loss decreased (0.2618). Saving model ...\n",
      "[Callback] Model saved at epoch 23\n",
      "[Callback] Epoch 23 ended.\n",
      "Training Loss: 0.2783, Training Accuracy: 0.7473\n",
      "Validation Loss: 0.2618, Validation Accuracy: 0.7652\n",
      "Starting epoch 24/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100] completed. Average Training Loss: 0.2791, Training Accuracy: 0.7485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100] Validation Loss: 0.2642, Validation Accuracy: 0.7642\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 24 ended.\n",
      "Training Loss: 0.2791, Training Accuracy: 0.7485\n",
      "Validation Loss: 0.2642, Validation Accuracy: 0.7642\n",
      "Starting epoch 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100] completed. Average Training Loss: 0.2719, Training Accuracy: 0.7582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100] Validation Loss: 0.2536, Validation Accuracy: 0.7898\n",
      "Validation loss decreased (0.2536). Saving model ...\n",
      "[Callback] Model saved at epoch 25\n",
      "[Callback] Epoch 25 ended.\n",
      "Training Loss: 0.2719, Training Accuracy: 0.7582\n",
      "Validation Loss: 0.2536, Validation Accuracy: 0.7898\n",
      "Starting epoch 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100] completed. Average Training Loss: 0.2686, Training Accuracy: 0.7565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100] Validation Loss: 0.2550, Validation Accuracy: 0.7760\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 26 ended.\n",
      "Training Loss: 0.2686, Training Accuracy: 0.7565\n",
      "Validation Loss: 0.2550, Validation Accuracy: 0.7760\n",
      "Starting epoch 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100] completed. Average Training Loss: 0.2722, Training Accuracy: 0.7523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100] Validation Loss: 0.2578, Validation Accuracy: 0.7859\n",
      "Epochs without improvement: 2/15\n",
      "[Callback] Epoch 27 ended.\n",
      "Training Loss: 0.2722, Training Accuracy: 0.7523\n",
      "Validation Loss: 0.2578, Validation Accuracy: 0.7859\n",
      "Starting epoch 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100] completed. Average Training Loss: 0.2678, Training Accuracy: 0.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100] Validation Loss: 0.2729, Validation Accuracy: 0.7711\n",
      "Epochs without improvement: 3/15\n",
      "[Callback] Epoch 28 ended.\n",
      "Training Loss: 0.2678, Training Accuracy: 0.7580\n",
      "Validation Loss: 0.2729, Validation Accuracy: 0.7711\n",
      "Starting epoch 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100] completed. Average Training Loss: 0.2620, Training Accuracy: 0.7687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100] Validation Loss: 0.2490, Validation Accuracy: 0.7888\n",
      "Validation loss decreased (0.2490). Saving model ...\n",
      "[Callback] Model saved at epoch 29\n",
      "[Callback] Epoch 29 ended.\n",
      "Training Loss: 0.2620, Training Accuracy: 0.7687\n",
      "Validation Loss: 0.2490, Validation Accuracy: 0.7888\n",
      "Starting epoch 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100] completed. Average Training Loss: 0.2541, Training Accuracy: 0.7769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100] Validation Loss: 0.2455, Validation Accuracy: 0.7878\n",
      "Validation loss decreased (0.2455). Saving model ...\n",
      "[Callback] Model saved at epoch 30\n",
      "[Callback] Epoch 30 ended.\n",
      "Training Loss: 0.2541, Training Accuracy: 0.7769\n",
      "Validation Loss: 0.2455, Validation Accuracy: 0.7878\n",
      "Starting epoch 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100] completed. Average Training Loss: 0.2591, Training Accuracy: 0.7651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100] Validation Loss: 0.2516, Validation Accuracy: 0.7878\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 31 ended.\n",
      "Training Loss: 0.2591, Training Accuracy: 0.7651\n",
      "Validation Loss: 0.2516, Validation Accuracy: 0.7878\n",
      "Starting epoch 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100] completed. Average Training Loss: 0.2565, Training Accuracy: 0.7731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 20.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100] Validation Loss: 0.2482, Validation Accuracy: 0.7917\n",
      "Epochs without improvement: 2/15\n",
      "[Callback] Epoch 32 ended.\n",
      "Training Loss: 0.2565, Training Accuracy: 0.7731\n",
      "Validation Loss: 0.2482, Validation Accuracy: 0.7917\n",
      "Starting epoch 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100] completed. Average Training Loss: 0.2604, Training Accuracy: 0.7655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100] Validation Loss: 0.2453, Validation Accuracy: 0.7898\n",
      "Validation loss decreased (0.2453). Saving model ...\n",
      "[Callback] Model saved at epoch 33\n",
      "[Callback] Epoch 33 ended.\n",
      "Training Loss: 0.2604, Training Accuracy: 0.7655\n",
      "Validation Loss: 0.2453, Validation Accuracy: 0.7898\n",
      "Starting epoch 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100] completed. Average Training Loss: 0.2577, Training Accuracy: 0.7645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100] Validation Loss: 0.2532, Validation Accuracy: 0.7780\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 34 ended.\n",
      "Training Loss: 0.2577, Training Accuracy: 0.7645\n",
      "Validation Loss: 0.2532, Validation Accuracy: 0.7780\n",
      "Starting epoch 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100] completed. Average Training Loss: 0.2593, Training Accuracy: 0.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100] Validation Loss: 0.2480, Validation Accuracy: 0.7859\n",
      "Epochs without improvement: 2/15\n",
      "[Callback] Epoch 35 ended.\n",
      "Training Loss: 0.2593, Training Accuracy: 0.7716\n",
      "Validation Loss: 0.2480, Validation Accuracy: 0.7859\n",
      "Starting epoch 36/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100] completed. Average Training Loss: 0.2542, Training Accuracy: 0.7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100] Validation Loss: 0.2480, Validation Accuracy: 0.7898\n",
      "Epochs without improvement: 3/15\n",
      "[Callback] Epoch 36 ended.\n",
      "Training Loss: 0.2542, Training Accuracy: 0.7704\n",
      "Validation Loss: 0.2480, Validation Accuracy: 0.7898\n",
      "Starting epoch 37/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100] completed. Average Training Loss: 0.2598, Training Accuracy: 0.7670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100] Validation Loss: 0.2489, Validation Accuracy: 0.7888\n",
      "Epochs without improvement: 4/15\n",
      "[Callback] Epoch 37 ended.\n",
      "Training Loss: 0.2598, Training Accuracy: 0.7670\n",
      "Validation Loss: 0.2489, Validation Accuracy: 0.7888\n",
      "Starting epoch 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100] completed. Average Training Loss: 0.2590, Training Accuracy: 0.7674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100] Validation Loss: 0.2433, Validation Accuracy: 0.7976\n",
      "Validation loss decreased (0.2433). Saving model ...\n",
      "[Callback] Model saved at epoch 38\n",
      "[Callback] Epoch 38 ended.\n",
      "Training Loss: 0.2590, Training Accuracy: 0.7674\n",
      "Validation Loss: 0.2433, Validation Accuracy: 0.7976\n",
      "Starting epoch 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100] completed. Average Training Loss: 0.2562, Training Accuracy: 0.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100] Validation Loss: 0.2492, Validation Accuracy: 0.7849\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 39 ended.\n",
      "Training Loss: 0.2562, Training Accuracy: 0.7699\n",
      "Validation Loss: 0.2492, Validation Accuracy: 0.7849\n",
      "Starting epoch 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100] completed. Average Training Loss: 0.2538, Training Accuracy: 0.7714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100] Validation Loss: 0.2457, Validation Accuracy: 0.7859\n",
      "Epochs without improvement: 2/15\n",
      "[Callback] Epoch 40 ended.\n",
      "Training Loss: 0.2538, Training Accuracy: 0.7714\n",
      "Validation Loss: 0.2457, Validation Accuracy: 0.7859\n",
      "Starting epoch 41/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100] completed. Average Training Loss: 0.2540, Training Accuracy: 0.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100] Validation Loss: 0.2445, Validation Accuracy: 0.7800\n",
      "Epochs without improvement: 3/15\n",
      "[Callback] Epoch 41 ended.\n",
      "Training Loss: 0.2540, Training Accuracy: 0.7754\n",
      "Validation Loss: 0.2445, Validation Accuracy: 0.7800\n",
      "Starting epoch 42/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100] completed. Average Training Loss: 0.2568, Training Accuracy: 0.7727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100] Validation Loss: 0.2464, Validation Accuracy: 0.7947\n",
      "Epochs without improvement: 4/15\n",
      "[Callback] Epoch 42 ended.\n",
      "Training Loss: 0.2568, Training Accuracy: 0.7727\n",
      "Validation Loss: 0.2464, Validation Accuracy: 0.7947\n",
      "Starting epoch 43/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100] completed. Average Training Loss: 0.2589, Training Accuracy: 0.7689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100] Validation Loss: 0.2401, Validation Accuracy: 0.7986\n",
      "Validation loss decreased (0.2401). Saving model ...\n",
      "[Callback] Model saved at epoch 43\n",
      "[Callback] Epoch 43 ended.\n",
      "Training Loss: 0.2589, Training Accuracy: 0.7689\n",
      "Validation Loss: 0.2401, Validation Accuracy: 0.7986\n",
      "Starting epoch 44/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100] completed. Average Training Loss: 0.2520, Training Accuracy: 0.7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100] Validation Loss: 0.2471, Validation Accuracy: 0.7809\n",
      "Epochs without improvement: 1/15\n",
      "[Callback] Epoch 44 ended.\n",
      "Training Loss: 0.2520, Training Accuracy: 0.7704\n",
      "Validation Loss: 0.2471, Validation Accuracy: 0.7809\n",
      "Starting epoch 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100] completed. Average Training Loss: 0.2534, Training Accuracy: 0.7743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100] Validation Loss: 0.2516, Validation Accuracy: 0.7859\n",
      "Epochs without improvement: 2/15\n",
      "[Callback] Epoch 45 ended.\n",
      "Training Loss: 0.2534, Training Accuracy: 0.7743\n",
      "Validation Loss: 0.2516, Validation Accuracy: 0.7859\n",
      "Starting epoch 46/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100] completed. Average Training Loss: 0.2583, Training Accuracy: 0.7674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100] Validation Loss: 0.2531, Validation Accuracy: 0.7819\n",
      "Epochs without improvement: 3/15\n",
      "[Callback] Epoch 46 ended.\n",
      "Training Loss: 0.2583, Training Accuracy: 0.7674\n",
      "Validation Loss: 0.2531, Validation Accuracy: 0.7819\n",
      "Starting epoch 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100] completed. Average Training Loss: 0.2520, Training Accuracy: 0.7720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100] Validation Loss: 0.2467, Validation Accuracy: 0.8006\n",
      "Epochs without improvement: 4/15\n",
      "[Callback] Epoch 47 ended.\n",
      "Training Loss: 0.2520, Training Accuracy: 0.7720\n",
      "Validation Loss: 0.2467, Validation Accuracy: 0.8006\n",
      "Starting epoch 48/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100] completed. Average Training Loss: 0.2550, Training Accuracy: 0.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100] Validation Loss: 0.2472, Validation Accuracy: 0.7917\n",
      "Epochs without improvement: 5/15\n",
      "[Callback] Epoch 48 ended.\n",
      "Training Loss: 0.2550, Training Accuracy: 0.7699\n",
      "Validation Loss: 0.2472, Validation Accuracy: 0.7917\n",
      "Starting epoch 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100] completed. Average Training Loss: 0.2552, Training Accuracy: 0.7725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100] Validation Loss: 0.2466, Validation Accuracy: 0.7898\n",
      "Epochs without improvement: 6/15\n",
      "[Callback] Epoch 49 ended.\n",
      "Training Loss: 0.2552, Training Accuracy: 0.7725\n",
      "Validation Loss: 0.2466, Validation Accuracy: 0.7898\n",
      "Starting epoch 50/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100] completed. Average Training Loss: 0.2526, Training Accuracy: 0.7800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100] Validation Loss: 0.2549, Validation Accuracy: 0.7731\n",
      "Epochs without improvement: 7/15\n",
      "[Callback] Epoch 50 ended.\n",
      "Training Loss: 0.2526, Training Accuracy: 0.7800\n",
      "Validation Loss: 0.2549, Validation Accuracy: 0.7731\n",
      "Starting epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100] completed. Average Training Loss: 0.2565, Training Accuracy: 0.7683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100] Validation Loss: 0.2455, Validation Accuracy: 0.7937\n",
      "Epochs without improvement: 8/15\n",
      "[Callback] Epoch 51 ended.\n",
      "Training Loss: 0.2565, Training Accuracy: 0.7683\n",
      "Validation Loss: 0.2455, Validation Accuracy: 0.7937\n",
      "Starting epoch 52/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100] completed. Average Training Loss: 0.2550, Training Accuracy: 0.7718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 21.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100] Validation Loss: 0.2462, Validation Accuracy: 0.7898\n",
      "Epochs without improvement: 9/15\n",
      "[Callback] Epoch 52 ended.\n",
      "Training Loss: 0.2550, Training Accuracy: 0.7718\n",
      "Validation Loss: 0.2462, Validation Accuracy: 0.7898\n",
      "Starting epoch 53/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100] completed. Average Training Loss: 0.2566, Training Accuracy: 0.7714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100] Validation Loss: 0.2449, Validation Accuracy: 0.7917\n",
      "Epochs without improvement: 10/15\n",
      "[Callback] Epoch 53 ended.\n",
      "Training Loss: 0.2566, Training Accuracy: 0.7714\n",
      "Validation Loss: 0.2449, Validation Accuracy: 0.7917\n",
      "Starting epoch 54/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100] completed. Average Training Loss: 0.2547, Training Accuracy: 0.7739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 19.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100] Validation Loss: 0.2438, Validation Accuracy: 0.7819\n",
      "Epochs without improvement: 11/15\n",
      "[Callback] Epoch 54 ended.\n",
      "Training Loss: 0.2547, Training Accuracy: 0.7739\n",
      "Validation Loss: 0.2438, Validation Accuracy: 0.7819\n",
      "Starting epoch 55/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100] completed. Average Training Loss: 0.2566, Training Accuracy: 0.7729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100] Validation Loss: 0.2479, Validation Accuracy: 0.7898\n",
      "Epochs without improvement: 12/15\n",
      "[Callback] Epoch 55 ended.\n",
      "Training Loss: 0.2566, Training Accuracy: 0.7729\n",
      "Validation Loss: 0.2479, Validation Accuracy: 0.7898\n",
      "Starting epoch 56/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100] completed. Average Training Loss: 0.2546, Training Accuracy: 0.7731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 22.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100] Validation Loss: 0.2461, Validation Accuracy: 0.7819\n",
      "Epochs without improvement: 13/15\n",
      "[Callback] Epoch 56 ended.\n",
      "Training Loss: 0.2546, Training Accuracy: 0.7731\n",
      "Validation Loss: 0.2461, Validation Accuracy: 0.7819\n",
      "Starting epoch 57/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:07<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100] completed. Average Training Loss: 0.2564, Training Accuracy: 0.7758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100] Validation Loss: 0.2455, Validation Accuracy: 0.7957\n",
      "Epochs without improvement: 14/15\n",
      "[Callback] Epoch 57 ended.\n",
      "Training Loss: 0.2564, Training Accuracy: 0.7758\n",
      "Validation Loss: 0.2455, Validation Accuracy: 0.7957\n",
      "Starting epoch 58/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:08<00:00,  9.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100] completed. Average Training Loss: 0.2540, Training Accuracy: 0.7743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:00<00:00, 17.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100] Validation Loss: 0.2537, Validation Accuracy: 0.7819\n",
      "Epochs without improvement: 15/15\n",
      "Early stopping triggered.\n",
      "Best model restored.\n",
      "[Callback] Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the training parameters\n",
    "training_params = {\n",
    "    'num_epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.001,\n",
    "    'patience': 15,\n",
    "    #'margin': 2.0,\n",
    "    'threshold': 0.5\n",
    "}\n",
    "\n",
    "# Check the number of classes and images\n",
    "print(f\"Number of classes in training set: {len(train_dataset)}\")\n",
    "print(f\"Number of classes in validation set: {len(val_dataset)}\")\n",
    "\n",
    "# Define training dataset parameters\n",
    "train_dataset_params = {\n",
    "    'dataset_class': 'dataset',\n",
    "    'data': train_dataset  # The image dataset built earlier\n",
    "}\n",
    "\n",
    "# Define validation dataset parameters\n",
    "val_dataset_params = {\n",
    "    'dataset_class': 'dataset',\n",
    "    'data': val_dataset  # The image dataset built earlier\n",
    "}\n",
    "#'''\n",
    "\n",
    "\n",
    "# Run the training\n",
    "trained_model = SiameseTrainer.train_model(\n",
    "    model_params,\n",
    "    train_dataset_params,\n",
    "    val_dataset_params,\n",
    "    training_params,\n",
    "    callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m evaluate_model(trained_model, \u001b[43mtrain_loader\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "predictor = SiamesePredictor(\n",
    "    model_path=trained_model,\n",
    "    model_params=model_params,\n",
    "    #transform=trained_model.transform,\n",
    "    threshold=training_params.get('threshold', 1.0)\n",
    ")\n",
    "\n",
    "# Define a function to evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels in tqdm(test_loader):\n",
    "            #print(img1.shape, img2.shape, labels.shape)\n",
    "            #print(f\"single input: {predictor._is_single_image(img1)} for {img1.shape}\")\n",
    "            distance, prediction = predictor.predict(img1, img2)\n",
    "\n",
    "            #print(f\"Euclidean Distance: {distance:.4f}\")\n",
    "            #print(f\"Prediction: {'Similar' if prediction == 0 else 'Dissimilar'}\")\n",
    "\n",
    "            # Compare predictions with labels\n",
    "            correct += (prediction == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(trained_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_difference [ 0  2  4  6  7  8  9 10 11 12 13 14 15 16]\n",
      "['/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part0.npz', '/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part1.npz', '/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part2.npz', '/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part3.npz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:01<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288614, 80, 80, 3) (288614,) (288614,) (288614,) (12253, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 18.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80)\n",
      "Color channels: 3\n",
      "Number of classes in training set: 2333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 37/37 [00:02<00:00, 17.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.39744584141550837, 0.8872696099442777)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "previous_dont_use_ids = [5,3,1]\n",
    "_all_ids  = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "set_difference = np.setxor1d(previous_dont_use_ids, _all_ids)\n",
    "print(\"set_difference\", set_difference)\n",
    "#_train_dataset, _ = load_npz_as_dataset('/Users/tristan/Videos/data/group_1_tracklet_images_single_part0.npz', dont_use_these_ids=set_difference, split=0.0)\n",
    "_train_dataset, _, _ = load_npz_as_dataset('/Users/tristan/Downloads/For Tristan/data/ForTristan', dont_use_these_ids=set_difference, split=0.0)\n",
    "print(f\"Number of classes in training set: {len(_train_dataset)}\")\n",
    "margin = training_params.get('margin', 2.0)\n",
    "criterion = ContrastiveLoss(margin=margin)\n",
    "threshold = training_params.get('threshold', margin / 2.0)\n",
    "\n",
    "#_val_loader = DataLoader(_val_dataset, shuffle=False, batch_size=64, num_workers=0)\n",
    "_train_loader = DataLoader(_train_dataset, shuffle=False, batch_size=64, num_workers=0)\n",
    "\n",
    "#print(SiameseTrainer.validate_model(trained_model, _val_loader, criterion, 'mps', threshold))\n",
    "print(SiameseTrainer.validate_model(trained_model, _train_loader, criterion, 'mps', threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part0.npz', '/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part1.npz', '/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part2.npz', '/Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part3.npz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images', 'dimensions', 'positions', 'frame_segment_indexes', 'frame_segments', 'frames', 'ids', 'encoding']\n",
      "(78125,)\n",
      "(78125, 80, 80, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 1/4 [00:01<00:04,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part0.npz: (78125, 80, 80, 3) 0 (78125,)\n",
      "['images', 'dimensions', 'positions', 'frame_segment_indexes', 'frame_segments', 'frames', 'ids', 'encoding']\n",
      "(78125,)\n",
      "(78125, 80, 80, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 2/4 [00:03<00:03,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part1.npz: (78125, 80, 80, 3) 6890 (78125,)\n",
      "['images', 'dimensions', 'positions', 'frame_segment_indexes', 'frame_segments', 'frames', 'ids', 'encoding']\n",
      "(78125,)\n",
      "(78125, 80, 80, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 3/4 [00:04<00:01,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part2.npz: (78125, 80, 80, 3) 14336 (78125,)\n",
      "['images', 'dimensions', 'positions', 'frame_segment_indexes', 'frame_segments', 'frames', 'ids', 'encoding']\n",
      "(54239,)\n",
      "(54239, 80, 80, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:05<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /Users/tristan/Downloads/For Tristan/data/ForTristan_tracklet_images_single_part3.npz: (54239, 80, 80, 3) 19902 (54239,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "OpenCV: FFMPEG: tag 0x34363248/'H264' is not supported with codec id 27 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x31637661/'avc1'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288614,) (288614,) (12253, 2) (288614,) (288614, 80, 80, 3)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(19508, 80, 80, 3) (19508,)\n",
      "Generating movie: ForTristan_7_10.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 19508/19508 [01:25<00:00, 229.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating movie: ForTristan_7_10.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "output_images_npz = \"/Users/tristan/Downloads/For Tristan/data/ForTristan\"\n",
    "video_base_name = output_images_npz.split(\"/\")[-1]\n",
    "#with np.load(\"/Users/tristan/Videos/data/group_1_tracklet_images_single_part0.npz\") as npz:\n",
    "\n",
    "result = concatenate_npzs(output_images_npz, verbose=True)\n",
    "images, frames, ids, frame_segment_indexes, frame_segments = result\n",
    "\n",
    "print(frames.shape, ids.shape, frame_segments.shape, frame_segment_indexes.shape, images.shape)\n",
    "print(np.unique(ids))\n",
    "\n",
    "def generate_movie(id, other=None):\n",
    "    mask = ids == id\n",
    "    other_mask = ids == other if other is not None else None\n",
    "    \n",
    "    predictor = SiamesePredictor(\n",
    "        model_path=trained_model,\n",
    "        model_params=model_params,\n",
    "        threshold=training_params.get('threshold', 1.0)\n",
    "    )\n",
    "\n",
    "    sub_images = images[mask]\n",
    "    sub_frames = frames[mask]\n",
    "\n",
    "    other_images = None\n",
    "    other_frames = None\n",
    "    if other_mask is not None:\n",
    "        other_images = images[other_mask]\n",
    "        other_frames = frames[other_mask]\n",
    "\n",
    "    # sort the images by frame\n",
    "    sort_index = np.argsort(sub_frames)\n",
    "    sub_images = sub_images[sort_index]\n",
    "    sub_frames = sub_frames[sort_index]\n",
    "\n",
    "    print(sub_images.shape, sub_frames.shape)\n",
    "\n",
    "    name = f'{video_base_name}_{id}.mp4' if other is None else f'{video_base_name}_{id}_{other}.mp4'\n",
    "    print(f\"Generating movie: {name}\")\n",
    "    cap = cv.VideoWriter(name, cv.VideoWriter_fourcc(*'H264'), 30, (80 * 2, 80))\n",
    "    previous_image = None\n",
    "    for frame, img in tqdm(zip(sub_frames, sub_images), total=len(sub_frames)):\n",
    "        similar = None\n",
    "        corresponding_image = None\n",
    "        \n",
    "        if other_images is None:\n",
    "            similar = True\n",
    "\n",
    "            if previous_image is not None:\n",
    "                distance, prediction = predictor.predict(previous_image, img)\n",
    "                if prediction == 1:\n",
    "                    similar = False\n",
    "                #print(f\"Euclidean Distance: {distance:.4f}\")\n",
    "                #print(f\"Prediction: {'Similar' if prediction == 0 else 'Dissimilar'}\")\n",
    "            previous_image = img\n",
    "        else:\n",
    "            # find the corresponding image that is closest to the current frame\n",
    "            min_frame_idx = np.argmin(np.abs(other_frames - frame))\n",
    "            corresponding_image = other_images[min_frame_idx]\n",
    "            other_frame = other_frames[min_frame_idx]\n",
    "\n",
    "            distance, prediction = predictor.predict(img, corresponding_image)\n",
    "            if prediction == 1:\n",
    "                similar = False\n",
    "            else:\n",
    "                similar = True\n",
    "\n",
    "            assert corresponding_image is not None\n",
    "        \n",
    "        if img.shape[-1] == 1:\n",
    "            img = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n",
    "        output = np.zeros((80, 80 * 2, 3), dtype=np.uint8)\n",
    "        output[:, :80] = img\n",
    "        if other_images is not None:\n",
    "            if corresponding_image.shape[-1] == 1:\n",
    "                corresponding_image = cv.cvtColor(corresponding_image, cv.COLOR_GRAY2BGR)\n",
    "            output[:, 80:] = corresponding_image\n",
    "        elif previous_image is not None:\n",
    "            if previous_image.shape[-1] == 1:\n",
    "                previous_image = cv.cvtColor(previous_image, cv.COLOR_GRAY2BGR)\n",
    "            output[:, 80:] = previous_image\n",
    "\n",
    "        cv.putText(output, f\"{id} {frame}\", (10, 10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "        if other_images is not None:\n",
    "            cv.putText(output, f\"{other} ({other_frame})\", (90, 10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "        else:\n",
    "            cv.putText(output, f\"previous frame\", (90, 10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "\n",
    "        if similar is None:\n",
    "            cv.putText(output, \"Unknown\", (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "            cv.rectangle(output, (0, 0), (80, 80), (255, 255, 255), 1)\n",
    "        elif similar:\n",
    "            cv.putText(output, \"Similar\", (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv.LINE_AA)\n",
    "            cv.rectangle(output, (0, 0), (80, 80), (0, 255, 0), 1)\n",
    "        else:\n",
    "            cv.putText(output, \"Dissimilar\", (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1, cv.LINE_AA)\n",
    "            cv.rectangle(output, (0, 0), (80, 80), (0, 0, 255), 1)\n",
    "\n",
    "        cap.write(output)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    print(f\"Finished generating movie: {name}\")\n",
    "\n",
    "generate_movie(7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: label: 0.0, id1: 9, frame1: 18478, id2: 9, frame2: 15489\n",
      "negative: label: 1.0, id1: 5, frame1: 2650, id2: 3, frame2: 2650\n",
      "negative: label: 1.0, id1: 5, frame1: 19266, id2: 9, frame2: 19266\n",
      "negative: label: 1.0, id1: 7, frame1: 6511, id2: 5, frame2: 6511\n",
      "negative: label: 1.0, id1: 0, frame1: 1246, id2: 2, frame2: 1246\n",
      "negative: label: 1.0, id1: 8, frame1: 9821, id2: 5, frame2: 9821\n",
      "negative: label: 1.0, id1: 3, frame1: 6221, id2: 4, frame2: 6221\n",
      "negative: label: 1.0, id1: 3, frame1: 2151, id2: 0, frame2: 2151\n",
      "negative: label: 1.0, id1: 5, frame1: 11327, id2: 9, frame2: 11327\n",
      "negative: label: 1.0, id1: 5, frame1: 445, id2: 9, frame2: 445\n",
      "negative: label: 1.0, id1: 0, frame1: 17050, id2: 7, frame2: 17050\n"
     ]
    }
   ],
   "source": [
    "for i, (label, id1, frame1, img1, id2, frame2, img2) in enumerate(all_positives):\n",
    "    print(f\"positive: label: {label}, id1: {id1}, frame1: {frame1}, id2: {id2}, frame2: {frame2}\")\n",
    "    break\n",
    "\n",
    "examples = [all_negatives[n] for n in np.random.randint(0, len(all_negatives), 10)]\n",
    "for i, (label, id1, frame1, img1, id2, frame2, img2) in enumerate(examples):\n",
    "    print(f\"negative: label: {label}, id1: {id1}, frame1: {frame1}, id2: {id2}, frame2: {frame2}\")\n",
    "    #break\n",
    "#print(f\"all_positives: {all_positives[0]}\")\n",
    "#print(f\"all_negatives: {np.shape(all_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the generated dataset into train and val folders\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get all the folders\n",
    "ids = glob(os.path.join(root, \"*\"))\n",
    "\n",
    "# create the train and val folders\n",
    "os.makedirs(os.path.join(root, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(root, 'val'), exist_ok=True)\n",
    "\n",
    "# split the ids into train and val\n",
    "train_ids, val_ids = train_test_split(ids, test_size=0.2)\n",
    "\n",
    "# move the folders to the correct location\n",
    "for folder in train_ids:\n",
    "    os.rename(folder, os.path.join(root, 'train', os.path.basename(folder)))\n",
    "\n",
    "for folder in val_ids:\n",
    "    os.rename(folder, os.path.join(root, 'val', os.path.basename(folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save_model(\"trained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetworkPyTorch(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): AdaptiveAvgPool2d(output_size=1)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = SiameseNetworkPyTorch.load_trained_model(\"trained_model.pt\", image_size=(80, 80), color_channels=1, embedding_size=128)\n",
    "test_model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 86/86 [00:02<00:00, 29.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.42423157289970753, 0.8795379537953796)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(SiameseTrainer.validate_model(test_model, _train_loader, criterion, 'mps', threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
