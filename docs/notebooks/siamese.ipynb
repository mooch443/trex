{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from matplotlib import backends\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiameseNetworkBase(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, image_size, color_channels, embedding_size):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_once(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, input1, input2):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_embedding(self, x):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model(self, filepath):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self, filepath):\n",
    "        pass\n",
    "\n",
    "class SiameseNetworkPyTorch(SiameseNetworkBase, nn.Module):\n",
    "    def __init__(self, image_size=(105, 105), color_channels=1, embedding_size=128):\n",
    "        nn.Module.__init__(self)\n",
    "        SiameseNetworkBase.__init__(self, image_size, color_channels, embedding_size)\n",
    "        self.image_size = image_size\n",
    "        self.color_channels = color_channels\n",
    "        self.embedding_size = embedding_size\n",
    "        self.transform = None\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.color_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # Calculate the size after convolutional layers to define the first linear layer\n",
    "        self._to_linear = self._calculate_conv_output_size()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, self.embedding_size)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _calculate_conv_output_size(self):\n",
    "        # Dummy input to calculate the size\n",
    "        with torch.no_grad():\n",
    "            #print(f\"Calculating the size after convolutional layers... (input size: {self.color_channels}x{self.image_size[0]}x{self.image_size[1]})\")\n",
    "            x = torch.zeros(1, self.color_channels, *self.image_size)\n",
    "            x = self.conv_layers(x)\n",
    "            return x.view(1, -1).size(1)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size()[0], -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n",
    "    @staticmethod\n",
    "    def load_trained_model(filepath, image_size=(105, 105), color_channels=3, embedding_size=128):\n",
    "        \"\"\"\n",
    "        Loads a trained Siamese network from a saved state dict.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the saved model file.\n",
    "            image_size (tuple): Size of the input images.\n",
    "            color_channels (int): Number of color channels.\n",
    "            embedding_size (int): Size of the embedding vector.\n",
    "\n",
    "        Returns:\n",
    "            SiameseNetworkPyTorch: An instance of the loaded model.\n",
    "        \"\"\"\n",
    "        model = SiameseNetworkPyTorch(\n",
    "            image_size=image_size,\n",
    "            color_channels=color_channels,\n",
    "            embedding_size=embedding_size\n",
    "        )\n",
    "        model.load_model(filepath)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def predict(self, img1, img2, transform, threshold):\n",
    "        \"\"\"\n",
    "        Predicts whether two images are similar or dissimilar.\n",
    "\n",
    "        Args:\n",
    "            img1 (PIL.Image or str): First image or path to the image.\n",
    "            img2 (PIL.Image or str): Second image or path to the image.\n",
    "            transform (callable): Transformations to apply to the images.\n",
    "            threshold (float): Threshold for determining similarity.\n",
    "\n",
    "        Returns:\n",
    "            float: The Euclidean distance between the embeddings.\n",
    "            int: Prediction label (0 for similar, 1 for dissimilar).\n",
    "        \"\"\"\n",
    "        # Load images if paths are provided\n",
    "        if isinstance(img1, str):\n",
    "            img1 = Image.open(img1).convert('RGB')\n",
    "        if isinstance(img2, str):\n",
    "            img2 = Image.open(img2).convert('RGB')\n",
    "\n",
    "        # Apply transformations\n",
    "        img1 = transform(img1).unsqueeze(0)  # Add batch dimension\n",
    "        img2 = transform(img2).unsqueeze(0)\n",
    "\n",
    "        # Move to device\n",
    "        device = next(self.parameters()).device\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            output1 = self.forward_once(img1)\n",
    "            output2 = self.forward_once(img2)\n",
    "\n",
    "        # Compute Euclidean distance\n",
    "        #euclidean_distance = F.pairwise_distance(output1, output2).item()\n",
    "        cosine_similarity = F.cosine_similarity(output1, output2).item()\n",
    "\n",
    "        # Make prediction based on threshold\n",
    "        #prediction = 1 if euclidean_distance > threshold else -1\n",
    "        prediction = 1.0 if cosine_similarity >= threshold else -1.0\n",
    "\n",
    "        #return euclidean_distance, prediction\n",
    "        return cosine_similarity, prediction\n",
    "\n",
    "    def compute_embedding(self, img, transform):\n",
    "        \"\"\"\n",
    "        Computes the embedding for a single image.\n",
    "\n",
    "        Args:\n",
    "            img (PIL.Image or str): Image or path to the image.\n",
    "            transform (callable): Transformations to apply to the image.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Embedding vector.\n",
    "        \"\"\"\n",
    "        # Load image if path is provided\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img).convert('RGB')\n",
    "\n",
    "        # Apply transformations\n",
    "        img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Move to device\n",
    "        device = next(self.parameters()).device\n",
    "        img = img.to(device)\n",
    "\n",
    "        # Compute embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = self.forward_once(img)\n",
    "\n",
    "        return embedding.cpu().numpy().flatten()\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        self.load_state_dict(torch.load(filepath))\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize weights using Xavier initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SiameseDatasetBase(Dataset, ABC):\n",
    "    def __init__(self, transform=None, callbacks=None):\n",
    "        \"\"\"\n",
    "        Base class for Siamese datasets.\n",
    "\n",
    "        Args:\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            callbacks (dict, optional): Dictionary of callback functions for data retrieval.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.callbacks = callbacks or {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class SiameseDatasetFromPaths(SiameseDatasetBase):\n",
    "    def __init__(self, image_folder_dataset, transform=None, num_pairs=10000):\n",
    "        super().__init__(transform=transform)\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.classes = list(self.image_folder_dataset.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "        # Precompute pairs\n",
    "        self.pairs = []\n",
    "        for _ in range(num_pairs):\n",
    "            should_get_same_class = random.randint(0, 1)\n",
    "            class1 = random.choice(self.classes)\n",
    "            img1_path = random.choice(self.image_folder_dataset[class1])\n",
    "\n",
    "            if should_get_same_class or len(self.classes) < 2:\n",
    "                # Positive pair\n",
    "                class2 = class1\n",
    "                label = 1.0\n",
    "            else:\n",
    "                # Negative pair\n",
    "                class2 = random.choice(self.classes)\n",
    "                while class1 == class2:\n",
    "                    class2 = random.choice(self.classes)\n",
    "                label = -1.0\n",
    "\n",
    "            img2_path = random.choice(self.image_folder_dataset[class2])\n",
    "            self.pairs.append((img1_path, img2_path, label))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1_path, img2_path, label = self.pairs[index]\n",
    "\n",
    "        img1 = Image.open(img1_path).convert('L')\n",
    "        img2 = Image.open(img2_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "def build_image_dataset_from_directory(root_dir):\n",
    "    \"\"\"\n",
    "    Builds a dictionary where each key is a class label and the value is a list of image paths.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Path to the root directory ('train' or 'val').\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with class labels as keys and lists of image paths as values.\n",
    "    \"\"\"\n",
    "    image_dataset = {}\n",
    "    classes = os.listdir(root_dir)\n",
    "    for cls in classes:\n",
    "        cls_path = os.path.join(root_dir, cls)\n",
    "        if os.path.isdir(cls_path):\n",
    "            images = [os.path.join(cls_path, img) for img in os.listdir(cls_path)\n",
    "                      if img.lower().endswith(('jpg', 'png', 'jpeg'))]\n",
    "            if images:\n",
    "                image_dataset[cls] = images\n",
    "    return image_dataset\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class SiameseDatasetFromCpp(SiameseDatasetBase):\n",
    "    def __init__(self, transform=None, callbacks=None):\n",
    "        super().__init__(transform=transform, callbacks=callbacks)\n",
    "\n",
    "        if 'get_data' not in self.callbacks:\n",
    "            raise ValueError(\"Callback 'get_data' must be provided for SiameseDatasetFromCpp\")\n",
    "\n",
    "        # Optionally, initialize any required state or connection here\n",
    "        # For example, establish a connection to the C++ application if needed\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves a data sample from the C++ application via the 'get_data' callback.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the sample. Since data is provided via callback, index can be ignored.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, Tensor]: Tuple containing the two images and the label.\n",
    "        \"\"\"\n",
    "        # Fetch image data and label from the C++ application via the 'get_data' callback\n",
    "        img1_data, img2_data, label = self.callbacks['get_data']()\n",
    "\n",
    "        # Convert raw data to PIL Images\n",
    "        img1 = Image.fromarray(img1_data).convert('RGB')\n",
    "        img2 = Image.fromarray(img2_data).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        label_tensor = torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "        return img1, img2, label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Since data is provided on-demand from the C++ application, we can set an arbitrary large number\n",
    "        or manage the length via a callback if needed.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        if 'get_length' in self.callbacks:\n",
    "            return self.callbacks['get_length']()\n",
    "        else:\n",
    "            # Return a large number to simulate an infinite dataset\n",
    "            return 1000000\n",
    "    \n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "            (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
    "        )\n",
    "        return loss_contrastive\n",
    "    \n",
    "# siamese_trainer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import or define your model and dataset classes here\n",
    "# from siamese_network import SiameseNetworkPyTorch\n",
    "# from contrastive_loss import ContrastiveLoss\n",
    "# from siamese_dataset import SiameseDatasetFromPaths, SiameseDatasetFromCpp\n",
    "\n",
    "class SiameseTrainer:\n",
    "    @staticmethod\n",
    "    def train_model(\n",
    "        model_params,\n",
    "        train_dataset_params,\n",
    "        val_dataset_params,\n",
    "        training_params,\n",
    "        callbacks\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Static method to train the Siamese network.\n",
    "\n",
    "        Args:\n",
    "            model_params (dict): Parameters for model initialization.\n",
    "            train_dataset_params (dict): Parameters for the training dataset.\n",
    "            val_dataset_params (dict): Parameters for the validation dataset.\n",
    "            training_params (dict): Training parameters (epochs, batch_size, learning_rate, etc.).\n",
    "            callbacks (dict): Dictionary of callback functions.\n",
    "        \"\"\"\n",
    "        # Unpack model parameters\n",
    "        image_size = model_params.get('image_size', (105, 105))\n",
    "        color_channels = model_params.get('color_channels', 3)\n",
    "        embedding_size = model_params.get('embedding_size', 128)\n",
    "\n",
    "        # Unpack training parameters\n",
    "        num_epochs = training_params.get('num_epochs', 10)\n",
    "        batch_size = training_params.get('batch_size', 32)\n",
    "        learning_rate = training_params.get('learning_rate', 0.0005)\n",
    "        max_batches = training_params.get('max_batches', None)\n",
    "        margin = training_params.get('margin', 2.0)\n",
    "        threshold = training_params.get('threshold', margin / 2.0)\n",
    "        patience = training_params.get('patience', 5)  # For early stopping\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "        # Define transformations\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5]*color_channels, std=[0.5]*color_channels)\n",
    "        ])\n",
    "\n",
    "        # Initialize the training dataset\n",
    "        train_dataset_class = train_dataset_params.get('dataset_class', 'paths')\n",
    "        if train_dataset_class == 'dataset':\n",
    "            train_dataset = train_dataset_params.get('data')\n",
    "        elif train_dataset_class == 'paths':\n",
    "            train_dataset = SiameseDatasetFromPaths(train_dataset_params.get('data'), transform=transform)\n",
    "        elif train_dataset_class == 'cpp':\n",
    "            train_dataset = SiameseDatasetFromCpp(transform=transform, callbacks=callbacks)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid train dataset_class specified.\")\n",
    "\n",
    "        # Initialize the validation dataset\n",
    "        val_dataset_class = val_dataset_params.get('dataset_class', 'paths')\n",
    "        if val_dataset_class == 'dataset':\n",
    "            val_dataset = val_dataset_params.get('data')\n",
    "        elif val_dataset_class == 'paths':\n",
    "            val_dataset = SiameseDatasetFromPaths(val_dataset_params.get('data'), transform=transform)\n",
    "        elif val_dataset_class == 'cpp':\n",
    "            val_dataset = SiameseDatasetFromCpp(transform=transform, callbacks=callbacks)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid val dataset_class specified.\")\n",
    "\n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "        val_loader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "        # Initialize the model\n",
    "        print(f\"Training on {device}, with {color_channels} color channels\")\n",
    "        model = SiameseNetworkPyTorch(\n",
    "            image_size=image_size,\n",
    "            color_channels=color_channels,\n",
    "            embedding_size=embedding_size\n",
    "        )\n",
    "        model.transform = transform  # Store the transform for later use\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        #criterion = ContrastiveLoss(margin=margin)\n",
    "        criterion = torch.nn.CosineEmbeddingLoss()#margin=margin)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Learning Rate Scheduler\n",
    "        from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "        # Early stopping parameters\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        early_stop = False\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            if early_stop:\n",
    "                print(\"Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            print(f\"Starting epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "            for batch_idx, (img1, img2, label) in enumerate(tqdm(train_loader)):\n",
    "                if max_batches and batch_idx >= max_batches:\n",
    "                    break  # Limit the number of batches if desired\n",
    "\n",
    "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                output1, output2 = model(img1, img2)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(output1, output2, label)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # Compute distances\n",
    "                #euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                cosine_similarity = F.cosine_similarity(output1, output2)\n",
    "\n",
    "                # Make predictions based on threshold\n",
    "                pred = torch.where(cosine_similarity >= threshold, torch.tensor(1.0).to(device), torch.tensor(-1.0).to(device))\n",
    "\n",
    "                # Make predictions based on threshold\n",
    "                #pred = (euclidean_distance > threshold).float()\n",
    "                \n",
    "                # Compare predictions with labels\n",
    "                correct += (pred == label).sum().item()\n",
    "                total += label.size(0)\n",
    "\n",
    "                # Call the 'on_batch_end' callback\n",
    "                if 'on_batch_end' in callbacks:\n",
    "                    callbacks['on_batch_end'](epoch, batch_idx, loss.item())\n",
    "\n",
    "            avg_loss = epoch_loss / (batch_idx + 1)\n",
    "            train_acc = correct / total\n",
    "\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] completed. Average Training Loss: {avg_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "            # Perform validation\n",
    "            val_loss, val_acc = SiameseTrainer.validate_model(model, val_loader, criterion, device, threshold)\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "                # Save the best model weights\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print(f\"Validation loss decreased ({best_val_loss:.4f}). Saving model ...\")\n",
    "                # Optionally, save the model to a file\n",
    "                if 'save_model' in callbacks:\n",
    "                    callbacks['save_model'](model, epoch)\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f'Epochs without improvement: {epochs_no_improve}/{patience}')\n",
    "\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print('Early stopping triggered.')\n",
    "                    early_stop = True\n",
    "                    break  # Break out of the epoch loop\n",
    "\n",
    "            # Call the 'on_epoch_end' callback\n",
    "            if 'on_epoch_end' in callbacks:\n",
    "                callbacks['on_epoch_end'](epoch, avg_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "        # Load the best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(\"Best model restored.\")\n",
    "\n",
    "        # Call the 'on_training_end' callback\n",
    "        if 'on_training_end' in callbacks:\n",
    "            callbacks['on_training_end'](model)\n",
    "\n",
    "        return model  # Return the trained model\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_model(model, val_loader, criterion, device, threshold):\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (img1, img2, label) in enumerate(tqdm(val_loader)):\n",
    "                img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output1, output2 = model(img1, img2)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(output1, output2, label)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Compute distances\n",
    "                #euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                cosine_similarity = F.cosine_similarity(output1, output2)\n",
    "\n",
    "                # Make predictions based on threshold\n",
    "                #pred = (euclidean_distance > threshold).float()\n",
    "                pred = torch.where(cosine_similarity >= threshold, torch.tensor(1.0).to(device), torch.tensor(-1.0).to(device))\n",
    "\n",
    "                # Compare predictions with labels\n",
    "                correct += (pred == label).sum().item()\n",
    "                total += label.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / (batch_idx + 1)\n",
    "        val_acc = correct / total\n",
    "\n",
    "        return avg_val_loss, val_acc\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define dummy callbacks\n",
    "def on_batch_end(epoch, batch_idx, loss):\n",
    "    pass\n",
    "    #print(f\"[Callback] Epoch {epoch}, Batch {batch_idx}, Loss: {loss:.4f}\")\n",
    "\n",
    "def on_epoch_end(epoch, avg_train_loss, train_acc, avg_val_loss, val_acc):\n",
    "    print(f\"[Callback] Epoch {epoch} ended.\")\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "def save_model(model, epoch):\n",
    "    # For testing, we won't actually save the model\n",
    "    print(f\"[Callback] Model saved at epoch {epoch}\")\n",
    "\n",
    "def on_training_end(model):\n",
    "    print(\"[Callback] Training completed.\")\n",
    "\n",
    "# Assemble the callbacks into a dictionary\n",
    "callbacks = {\n",
    "    'on_batch_end': on_batch_end,\n",
    "    'on_epoch_end': on_epoch_end,\n",
    "    'save_model': save_model,\n",
    "    'on_training_end': on_training_end\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model parameters\n",
    "model_params = {\n",
    "    'image_size': (80, 80),\n",
    "    'color_channels': 1,\n",
    "    'embedding_size': 128\n",
    "}\n",
    "\n",
    "# Define training parameters\n",
    "training_params = {\n",
    "    'num_epochs': 25,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.0005,\n",
    "    'margin': 2.0,\n",
    "    'max_batches': 50  # Limit the number of batches for testing\n",
    "}\n",
    "\n",
    "# For c++ usage\n",
    "import numpy as np\n",
    "\n",
    "def get_data():\n",
    "    # Generate dummy image data (random noise)\n",
    "    height, width, channels = 105, 105, 3\n",
    "    img1_data = np.random.randint(0, 256, (height, width, channels), dtype=np.uint8)\n",
    "    img2_data = np.random.randint(0, 256, (height, width, channels), dtype=np.uint8)\n",
    "    label = np.random.choice([0, 1])  # Randomly choose 0 or 1\n",
    "\n",
    "    return img1_data, img2_data, label\n",
    "\n",
    "# /// Update callbacks\n",
    "'''\n",
    "callbacks['get_data'] = get_data\n",
    "\n",
    "train_dataset_params = {\n",
    "    'dataset_class': 'cpp',  # Indicate that we're using the dataset that interfaces with C++\n",
    "}\n",
    "\n",
    "# Run the training\n",
    "trained_model = SiameseTrainer.train_model(\n",
    "    model_params,\n",
    "    train_dataset_params,\n",
    "    training_params,\n",
    "    callbacks\n",
    ")\n",
    "#'''\n",
    "# ///\n",
    "\n",
    "#'''\n",
    "# Build the image dataset\n",
    "root_dir = 'id_dataset'  # Replace with your dataset path\n",
    "\n",
    "# Paths to the 'train' and 'val' directories\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "val_dir = os.path.join(root_dir, 'val')\n",
    "\n",
    "# Build the image datasets\n",
    "train_image_dataset = build_image_dataset_from_directory(train_dir)\n",
    "val_image_dataset = build_image_dataset_from_directory(val_dir)\n",
    "\n",
    "# Check the number of classes and images\n",
    "print(f\"Number of classes in training set: {len(train_image_dataset)}\")\n",
    "print(f\"Number of classes in validation set: {len(val_image_dataset)}\")\n",
    "\n",
    "# Define training dataset parameters\n",
    "train_dataset_params = {\n",
    "    'dataset_class': 'paths',\n",
    "    'data': train_image_dataset  # The image dataset built earlier\n",
    "}\n",
    "\n",
    "# Define validation dataset parameters\n",
    "val_dataset_params = {\n",
    "    'dataset_class': 'paths',\n",
    "    'data': val_image_dataset  # The image dataset built earlier\n",
    "}\n",
    "#'''\n",
    "\n",
    "# Run the training\n",
    "trained_model = SiameseTrainer.train_model(\n",
    "    model_params,\n",
    "    train_dataset_params,\n",
    "    val_dataset_params,\n",
    "    training_params,\n",
    "    callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class SiamesePredictor:\n",
    "    def __init__(self, model_path, model_params, threshold):\n",
    "        \"\"\"\n",
    "        Initializes the predictor with a trained model.\n",
    "\n",
    "        Args:\n",
    "            model_path (str or nn.Module): Path to the saved model file or an instance of the model.\n",
    "            model_params (dict): Parameters used to initialize the model.\n",
    "            threshold (float): Threshold for determining similarity.\n",
    "        \"\"\"\n",
    "        # Extract model parameters\n",
    "        self.image_size = model_params.get('image_size', (105, 105))\n",
    "        self.color_channels = model_params.get('color_channels', 1)\n",
    "        self.embedding_size = model_params.get('embedding_size', 128)\n",
    "\n",
    "        # Device configuration\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "\n",
    "        # Load the model\n",
    "        if isinstance(model_path, str):\n",
    "            self.model = SiameseNetworkPyTorch.load_trained_model(\n",
    "                filepath=model_path,\n",
    "                image_size=self.image_size,\n",
    "                color_channels=self.color_channels,\n",
    "                embedding_size=self.embedding_size\n",
    "            )\n",
    "        elif isinstance(model_path, nn.Module):\n",
    "            self.model = model_path\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model_path provided.\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Define transformations based on color channels\n",
    "        self.transform = self._get_transform()\n",
    "\n",
    "    def _get_transform(self):\n",
    "        \"\"\"\n",
    "        Defines the transformations to apply to input images.\n",
    "        Adjusts for grayscale or RGB images based on the model's color_channels.\n",
    "        \"\"\"\n",
    "        if self.color_channels == 1:\n",
    "            # Grayscale transformations\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "        else:\n",
    "            # RGB transformations\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.5]*self.color_channels, std=[0.5]*self.color_channels)\n",
    "            ])\n",
    "        return transform\n",
    "\n",
    "    def predict(self, input1, input2):\n",
    "        \"\"\"\n",
    "        Predicts similarity between inputs. Automatically handles different input types.\n",
    "\n",
    "        Args:\n",
    "            input1: Single image, list of images, or set of images.\n",
    "            input2: Single image, list of images, set of images, or dict of sets.\n",
    "\n",
    "        Returns:\n",
    "            See the method's docstring in previous answers for possible returns.\n",
    "        \"\"\"\n",
    "        # Handle single image vs single image\n",
    "        if self._is_single_image(input1) and self._is_single_image(input2):\n",
    "            similarity, prediction = self._predict_single_pair(input1, input2)\n",
    "            return similarity, prediction\n",
    "\n",
    "        # Handle single image vs list of images\n",
    "        elif self._is_single_image(input1) and self._is_list_of_images(input2):\n",
    "            results = self._predict_image_to_images(input1, input2)\n",
    "            return results\n",
    "\n",
    "        # Handle list of images vs list of images\n",
    "        elif self._is_list_of_images(input1) and self._is_list_of_images(input2):\n",
    "            results = self._predict_images_to_images(input1, input2)\n",
    "            return results\n",
    "\n",
    "        # Handle set comparison: input2 is a dict of sets\n",
    "        elif self._is_list_of_images(input1) and isinstance(input2, dict):\n",
    "            closest_set, distances = self._compare_set_to_sets(input1, input2)\n",
    "            return closest_set, distances\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input types for prediction.\")\n",
    "\n",
    "    # Helper methods to determine input types\n",
    "    def _is_single_image(self, input):\n",
    "        if isinstance(input, (str, Image.Image)):\n",
    "            return True\n",
    "        if isinstance(input, np.ndarray) and (len(input.shape) == 3 or (len(input.shape) == 4 and input.shape[0] == 1)):\n",
    "            return True\n",
    "        elif isinstance(input, torch.Tensor) and input.dim() == 4 and input.size(0) == 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _is_list_of_images(self, input):\n",
    "        if isinstance(input, np.ndarray) and len(input.shape) == 4 and input.shape[0] > 1:\n",
    "            return True\n",
    "        return isinstance(input, list) and all(isinstance(i, (str, Image.Image, np.ndarray, torch.Tensor)) for i in input)\n",
    "\n",
    "    # Prediction methods\n",
    "    def _predict_single_pair(self, img1, img2):\n",
    "        # Load and preprocess images\n",
    "        img1 = self._prepare_input(img1)\n",
    "        img2 = self._prepare_input(img2)\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            output1 = self.model.forward_once(img1)\n",
    "            output2 = self.model.forward_once(img2)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        cosine_similarity = F.cosine_similarity(output1, output2).item()\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = 1.0 if cosine_similarity >= self.threshold else -1.0\n",
    "\n",
    "        return cosine_similarity, prediction\n",
    "\n",
    "    def _predict_image_to_images(self, img1, images):\n",
    "        # Load and preprocess images\n",
    "        img1 = self._prepare_input(img1)\n",
    "        img2_batch = self._prepare_batch_input(images)\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            output1 = self.model.forward_once(img1)\n",
    "            output2 = self.model.forward_once(img2_batch)\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        cosine_similarities = F.cosine_similarity(output1, output2)\n",
    "\n",
    "        # Make predictions\n",
    "        preds = torch.where(cosine_similarities >= self.threshold, torch.tensor(1.0).to(self.device), torch.tensor(-1.0).to(self.device))\n",
    "        similarities = cosine_similarities.cpu().numpy()\n",
    "        preds = preds.cpu().numpy()\n",
    "\n",
    "        # Pair image identifiers with results\n",
    "        results = list(zip(images, similarities, preds))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _predict_images_to_images(self, images1, images2):\n",
    "        # Load and preprocess images\n",
    "        img1_batch = self._prepare_batch_input(images1)\n",
    "        img2_batch = self._prepare_batch_input(images2)\n",
    "\n",
    "        # Ensure same batch size\n",
    "        if len(images1) != len(images2):\n",
    "            raise ValueError(\"Both image lists must have the same number of images.\")\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            output1 = self.model.forward_once(img1_batch)\n",
    "            output2 = self.model.forward_once(img2_batch)\n",
    "\n",
    "        # Compute cosine similarities\n",
    "        cosine_similarities = F.cosine_similarity(output1, output2)\n",
    "\n",
    "        # Make predictions\n",
    "        preds = torch.where(cosine_similarities >= self.threshold, torch.tensor(1.0).to(self.device), torch.tensor(-1.0).to(self.device))\n",
    "        similarities = cosine_similarities.cpu().numpy()\n",
    "        preds = preds.cpu().numpy()\n",
    "\n",
    "        # Pair image identifiers with results\n",
    "        results = list(zip(images1, images2, similarities, preds))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _compare_set_to_sets(self, comp_images, sets_dict):\n",
    "        \"\"\"\n",
    "        Compares a set of images (comp_images) to multiple sets (sets_dict).\n",
    "\n",
    "        Args:\n",
    "            comp_images (list): List of images in COMP set.\n",
    "            sets_dict (dict): Dictionary with set names as keys and list of images as values.\n",
    "\n",
    "        Returns:\n",
    "            str: Name of the closest set.\n",
    "            dict: Average similarities to each set.\n",
    "        \"\"\"\n",
    "        comp_embeddings = self._compute_embeddings(comp_images)\n",
    "\n",
    "        set_similarities = {}\n",
    "        for set_name, images in sets_dict.items():\n",
    "            set_embeddings = self._compute_embeddings(images)\n",
    "            # Compute similarities between every pair of embeddings\n",
    "            similarities = self._compute_similarity_matrix(comp_embeddings, set_embeddings)\n",
    "            # Compute average similarity\n",
    "            avg_similarity = similarities.mean()\n",
    "            set_similarities[set_name] = avg_similarity\n",
    "\n",
    "        # Determine the closest set (highest average similarity)\n",
    "        closest_set = max(set_similarities, key=set_similarities.get)\n",
    "\n",
    "        return closest_set, set_similarities\n",
    "\n",
    "    def _prepare_input(self, img):\n",
    "        \"\"\"\n",
    "        Prepares a single image input, handling different data types efficiently.\n",
    "\n",
    "        Args:\n",
    "            img: Image input, can be a path, PIL Image, NumPy array, or tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Preprocessed image tensor ready for model input.\n",
    "        \"\"\"\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            # Assume tensor is already preprocessed\n",
    "            img = img.to(self.device)\n",
    "            if img.dim() == 3:\n",
    "                img = img.unsqueeze(0)  # Add batch dimension\n",
    "            return img\n",
    "\n",
    "        else:\n",
    "            # Load image if path is provided\n",
    "            if isinstance(img, str):\n",
    "                img = Image.open(img)\n",
    "            elif isinstance(img, np.ndarray):\n",
    "                # Convert NumPy array to PIL Image\n",
    "                if img.ndim == 3:\n",
    "                    if img.shape[2] == 1:\n",
    "                        # Grayscale image\n",
    "                        img = Image.fromarray(np.squeeze(img, axis=2).astype('uint8'), mode='L')\n",
    "                    elif img.shape[2] == 3:\n",
    "                        # RGB image\n",
    "                        img = Image.fromarray(img.astype('uint8'), mode='RGB')\n",
    "                    else:\n",
    "                        # Handle images with other numbers of channels\n",
    "                        img = Image.fromarray(img[:, :, :3].astype('uint8'), mode='RGB')\n",
    "                elif img.ndim == 2:\n",
    "                    # Grayscale image\n",
    "                    img = Image.fromarray(img.astype('uint8'), mode='L')\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported NumPy array shape: {img.shape}\")\n",
    "            elif isinstance(img, Image.Image):\n",
    "                # If it's already a PIL Image, do nothing\n",
    "                pass\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported image type: {type(img)}\")\n",
    "\n",
    "            # Convert image to the appropriate mode\n",
    "            img = self._convert_image_mode(img)\n",
    "\n",
    "            # Apply transformations\n",
    "            img = self.transform(img).unsqueeze(0).to(self.device)\n",
    "            return img\n",
    "\n",
    "    def _prepare_batch_input(self, images):\n",
    "        \"\"\"\n",
    "        Prepares a batch of images efficiently.\n",
    "\n",
    "        Args:\n",
    "            images: List of image inputs.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Batch of preprocessed images.\n",
    "        \"\"\"\n",
    "        tensors = []\n",
    "        for img in images:\n",
    "            tensors.append(self._prepare_input(img))\n",
    "        # Concatenate tensors along the batch dimension\n",
    "        batch_tensor = torch.cat(tensors, dim=0)\n",
    "        return batch_tensor\n",
    "\n",
    "    def _convert_image_mode(self, img):\n",
    "        \"\"\"\n",
    "        Converts the image to the appropriate mode based on color_channels.\n",
    "\n",
    "        Args:\n",
    "            img (PIL.Image): Input image.\n",
    "\n",
    "        Returns:\n",
    "            PIL.Image: Image converted to the correct mode.\n",
    "        \"\"\"\n",
    "        if self.color_channels == 1:\n",
    "            # Convert to grayscale if necessary\n",
    "            if img.mode != 'L':\n",
    "                img = img.convert('L')\n",
    "        else:\n",
    "            # Convert to RGB if necessary\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "        return img\n",
    "\n",
    "    def _compute_embeddings(self, images):\n",
    "        # Prepare batch input\n",
    "        img_batch = self._prepare_batch_input(images)\n",
    "\n",
    "        # Compute embeddings\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model.forward_once(img_batch)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _compute_similarity_matrix(self, embeddings1, embeddings2):\n",
    "        \"\"\"\n",
    "        Computes the pairwise cosine similarity matrix between two sets of embeddings.\n",
    "\n",
    "        Args:\n",
    "            embeddings1 (Tensor): Embeddings of the first set (N x D).\n",
    "            embeddings2 (Tensor): Embeddings of the second set (M x D).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Similarity matrix of size N x M.\n",
    "        \"\"\"\n",
    "        # Normalize embeddings\n",
    "        embeddings1_norm = F.normalize(embeddings1, p=2, dim=1)\n",
    "        embeddings2_norm = F.normalize(embeddings2, p=2, dim=1)\n",
    "\n",
    "        # Compute cosine similarity matrix\n",
    "        similarities = torch.mm(embeddings1_norm, embeddings2_norm.t())\n",
    "\n",
    "        # Flatten to a single vector\n",
    "        similarities = similarities.view(-1).cpu().numpy()\n",
    "\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from sys import dont_write_bytecode\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class SiameseDatasetFromArrays(Dataset):\n",
    "    def __init__(self, positives, negatives, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with positive and negative examples.\n",
    "\n",
    "        Args:\n",
    "            positives (list): List of positive examples.\n",
    "            negatives (list): List of negative examples.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.data = positives + negatives  # Combine positive and negative examples\n",
    "        random.shuffle(self.data)  # Shuffle the combined data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        label, id1, frame1, image1, id2, frame2, image2 = item\n",
    "\n",
    "        # Process images\n",
    "        img1 = self._process_image(image1)\n",
    "        img2 = self._process_image(image2)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return img1, img2, label\n",
    "\n",
    "    def _process_image(self, image):\n",
    "        #print(image.shape, image.dtype, image.min(), image.max())\n",
    "\n",
    "        # Convert the NumPy array to a tensor\n",
    "        # Expected shape: (H, W, C) or (H, W)\n",
    "        '''if image.ndim == 3:\n",
    "            # Image with channels\n",
    "            image = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n",
    "        elif image.ndim == 2:\n",
    "            # Grayscale image, add channel dimension\n",
    "            image = image[np.newaxis, :, :]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image shape: {image.shape}\")'''\n",
    "\n",
    "        # Convert to float and normalize to [0, 1]\n",
    "        #image = image.astype(np.float32) / 255.0\n",
    "        #print(image.shape)\n",
    "\n",
    "        # Convert to tensor\n",
    "        #img_tensor = torch.from_numpy(image)\n",
    "        if image.shape[-1] == 3:\n",
    "            img_tensor = Image.fromarray(image, 'RGB')\n",
    "        else:\n",
    "            img_tensor = Image.fromarray(image[..., 0], 'L')\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "        else:\n",
    "            # Normalize manually if no transform is provided\n",
    "            raise ValueError(\"A transform function is required.\")\n",
    "            img_tensor = (img_tensor - 0.5) / 0.5  # Assuming mean=0.5, std=0.5\n",
    "\n",
    "        return img_tensor\n",
    "\n",
    "root = '/Users/tristan/trex/docs/notebooks/id_dataset'\n",
    "\n",
    "def concatenate_npzs(base, verbose=False):\n",
    "    paths = sorted(glob(base+\"_tracklet_images_single_part*.npz\"))\n",
    "    print(paths)\n",
    "\n",
    "    all_images = []\n",
    "    all_frames = []\n",
    "    all_ids = []\n",
    "    all_frame_segment_indexes = []\n",
    "    all_frame_segments = []\n",
    "\n",
    "    number_of_segments = 0\n",
    "\n",
    "    for path in tqdm(paths):\n",
    "        with np.load(path) as npz:\n",
    "            if verbose:\n",
    "                print(npz.files)\n",
    "                print(npz['ids'].shape) # (192762,)\n",
    "                print(npz['images'].shape) # (192762, 80, 80, 1)\n",
    "\n",
    "            _images = npz['images']\n",
    "            _frames = npz['frames']\n",
    "            _ids = npz['ids']\n",
    "            _frame_segment_indexes = npz['frame_segment_indexes']\n",
    "\n",
    "            for i in range(_frame_segment_indexes.shape[0]):\n",
    "                _frame_segment_indexes[i] += number_of_segments\n",
    "            #_frame_segment_indexes = _frame_segment_indexes + number_of_segments\n",
    "            _frame_segments = npz['frame_segments']\n",
    "            number_of_segments += _frame_segments.shape[0]\n",
    "\n",
    "            all_images.append(_images)\n",
    "            all_frames.append(_frames)\n",
    "            all_ids.append(_ids)\n",
    "            \n",
    "            all_frame_segment_indexes.append(_frame_segment_indexes)\n",
    "            all_frame_segments.append(_frame_segments)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"loading {path}: {npz['images'].shape} {_frame_segment_indexes.min()} {_frame_segment_indexes.shape}\")\n",
    "\n",
    "    all_images = np.concatenate(all_images, axis=0)\n",
    "    all_frames = np.concatenate(all_frames, axis=0)\n",
    "    all_ids = np.concatenate(all_ids, axis=0)\n",
    "    all_frame_segment_indexes = np.concatenate(all_frame_segment_indexes, axis=0)\n",
    "    # assert that all segment indexes are unique\n",
    "    #assert len(np.unique(all_frame_segment_indexes)) == all_frame_segment_indexes.shape[0]\n",
    "    all_frame_segments = np.concatenate(all_frame_segments, axis=0)\n",
    "\n",
    "    return all_images, all_frames, all_ids, all_frame_segment_indexes, all_frame_segments\n",
    "\n",
    "def load_npz_as_dataset(path, dont_use_these_ids = [], verbose=False, split=0.75):\n",
    "    all_images, all_frames, all_ids, all_frame_segment_indexes, all_frame_segments = concatenate_npzs(path, verbose=verbose)\n",
    "\n",
    "    # print shapes\n",
    "    print(all_images.shape, all_frames.shape, all_ids.shape, all_frame_segment_indexes.shape, all_frame_segments.shape)\n",
    "\n",
    "    all_positives = []\n",
    "    all_negatives = []\n",
    "\n",
    "    # save these images (sorted by id) to folders.\n",
    "    # the root folder is given, then below that we create a folder / id and enumerate\n",
    "    # the images in that folder.\n",
    "\n",
    "    # create a folder for each id\n",
    "    for i, id in enumerate(tqdm(np.unique(all_ids))):\n",
    "        if id in dont_use_these_ids:\n",
    "            continue\n",
    "        folder = os.path.join(root, str(id))\n",
    "        #os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "        mask = all_ids == id\n",
    "        if verbose:\n",
    "            print(f\"id: {all_ids.shape}, images: {all_images[mask].shape}, segments: {all_frame_segments.shape} mask: {mask.shape}\")\n",
    "        _used_indexes = np.unique(all_frame_segment_indexes[mask])\n",
    "\n",
    "        used_indexes = []\n",
    "        for s in _used_indexes:\n",
    "            if s < 0 or s >= all_frame_segments.shape[0]:\n",
    "                continue\n",
    "            used_indexes.append(s)\n",
    "            assert s >= 0 and s < all_frame_segments.shape[0], f\"{s} not in {all_frame_segments.shape}\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"id: {id}, images: {all_images[mask].shape}, segments: {all_frame_segments[used_indexes]}\")\n",
    "\n",
    "        # get number of images / frame segment\n",
    "        for s in used_indexes:\n",
    "            mask = all_frame_segment_indexes == s\n",
    "            N = mask.sum()\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"segment: {s}, images: {N} frame_segments: {all_frame_segments.shape}\")\n",
    "            segment = all_frame_segments[s]\n",
    "            L = segment[-1] - segment[0] + 1\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"segment: {segment}, images: {N}, L: {L}\")\n",
    "\n",
    "            if N < 100:\n",
    "                continue\n",
    "\n",
    "            my_frames = all_frames[mask]\n",
    "            my_images = all_images[mask]\n",
    "\n",
    "            # find data of other ids that are in the same frame segment\n",
    "            current_frame_mask = (all_frames == my_frames[len(my_frames) // 2]) & (all_ids != id) & (np.isin(all_ids, dont_use_these_ids, invert=True))\n",
    "            _filtered_ids = np.unique(all_ids[current_frame_mask])\n",
    "            if verbose:\n",
    "                print(_filtered_ids, \"not in\", dont_use_these_ids, np.isin(_filtered_ids, dont_use_these_ids).any())\n",
    "            assert np.isin(id, _filtered_ids).sum() == 0\n",
    "            assert np.isin(_filtered_ids, dont_use_these_ids).any() == False\n",
    "\n",
    "            other_indexes = np.unique(all_frame_segment_indexes[current_frame_mask])\n",
    "            #other_segments = np.array([npz[\"frame_segments\"][idx] for idx in other_indexes])\n",
    "\n",
    "            # calculate the maximum number of positive examples we can get from within\n",
    "            # the same segment and the current individual (while maintaining a min_distance\n",
    "            # between the images):\n",
    "            min_distance = 5\n",
    "            n_positives = max(1, int(L / min_distance / 2))\n",
    "\n",
    "            # sample positives\n",
    "            _frames = my_frames\n",
    "            chosen_frames = np.random.choice(_frames, size=n_positives, replace=False)\n",
    "            chosen_frames = np.sort(chosen_frames)\n",
    "            # remove the chosen frames from the list of all frames\n",
    "            _frames = np.setdiff1d(_frames, chosen_frames)\n",
    "\n",
    "            #print(f\"chosen_frames: {chosen_frames} frames: {_frames}\")\n",
    "\n",
    "            # now sample the positives, removing a used paired image from the list of all frames\n",
    "            positives = []\n",
    "            for frame in chosen_frames:\n",
    "                if len(_frames) == 0:\n",
    "                    print(\"***> no more frames to choose from\")\n",
    "                    break\n",
    "\n",
    "                my_image = my_images[my_frames == frame]\n",
    "                assert len(my_image) == 1\n",
    "                my_image = my_image[0]\n",
    "\n",
    "                # now choose the index of the paired image.\n",
    "                # prefer to choose an image as far away as possible within the same segment\n",
    "                paired_frame = _frames[np.abs(_frames - frame).argmax()]\n",
    "                _frames = np.setdiff1d(_frames, [paired_frame])\n",
    "\n",
    "                paired_image = my_images[my_frames == paired_frame]\n",
    "                assert len(paired_image) == 1\n",
    "\n",
    "                assert not id in dont_use_these_ids\n",
    "\n",
    "                paired_image = paired_image[0]\n",
    "                positives.append((1.0, id, frame, my_image, id, paired_frame, paired_image))\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\tpositives: {len(positives)}\")\n",
    "\n",
    "            # sample images from other segments to get negative examples (dissimilar)\n",
    "            # we can assume that since the frame segments overlap, both individuals which\n",
    "            # are visible in the same frame segment are dissimilar. lets sample 10% of the\n",
    "            # length of the current individuals segment as negative examples.\n",
    "            n_negatives = max(1, int(1.0 * len(positives)))\n",
    "\n",
    "            # cache the frames for all other indexes\n",
    "            other_index_masks = {}\n",
    "            for idx in other_indexes:\n",
    "                other_frame_mask = (all_frame_segment_indexes == idx)\n",
    "                ids = np.unique(all_ids[other_frame_mask])\n",
    "                assert len(ids) == 1, f\"more than one id in frame segment when searching for idx {idx}: {ids}\"\n",
    "                assert not ids[0] in dont_use_these_ids\n",
    "                other_index_masks[idx] = (other_frame_mask, ids[0], all_frames[other_frame_mask], all_images[other_frame_mask])\n",
    "\n",
    "            # sample negatives\n",
    "            #print(f\"other_indexes: {len(other_indexes)}\")\n",
    "            if len(other_indexes) == 0:\n",
    "                continue\n",
    "            \n",
    "            chosen_negatives = np.random.randint(0, len(other_indexes), size=n_negatives)\n",
    "            chosen_indexes = other_indexes[chosen_negatives]\n",
    "\n",
    "            negatives = []\n",
    "\n",
    "            for other_index in chosen_indexes:\n",
    "                # sample a random index from that segment\n",
    "                other_frame_mask, other_id, _frames, _images = other_index_masks[other_index]\n",
    "\n",
    "                found = False\n",
    "                for _ in range(5):\n",
    "                    index = np.random.randint(0, len(_images))\n",
    "                    frame = _frames[index]\n",
    "\n",
    "                    my_image = my_images[my_frames == frame]\n",
    "                    if len(my_image) == 0:\n",
    "                        if verbose:\n",
    "                            print(f\"\\t XX frame: {frame} no image found\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        my_image = my_image[0]\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"\\t => frame: {frame}, image: {len(my_image)} my_images: {my_images.shape}, other_images: {_images.shape}\")\n",
    "                            print(frame, \"in\", my_frames)\n",
    "\n",
    "                            print(f\"\\t\\t my_image: {my_image.shape} other_images: {_images[index].shape}\")\n",
    "\n",
    "                        assert not id in dont_use_these_ids\n",
    "                        assert not other_id in dont_use_these_ids\n",
    "                        \n",
    "                        other_image = (-1.0, id, frame, my_image, other_id, frame, _images[index])\n",
    "                        negatives.append(other_image)\n",
    "                        found = True\n",
    "                        break\n",
    "                \n",
    "                if not found and verbose:\n",
    "                    print(f\"\\t ** frame: {frame} no image found at all\")\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\tnegatives: {len(negatives)}\")\n",
    "\n",
    "            all_positives.extend(positives)\n",
    "            all_negatives.extend(negatives)\n",
    "\n",
    "            \n",
    "        #break\n",
    "\n",
    "    for label, id1, frame1, image1, id2, frame2, image2 in all_positives:\n",
    "        assert np.isin([id1, id2], list(dont_use_these_ids)).sum() == 0\n",
    "    \n",
    "    for label, id1, frame1, image1, id2, frame2, image2 in all_negatives:\n",
    "        assert np.isin([id1, id2], list(dont_use_these_ids)).sum() == 0\n",
    "\n",
    "    # Define model parameters\n",
    "    image_size = all_images.shape[1:-1]\n",
    "    print(image_size)\n",
    "\n",
    "    color_channels = all_images.shape[-1]\n",
    "    print(f\"Color channels: {color_channels}\")\n",
    "\n",
    "    # split the data into training and validation sets\n",
    "    split_ratio = split\n",
    "    split_index = int(split_ratio * len(all_positives))\n",
    "\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(all_positives)\n",
    "    np.random.shuffle(all_negatives)\n",
    "\n",
    "    train_positives = all_positives[:split_index]\n",
    "    val_positives = all_positives[split_index:]\n",
    "\n",
    "    train_negatives = all_negatives[:split_index]\n",
    "    val_negatives = all_negatives[split_index:]\n",
    "\n",
    "    # Define the model parameters\n",
    "    model_params = {\n",
    "        'image_size': image_size,\n",
    "        'color_channels': color_channels,\n",
    "        'embedding_size': 128\n",
    "    }\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.RandomAffine(degrees=0.05, translate=(0.05, 0.1), scale=None), #(0.99, 1.01)),\n",
    "        #transforms.RandomAutocontrast(),\n",
    "        #transforms.RandAugment(2, 9),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        #transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5] * model_params[\"color_channels\"], std=[0.5] * model_params[\"color_channels\"])\n",
    "    ])\n",
    "\n",
    "    val_dataset = SiameseDatasetFromArrays(\n",
    "        val_positives,\n",
    "        val_negatives,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_dataset = SiameseDatasetFromArrays(\n",
    "        train_positives,\n",
    "        train_negatives,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    return val_dataset, train_dataset, model_params\n",
    "\n",
    "#val_dataset, train_dataset = load_npz_as_dataset('/Users/tristan/Videos/data/group_1_tracklet_images_single_part0.npz', dont_use_these_ids=[5,3,1])\n",
    "val_dataset, train_dataset, model_params = load_npz_as_dataset('/Users/tristan/Downloads/For Tristan/data/ForTristan', dont_use_these_ids=[4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training parameters\n",
    "training_params = {\n",
    "    'num_epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.0005,\n",
    "    'patience': 100,\n",
    "    #'margin': 2.0,\n",
    "    'threshold': 0.0\n",
    "}\n",
    "\n",
    "# Check the number of classes and images\n",
    "print(f\"Number of classes in training set: {len(train_dataset)}\")\n",
    "print(f\"Number of classes in validation set: {len(val_dataset)}\")\n",
    "\n",
    "# Define training dataset parameters\n",
    "train_dataset_params = {\n",
    "    'dataset_class': 'dataset',\n",
    "    'data': train_dataset  # The image dataset built earlier\n",
    "}\n",
    "\n",
    "# Define validation dataset parameters\n",
    "val_dataset_params = {\n",
    "    'dataset_class': 'dataset',\n",
    "    'data': val_dataset  # The image dataset built earlier\n",
    "}\n",
    "#'''\n",
    "\n",
    "\n",
    "# Run the training\n",
    "trained_model = SiameseTrainer.train_model(\n",
    "    model_params,\n",
    "    train_dataset_params,\n",
    "    val_dataset_params,\n",
    "    training_params,\n",
    "    callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SiamesePredictor(\n",
    "    model_path=trained_model,\n",
    "    model_params=model_params,\n",
    "    #transform=trained_model.transform,\n",
    "    threshold=training_params.get('threshold', 1.0)\n",
    ")\n",
    "\n",
    "# Define a function to evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels in tqdm(test_loader):\n",
    "            #print(img1.shape, img2.shape, labels.shape)\n",
    "            #print(f\"single input: {predictor._is_single_image(img1)} for {img1.shape}\")\n",
    "            distance, prediction = predictor.predict(img1, img2)\n",
    "\n",
    "            #print(f\"Euclidean Distance: {distance:.4f}\")\n",
    "            #print(f\"Prediction: {'Similar' if prediction == 0 else 'Dissimilar'}\")\n",
    "\n",
    "            # Compare predictions with labels\n",
    "            correct += (prediction == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(trained_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_dont_use_ids = [4,5]\n",
    "_all_ids  = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "set_difference = np.setxor1d(previous_dont_use_ids, _all_ids)\n",
    "print(\"set_difference\", set_difference)\n",
    "#_train_dataset, _ = load_npz_as_dataset('/Users/tristan/Videos/data/group_1_tracklet_images_single_part0.npz', dont_use_these_ids=set_difference, split=0.0)\n",
    "_train_dataset, _, _ = load_npz_as_dataset('/Users/tristan/Downloads/For Tristan/data/ForTristan', dont_use_these_ids=set_difference, split=0.0)\n",
    "print(f\"Number of classes in training set: {len(_train_dataset)}\")\n",
    "margin = training_params.get('margin', 2.0)\n",
    "#criterion = ContrastiveLoss(margin=margin)\n",
    "criterion = nn.CosineEmbeddingLoss()#margin=margin)\n",
    "threshold = training_params.get('threshold', margin / 2.0)\n",
    "\n",
    "#_val_loader = DataLoader(_val_dataset, shuffle=False, batch_size=64, num_workers=0)\n",
    "_train_loader = DataLoader(_train_dataset, shuffle=False, batch_size=64, num_workers=0)\n",
    "\n",
    "#print(SiameseTrainer.validate_model(trained_model, _val_loader, criterion, 'mps', threshold))\n",
    "print(SiameseTrainer.validate_model(trained_model, _train_loader, criterion, 'mps', threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "output_images_npz = \"/Users/tristan/Downloads/For Tristan/data/ForTristan\"\n",
    "video_base_name = output_images_npz.split(\"/\")[-1]\n",
    "#with np.load(\"/Users/tristan/Videos/data/group_1_tracklet_images_single_part0.npz\") as npz:\n",
    "\n",
    "result = concatenate_npzs(output_images_npz, verbose=True)\n",
    "images, frames, ids, frame_segment_indexes, frame_segments = result\n",
    "\n",
    "print(frames.shape, ids.shape, frame_segments.shape, frame_segment_indexes.shape, images.shape)\n",
    "print(np.unique(ids))\n",
    "\n",
    "def generate_movie(id, other=None):\n",
    "    mask = ids == id\n",
    "    other_mask = ids == other if other is not None else None\n",
    "    \n",
    "    predictor = SiamesePredictor(\n",
    "        model_path=trained_model,\n",
    "        model_params=model_params,\n",
    "        threshold=training_params.get('threshold', 1.0)\n",
    "    )\n",
    "\n",
    "    sub_images = images[mask]\n",
    "    sub_frames = frames[mask]\n",
    "\n",
    "    other_images = None\n",
    "    other_frames = None\n",
    "    if other_mask is not None:\n",
    "        other_images = images[other_mask]\n",
    "        other_frames = frames[other_mask]\n",
    "\n",
    "    # sort the images by frame\n",
    "    sort_index = np.argsort(sub_frames)\n",
    "    sub_images = sub_images[sort_index]\n",
    "    sub_frames = sub_frames[sort_index]\n",
    "\n",
    "    print(sub_images.shape, sub_frames.shape)\n",
    "\n",
    "    name = f'{video_base_name}_{id}.mp4' if other is None else f'{video_base_name}_{id}_{other}.mp4'\n",
    "    print(f\"Generating movie: {name}\")\n",
    "    cap = cv.VideoWriter(name, cv.VideoWriter_fourcc(*'H264'), 30, (80 * 2, 80))\n",
    "    previous_image = None\n",
    "    for frame, img in tqdm(zip(sub_frames, sub_images), total=len(sub_frames)):\n",
    "        similar = None\n",
    "        corresponding_image = None\n",
    "        \n",
    "        if other_images is None:\n",
    "            similar = True\n",
    "\n",
    "            if previous_image is not None:\n",
    "                distance, prediction = predictor.predict(previous_image, img)\n",
    "                if prediction == 1:\n",
    "                    similar = False\n",
    "                #print(f\"Euclidean Distance: {distance:.4f}\")\n",
    "                #print(f\"Prediction: {'Similar' if prediction == 0 else 'Dissimilar'}\")\n",
    "            previous_image = img\n",
    "        else:\n",
    "            # find the corresponding image that is closest to the current frame\n",
    "            min_frame_idx = np.argmin(np.abs(other_frames - frame))\n",
    "            corresponding_image = other_images[min_frame_idx]\n",
    "            other_frame = other_frames[min_frame_idx]\n",
    "\n",
    "            distance, prediction = predictor.predict(img, corresponding_image)\n",
    "            if prediction == 1:\n",
    "                similar = True\n",
    "            else:\n",
    "                similar = False\n",
    "\n",
    "            assert corresponding_image is not None\n",
    "        \n",
    "        if img.shape[-1] == 1:\n",
    "            img = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n",
    "        output = np.zeros((80, 80 * 2, 3), dtype=np.uint8)\n",
    "        output[:, :80] = img\n",
    "        if other_images is not None:\n",
    "            if corresponding_image.shape[-1] == 1:\n",
    "                corresponding_image = cv.cvtColor(corresponding_image, cv.COLOR_GRAY2BGR)\n",
    "            output[:, 80:] = corresponding_image\n",
    "        elif previous_image is not None:\n",
    "            if previous_image.shape[-1] == 1:\n",
    "                previous_image = cv.cvtColor(previous_image, cv.COLOR_GRAY2BGR)\n",
    "            output[:, 80:] = previous_image\n",
    "\n",
    "        cv.putText(output, f\"{id} {frame}\", (10, 10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "        if other_images is not None:\n",
    "            cv.putText(output, f\"{other} ({other_frame})\", (90, 10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "        else:\n",
    "            cv.putText(output, f\"previous frame\", (90, 10), cv.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "\n",
    "        if similar is None:\n",
    "            cv.putText(output, \"Unknown\", (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1, cv.LINE_AA)\n",
    "            cv.rectangle(output, (0, 0), (80, 80), (255, 255, 255), 1)\n",
    "        elif similar:\n",
    "            cv.putText(output, \"Similar\", (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1, cv.LINE_AA)\n",
    "            cv.rectangle(output, (0, 0), (80, 80), (0, 255, 0), 1)\n",
    "        else:\n",
    "            cv.putText(output, \"Dissimilar\", (10, 20), cv.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1, cv.LINE_AA)\n",
    "            cv.rectangle(output, (0, 0), (80, 80), (0, 0, 255), 1)\n",
    "\n",
    "        cap.write(output)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    print(f\"Finished generating movie: {name}\")\n",
    "\n",
    "generate_movie(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (label, id1, frame1, img1, id2, frame2, img2) in enumerate(all_positives):\n",
    "    print(f\"positive: label: {label}, id1: {id1}, frame1: {frame1}, id2: {id2}, frame2: {frame2}\")\n",
    "    break\n",
    "\n",
    "examples = [all_negatives[n] for n in np.random.randint(0, len(all_negatives), 10)]\n",
    "for i, (label, id1, frame1, img1, id2, frame2, img2) in enumerate(examples):\n",
    "    print(f\"negative: label: {label}, id1: {id1}, frame1: {frame1}, id2: {id2}, frame2: {frame2}\")\n",
    "    #break\n",
    "#print(f\"all_positives: {all_positives[0]}\")\n",
    "#print(f\"all_negatives: {np.shape(all_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort the generated dataset into train and val folders\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get all the folders\n",
    "ids = glob(os.path.join(root, \"*\"))\n",
    "\n",
    "# create the train and val folders\n",
    "os.makedirs(os.path.join(root, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(root, 'val'), exist_ok=True)\n",
    "\n",
    "# split the ids into train and val\n",
    "train_ids, val_ids = train_test_split(ids, test_size=0.2)\n",
    "\n",
    "# move the folders to the correct location\n",
    "for folder in train_ids:\n",
    "    os.rename(folder, os.path.join(root, 'train', os.path.basename(folder)))\n",
    "\n",
    "for folder in val_ids:\n",
    "    os.rename(folder, os.path.join(root, 'val', os.path.basename(folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save_model(\"trained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = SiameseNetworkPyTorch.load_trained_model(\"trained_model.pt\", image_size=(80, 80), color_channels=1, embedding_size=128)\n",
    "test_model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SiameseTrainer.validate_model(test_model, _train_loader, criterion, 'mps', threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
