{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d09038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  hardest_validation_samples.py\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from os import name\n",
    "from pathlib import Path\n",
    "import math, json, numpy as np\n",
    "from ultralytics import YOLO\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def hardest_validation_samples(\n",
    "    model_weight: str,\n",
    "    data_yaml: str,\n",
    "    save_dir: str = \"runs/val_hardness\",\n",
    "    conf_thr: float = 0.25,\n",
    "    top_percent: float = 0.10,\n",
    "    iou_type: str = \"keypoints\",        # \"keypoints\"  |  \"bbox\"\n",
    "    oks_iou: float = 0.50               # IoU / OKS threshold that counts as “found”\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_weight : str\n",
    "        Path to the trained *.pt* checkpoint.\n",
    "    data_yaml : str\n",
    "        Your dataset YAML (must list train/val paths and key-point metadata).\n",
    "    save_dir : str, optional\n",
    "        Where intermediate JSON/TXT files will be written.\n",
    "    conf_thr : float, optional\n",
    "        Minimum confidence a prediction must have to be kept during validation.\n",
    "    top_percent : float, optional\n",
    "        Fraction of validation images to return as the “hardest” subset.\n",
    "    iou_type : {\"keypoints\",\"bbox\"}, optional\n",
    "        Metric family to evaluate with COCOeval.\n",
    "    oks_iou : float, optional\n",
    "        IoU/OKS level that defines a *successful* detection.\n",
    "    ---------------------------------------------------------------------------\n",
    "    Returns\n",
    "    -------\n",
    "    hard_samples : list[(file_name:str, score:float)]\n",
    "        Sorted hardest→easiest images and their AP/OKS scores.\n",
    "    detected_percent : float\n",
    "        Percentage of ground-truth objects that were correctly detected\n",
    "        (IoU/OKS ≥ `oks_iou` and confidence ≥ `conf_thr`).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Ultralytics validation (writes predictions.json & labels.json)\n",
    "    model = YOLO(model_weight)\n",
    "    model.to('mps')\n",
    "\n",
    "    if Path(save_dir+\"/val\").exists():\n",
    "        print(f\"Deleting previous validation results in {save_dir}/val\")\n",
    "        for f in Path(save_dir+\"/val\").glob(\"*\"):\n",
    "            f.unlink()\n",
    "        Path(save_dir+\"/val\").rmdir()\n",
    "    \n",
    "    val_res = model.val(data=data_yaml,\n",
    "                        save_json=True,\n",
    "                        save_txt=False,\n",
    "                        save_conf=True,\n",
    "                        conf=conf_thr,\n",
    "                        project=save_dir)\n",
    "\n",
    "    work = Path(val_res.save_dir)                 # ← folder created by YOLO\n",
    "    preds_json = work / \"predictions.json\"\n",
    "    gts_json   = work / \"labels.json\"             # auto-generated GT in COCO format\n",
    "\n",
    "    # 2) COCO-style evaluation, but we keep every per-image record\n",
    "    coco_gt = COCO(str(gts_json))\n",
    "    coco_dt = coco_gt.loadRes(str(preds_json))\n",
    "    ev      = COCOeval(coco_gt, coco_dt, iouType=iou_type)\n",
    "    ev.params.useCats = 1\n",
    "    ev.evaluate(); ev.accumulate()\n",
    "\n",
    "    thr_idx       = list(ev.params.iouThrs).index(oks_iou)\n",
    "    img_scores    = {}\n",
    "    gt_detected   = 0\n",
    "\n",
    "    for rec in tqdm(ev.evalImgs):                       # one dict per image×category\n",
    "        if rec is None:\n",
    "            continue\n",
    "        img_id      = rec[\"image_id\"]\n",
    "        # precision dims: [T×R×K×A×M]\n",
    "        precisions  = rec[\"precision\"][thr_idx, :, :, 0, 0]\n",
    "        score       = np.nanmean(precisions)      # mean over recalls & classes\n",
    "        img_scores.setdefault(img_id, []).append(score)\n",
    "\n",
    "        # how many GTs matched a prediction at this IoU/OKS?\n",
    "        matched     = (rec[\"matches\"][thr_idx] > 0).sum()\n",
    "        gt_detected += matched\n",
    "\n",
    "    # mean over classes for each image\n",
    "    img_scores = {k: float(np.nanmean(v)) for k, v in img_scores.items()}\n",
    "\n",
    "    # 3) hardest N %\n",
    "    n_imgs      = len(img_scores)\n",
    "    n_hard      = max(1, math.ceil(n_imgs * top_percent))\n",
    "    worst_first = sorted(img_scores.items(), key=lambda kv: kv[1])[:n_hard]\n",
    "    hard_samples = [(coco_gt.imgs[i][\"file_name\"], s) for i, s in worst_first]\n",
    "\n",
    "    # 4) overall detection percentage\n",
    "    n_gt             = sum(len(v) for v in coco_gt.imgToAnns.values())\n",
    "    detected_percent = 100.0 * gt_detected / n_gt if n_gt else 0.0\n",
    "\n",
    "    return hard_samples, detected_percent\n",
    "\n",
    "\n",
    "# ───────────────────────────── example usage ────────────────────────────────\n",
    "'''hardest, pct = hardest_validation_samples(\n",
    "    model_weight=\"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt\",\n",
    "    data_yaml=\"/Users/tristan/Downloads/yolo11_v04_trainingdata/data.yaml\",\n",
    "    conf_thr=0.25,           # same threshold you use in production\n",
    "    top_percent=0.10,\n",
    ")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Hard‑sample mining WITHOUT labels.json\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "from pathlib import Path\n",
    "import yaml, cv2, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Image\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import csv                      # ✱ FP/DUP\n",
    "from collections import Counter # ✱ FP/DUP\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  CONFIG  – EDIT THESE\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "MODEL_W   = \"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt\"     # your checkpoint\n",
    "#MODEL_W = \"/Users/tristan/Downloads/1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v04_trainingdata-1-mAP5095_0.61153-mAP50_0.97339.pt\"     \n",
    "DATA_YAML = \"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8_format/data.yaml\"  # your dataset yaml\n",
    "SAVE_DIR  = Path(\"val_hardness\")             # ← must match the folder\n",
    "IMG_SZ    = 2560                                   # same as your val size\n",
    "CONF_THR  = 0.25                                  # deployment conf\n",
    "TOP_PERC  = 1.0                                  # top‑10 % hardest\n",
    "IOU_THR   = 0.50\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  helper: IoU for two XYXY boxes\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "def xywhn_to_xyxy(box, w, h):\n",
    "    xc, yc, bw, bh = box\n",
    "    return np.array([(xc - bw/2)*w,\n",
    "                     (yc - bh/2)*h,\n",
    "                     (xc + bw/2)*w,\n",
    "                     (yc + bh/2)*h])\n",
    "\n",
    "def box_iou_matrix(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    boxes*: (N,4) or (M,4) arrays, xyxy in absolute pixels.\n",
    "    Returns IoU matrix shape (N, M).\n",
    "    \"\"\"\n",
    "    if boxes1.size == 0 or boxes2.size == 0:\n",
    "        return np.zeros((len(boxes1), len(boxes2)), dtype=float)\n",
    "    tl = np.maximum(boxes1[:, None, :2], boxes2[None, :, :2])  # top‑left\n",
    "    br = np.minimum(boxes1[:, None, 2:], boxes2[None, :, 2:])  # bottom‑right\n",
    "    wh = np.clip(br - tl, 0, None)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    a1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    a2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    return inter / (a1[:, None] + a2[None, :] - inter + 1e-6)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Part 1 – find hardest validation frames\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# … all previous imports & helpers stay exactly the same …\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Part 1 – find hardest frames  (now ranked by two metrics)\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "def hardest_validation_samples_txt(\n",
    "    model_weight: str,\n",
    "    data_yaml: str,\n",
    "    conf_thr: float = 0.25,\n",
    "    iou_thr: float = 0.50,\n",
    "    top_percent: float = 0.10,\n",
    "    img_size: int = 640,\n",
    "    subset: str = \"val\",\n",
    "):\n",
    "    meta      = yaml.safe_load(open(data_yaml))\n",
    "    data_root = Path(data_yaml).resolve().parent\n",
    "    val_root  = (data_root / meta[subset]).resolve()\n",
    "    is_txt_listing = val_root.suffix == \".txt\"\n",
    "\n",
    "    if is_txt_listing:\n",
    "        with open(val_root) as f:\n",
    "            img_paths = [Path(p.strip()) for p in f if p.strip()]\n",
    "    else:\n",
    "        img_paths = sorted([p for p in val_root.rglob(\"*\")\n",
    "                            if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}])\n",
    "\n",
    "    model = YOLO(model_weight).to(\"mps\")\n",
    "\n",
    "    per_image_stats = []      # ★ MULTI‑METRIC  (store all we need in 1 tuple)\n",
    "    csv_rows        = []\n",
    "    total_gt = total_found = fp_total = dup_total = 0\n",
    "\n",
    "    for p in tqdm(img_paths, desc=\"validating\"):\n",
    "        lbl_path = Path(f\"{data_root}/{meta[subset]}/../labels\") / p.with_suffix(\".txt\").name\n",
    "        gt_boxes = np.empty((0,4))\n",
    "        if lbl_path.exists() and lbl_path.stat().st_size:\n",
    "            h, w = cv2.imread(str(p)).shape[:2]\n",
    "            rows = np.loadtxt(lbl_path, ndmin=2, dtype=float)\n",
    "            gt_boxes = np.stack([xywhn_to_xyxy(r[1:5], w, h) for r in rows])\n",
    "        n_gt = len(gt_boxes)\n",
    "\n",
    "        pred = model.predict(str(p), imgsz=img_size, conf=conf_thr,\n",
    "                             verbose=False)[0]\n",
    "        pred_boxes = pred.boxes.xyxy.cpu().numpy() if pred.boxes else np.empty((0,4))\n",
    "        n_pred = len(pred_boxes)\n",
    "\n",
    "        false_pos = duplicates = found = 0\n",
    "        if n_pred:\n",
    "            if n_gt:\n",
    "                ious = box_iou_matrix(gt_boxes, pred_boxes)\n",
    "                best_gt   = ious.argmax(0)\n",
    "                best_iou  = ious[best_gt, range(n_pred)]\n",
    "                assigned  = np.where(best_iou >= iou_thr, best_gt, -1)\n",
    "                ctr       = Counter(assigned[assigned >= 0])\n",
    "                found      = len(ctr)\n",
    "                duplicates = sum(c-1 for c in ctr.values())\n",
    "                false_pos  = (assigned == -1).sum()\n",
    "            else:\n",
    "                false_pos = n_pred\n",
    "\n",
    "        recall = found / n_gt if n_gt else 1.0\n",
    "\n",
    "        # ★ MULTI‑METRIC — keep both recall & fp+dup\n",
    "        per_image_stats.append((str(p), recall, false_pos + duplicates))\n",
    "\n",
    "        csv_rows.append([str(p), n_gt, n_pred, found,\n",
    "                         false_pos, duplicates, recall])\n",
    "\n",
    "        total_gt    += n_gt\n",
    "        total_found += found\n",
    "        fp_total    += false_pos\n",
    "        dup_total   += duplicates\n",
    "\n",
    "    # ★ MULTI‑METRIC — sort by tuple:  (low recall  →  high fp+dup)\n",
    "    n_hard = max(1, int(len(per_image_stats) * top_percent))\n",
    "    hard_sorted = sorted(\n",
    "        per_image_stats,\n",
    "        key=lambda t: (-t[2], t[1])   # (recall ↑ , fp+dup ↓)  so we invert fp\n",
    "    )[:n_hard]\n",
    "\n",
    "    overall_recall = 100.0 * total_found / total_gt if total_gt else 0.0\n",
    "\n",
    "    # … CSV / TXT saving section stays unchanged …\n",
    "    # (use csv_rows as before)\n",
    "\n",
    "    return hard_sorted, overall_recall\n",
    "\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Run the pipeline\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "hardest, pct = hardest_validation_samples_txt(\n",
    "    model_weight=MODEL_W,\n",
    "    data_yaml=DATA_YAML,\n",
    "    conf_thr=CONF_THR,\n",
    "    iou_thr=IOU_THR,\n",
    "    top_percent=TOP_PERC,\n",
    "    img_size=IMG_SZ,\n",
    "    subset=\"train\",\n",
    ")\n",
    "print(f\"\\n📊  overall recall (IoU≥{IOU_THR}, conf≥{CONF_THR}) = {pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b102f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml, cv2, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Image\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import csv                      # ✱ FP/DUP\n",
    "from collections import Counter # ✱ FP/DUP\n",
    "\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  evaluate_model_txt  – now with mAP50‑95\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "def evaluate_model_txt(\n",
    "    model_weight: str,\n",
    "    data_yaml: str,\n",
    "    subset: str = \"val\",\n",
    "    conf_thr: float = 0.25,\n",
    "    iou_thr: float = 0.50,   # still used for recall / precision columns\n",
    "    img_size: int = 640,\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyse *every* image in `subset` (train/val) and return:\n",
    "\n",
    "        { model, subset, n_images, n_gt, n_pred,\n",
    "          matched_gt, false_pos, duplicates,\n",
    "          recall, precision,\n",
    "          mAP50, mAP50_95 }\n",
    "\n",
    "    * mAP50 is AP at IoU ≥ `iou_thr` (default 0.50)\n",
    "    * mAP50_95 is the mean AP over IoU = 0.50 … 0.95 (step 0.05)\n",
    "      – 10 thresholds, class‑agnostic.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ setup\n",
    "    meta      = yaml.safe_load(open(data_yaml))\n",
    "    data_root = Path(data_yaml).resolve().parent\n",
    "    split_dir = (data_root / meta[subset]).resolve()\n",
    "\n",
    "    img_paths = ( [Path(p.strip()) for p in open(split_dir) if p.strip()]\n",
    "                  if split_dir.suffix == \".txt\"\n",
    "                  else sorted([p for p in split_dir.rglob(\"*\")\n",
    "                               if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}]) )\n",
    "\n",
    "    model = YOLO(model_weight).to(\"mps\")\n",
    "\n",
    "    # running tallies --------------------------------------------------------\n",
    "    total_gt = total_pred = matched_gt = fp_total = dup_total = 0\n",
    "\n",
    "    # AP data  ---------------------------------------------------------------\n",
    "    iou_thresholds = np.arange(0.50, 0.96, 0.05)         # 0.50 … 0.95\n",
    "    n_thr          = len(iou_thresholds)\n",
    "    scores_all     = [[] for _ in range(n_thr)]          # per‑thr score list\n",
    "    tp_flags_all   = [[] for _ in range(n_thr)]          # per‑thr TP mask\n",
    "\n",
    "\n",
    "    per_image_stats = []  \n",
    "\n",
    "    # ---------------------------------------------------------------- loop over images\n",
    "    for img_path in tqdm(img_paths, desc=f\"evaluating {subset}\"):\n",
    "        lbl_path = (data_root / meta[subset] / \"../labels\" /\n",
    "                    img_path.with_suffix(\".txt\").name).resolve()\n",
    "\n",
    "        gt_boxes = np.empty((0, 4))\n",
    "        if lbl_path.exists() and lbl_path.stat().st_size:\n",
    "            h, w = cv2.imread(str(img_path)).shape[:2]\n",
    "            rows = np.loadtxt(lbl_path, ndmin=2, dtype=float)\n",
    "            gt_boxes = np.stack([xywhn_to_xyxy(r[1:5], w, h) for r in rows])\n",
    "        n_gt = len(gt_boxes)\n",
    "        total_gt += n_gt\n",
    "\n",
    "        # make separate 'seen' flags for every IoU threshold\n",
    "        gt_seen = [np.zeros(n_gt, dtype=bool) for _ in range(n_thr)]\n",
    "\n",
    "        pred = model.predict(str(img_path), imgsz=img_size,\n",
    "                             conf=conf_thr, verbose=False)[0]\n",
    "        # ── handle case: NO predictions ─────────────────────────────────\n",
    "        if pred.boxes is None or len(pred.boxes) == 0:\n",
    "            matched_here = 0\n",
    "            fp_local     = dup_local = 0\n",
    "            recall_img   = 0.0 if n_gt else 1.0\n",
    "            missed_local = n_gt - matched_here                          # ★ add FN\n",
    "            per_image_stats.append(\n",
    "                (str(img_path), recall_img, fp_local + dup_local + missed_local)\n",
    "            )\n",
    "            matched_gt  += matched_here\n",
    "            fp_total    += fp_local\n",
    "            dup_total   += dup_local\n",
    "            continue\n",
    "\n",
    "        boxes   = pred.boxes.xyxy.cpu().numpy()\n",
    "        scores  = pred.boxes.conf.cpu().numpy()\n",
    "        order   = scores.argsort()[::-1]          # high → low\n",
    "        boxes, scores = boxes[order], scores[order]\n",
    "        total_pred += len(boxes)\n",
    "\n",
    "        # IoU matrix once per image\n",
    "        ious_img = ( box_iou_matrix(boxes, gt_boxes) if n_gt else\n",
    "                     np.zeros((len(boxes), 0)) )\n",
    "\n",
    "        # ---- per prediction, update every threshold -------------------\n",
    "        for j, (bx, sc) in enumerate(zip(boxes, scores)):\n",
    "            ious_pred = ious_img[j] if n_gt else []\n",
    "\n",
    "            for t_idx, thr in enumerate(iou_thresholds):\n",
    "                is_tp = False\n",
    "                if n_gt:\n",
    "                    best = ious_pred.argmax()\n",
    "                    if ious_pred[best] >= thr and not gt_seen[t_idx][best]:\n",
    "                        is_tp = True\n",
    "                        gt_seen[t_idx][best] = True\n",
    "                scores_all[t_idx].append(sc)\n",
    "                tp_flags_all[t_idx].append(int(is_tp))\n",
    "\n",
    "        # counts for recall/precision columns (use iou_thr = 0.50 by default)\n",
    "        # counts for recall / FP / dup at reference IoU\n",
    "        # counts for recall / FP / dup at reference IoU\n",
    "        if n_gt:\n",
    "            ref_idx      = iou_thresholds.tolist().index(iou_thr)\n",
    "            matched_here = gt_seen[ref_idx].sum()\n",
    "            fp_local     = len(boxes) - matched_here\n",
    "            dup_local    = max(0, fp_local - (n_gt - matched_here))\n",
    "            recall_img   = matched_here / n_gt\n",
    "            missed_local = n_gt - matched_here                          # ★ add FN\n",
    "        else:\n",
    "            matched_here = 0\n",
    "            fp_local     = len(boxes)\n",
    "            dup_local    = 0\n",
    "            recall_img   = 1.0\n",
    "            missed_local = 0\n",
    "\n",
    "        # ★ NEW ➌  – store (path, recall, FP+dup) for this image\n",
    "        per_image_stats.append(\n",
    "            (str(img_path), recall_img, fp_local + dup_local + missed_local)\n",
    "        )\n",
    "\n",
    "        matched_gt  += matched_here\n",
    "        fp_total    += fp_local\n",
    "        dup_total   += dup_local\n",
    "\n",
    "    # ---------------------------------------------------------------- AP calc\n",
    "    def ap_from_lists(scores, tps, total_gt):\n",
    "        s = np.asarray(scores)\n",
    "        t = np.asarray(tps)\n",
    "        order = s.argsort()[::-1]\n",
    "        t = t[order]\n",
    "        fp = 1 - t\n",
    "        tp_cum = np.cumsum(t)\n",
    "        fp_cum = np.cumsum(fp)\n",
    "        rec = tp_cum / (total_gt + 1e-6)\n",
    "        prec = tp_cum / (tp_cum + fp_cum + 1e-6)\n",
    "        return np.trapz(prec, rec)                    # trapezoidal AP\n",
    "\n",
    "    ap_list = [ap_from_lists(sc, tp, total_gt) for sc, tp in\n",
    "               zip(scores_all, tp_flags_all)]\n",
    "    mAP50     = ap_list[0]\n",
    "    mAP50_95  = sum(ap_list) / n_thr\n",
    "\n",
    "    # ---------------------------------------------------------------- bundle\n",
    "    metrics = dict(\n",
    "        model       = Path(model_weight).name,\n",
    "        subset      = subset,\n",
    "        n_images    = len(img_paths),\n",
    "        n_gt        = total_gt,\n",
    "        n_pred      = total_pred,\n",
    "        matched_gt  = matched_gt,\n",
    "        false_pos   = fp_total,\n",
    "        duplicates  = dup_total,\n",
    "        recall      = 100.0 * matched_gt / total_gt if total_gt else 0.0,\n",
    "        precision   = 100.0 * matched_gt / total_pred if total_pred else 0.0,\n",
    "        mAP50       = mAP50   * 100.0,\n",
    "        mAP50_95    = mAP50_95 * 100.0,\n",
    "        per_image   = per_image_stats,        \n",
    "        data_yaml  = data_yaml,\n",
    "    )\n",
    "\n",
    "    import torch\n",
    "    if torch.backends.mps.is_available():\n",
    "        # Release GPU memory on macOS\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8fab043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_W   = \"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt\"     # your checkpoint\n",
    "#MODEL_W = \"/Users/tristan/Downloads/1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v04_trainingdata-1-mAP5095_0.61153-mAP50_0.97339.pt\"     \n",
    "DATA_YAML = \"/Users/tristan/trex/docs/notebooks/wolf-trial-pose-26/data.yaml\"  # your dataset yaml\n",
    "SAVE_DIR  = Path(\"val_hardness\")             # ← must match the folder\n",
    "IMG_SZ    = 2560                                   # same as your val size\n",
    "CONF_THR  = 0.25                                  # deployment conf\n",
    "TOP_PERC  = 1.0                                  # top‑10 % hardest\n",
    "IOU_THR   = 0.50\n",
    "\n",
    "def xywhn_to_xyxy(box, w, h):\n",
    "    xc, yc, bw, bh = box\n",
    "    return np.array([(xc - bw/2)*w,\n",
    "                     (yc - bh/2)*h,\n",
    "                     (xc + bw/2)*w,\n",
    "                     (yc + bh/2)*h])\n",
    "\n",
    "def box_iou_matrix(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    boxes*: (N,4) or (M,4) arrays, xyxy in absolute pixels.\n",
    "    Returns IoU matrix shape (N, M).\n",
    "    \"\"\"\n",
    "    if boxes1.size == 0 or boxes2.size == 0:\n",
    "        return np.zeros((len(boxes1), len(boxes2)), dtype=float)\n",
    "    tl = np.maximum(boxes1[:, None, :2], boxes2[None, :, :2])  # top‑left\n",
    "    br = np.minimum(boxes1[:, None, 2:], boxes2[None, :, 2:])  # bottom‑right\n",
    "    wh = np.clip(br - tl, 0, None)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    a1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    a2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    return inter / (a1[:, None] + a2[None, :] - inter + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6776168e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cbaae1f8984ae894c5501588da945f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating train:   0%|          | 0/3154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_s   = evaluate_model_txt(\n",
    "    \"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt\", \n",
    "    DATA_YAML, \n",
    "    subset=\"train\", img_size=2560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6203c6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>n_images</th>\n",
       "      <th>n_gt</th>\n",
       "      <th>n_pred</th>\n",
       "      <th>matched_gt</th>\n",
       "      <th>false_pos</th>\n",
       "      <th>duplicates</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>mAP50</th>\n",
       "      <th>mAP50_95</th>\n",
       "      <th>per_image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>2969</td>\n",
       "      <td>2857</td>\n",
       "      <td>112</td>\n",
       "      <td>64</td>\n",
       "      <td>97.143829</td>\n",
       "      <td>96.227686</td>\n",
       "      <td>96.483682</td>\n",
       "      <td>65.448167</td>\n",
       "      <td>[(/Users/tristan/trex/docs/notebooks/wolf-tria...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   subset  n_images  n_gt  \\\n",
       "model                                                                       \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...    val       450  2941   \n",
       "\n",
       "                                                    n_pred  matched_gt  \\\n",
       "model                                                                    \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...    2969        2857   \n",
       "\n",
       "                                                    false_pos  duplicates  \\\n",
       "model                                                                       \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...        112          64   \n",
       "\n",
       "                                                       recall  precision  \\\n",
       "model                                                                      \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...  97.143829  96.227686   \n",
       "\n",
       "                                                        mAP50   mAP50_95  \\\n",
       "model                                                                      \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...  96.483682  65.448167   \n",
       "\n",
       "                                                                                            per_image  \n",
       "model                                                                                                  \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...  [(/Users/tristan/trex/docs/notebooks/wolf-tria...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame([metrics_s]).set_index(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "375f3c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f25629ca68d44a3afa5261f049a5227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_m   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v04_trainingdata-1-mAP5095_0.61153-mAP50_0.97339.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=1280)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e39c244e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c6ed1e8b2d4c95b22c5156fb249e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_2560m   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.50636-mAP50_0.96336.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=2560)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0819d147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8025dfda334eec995855d6b081db1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_mAP5095_067422mAP50_098795   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.67422-mAP50_0.98795.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=2560)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b46e41dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1307a16661548d48686c986af113155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_adwait   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/yolo11_wolf_V047.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2a6c762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6b734c09664745b95bda9a55bd482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_3008   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=3008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0d4560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608c7fbcd16843fa8d315148bbd8884e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_3008inter2   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.60856-mAP50_0.97049.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=3008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7076db09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a7a754fd704e058f280fce3552da76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_3008inter3   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt\"  , \"/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/data.yaml\",\n",
    "                 subset=\"val\",img_size=3008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88569437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dece1714d204d9bb0654fb753f67018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'model': '2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt',\n",
       " 'subset': 'val',\n",
       " 'n_images': 9,\n",
       " 'n_gt': 22,\n",
       " 'n_pred': 6,\n",
       " 'matched_gt': 5,\n",
       " 'false_pos': 1,\n",
       " 'duplicates': 0,\n",
       " 'recall': 22.727272727272727,\n",
       " 'precision': 83.33333333333333,\n",
       " 'mAP50': 16.325750061586035,\n",
       " 'mAP50_95': 7.708329087439621,\n",
       " 'per_image': [('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0369_MP4-100_jpg.rf.5725c8e40a61f4b2c52939905a2e5cb8.jpg',\n",
       "   0.0,\n",
       "   2),\n",
       "  ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0369_MP4-117_jpg.rf.748308a3ab79a92920ebfdcb6d2b139e.jpg',\n",
       "   0.0,\n",
       "   2),\n",
       "  ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0369_MP4-69_jpg.rf.9a655e5a41a544fb0b94e8932c6f7e6a.jpg',\n",
       "   0.0,\n",
       "   2),\n",
       "  ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0370_MP4-114_jpg.rf.b9c94cb68ba80def50714b96b413fe3c.jpg',\n",
       "   0.0,\n",
       "   2),\n",
       "  ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0370_MP4-192_jpg.rf.6cd2381146f8dd5052601cd8385ef147.jpg',\n",
       "   1.0,\n",
       "   0),\n",
       "  ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0370_MP4-40_jpg.rf.37d87c01d2d00144d03143b47c6d58ab.jpg',\n",
       "   0.0,\n",
       "   2),\n",
       "  ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0370_MP4-77_jpg.rf.83164fb91f498e9c41eea30da95a27c3.jpg',\n",
       "   0.0,\n",
       "   2),\n",
       "  ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0380_MP4-24_jpg.rf.27f349211443ca6edd70ccdde9de96bf.jpg',\n",
       "   0.42857142857142855,\n",
       "   5),\n",
       "  ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/youtube-42_jpg.rf.8d710a872eabc5b044d188a3feec255c.jpg',\n",
       "   0.0,\n",
       "   1)],\n",
       " 'data_yaml': '/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/data.yaml'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_s_extra   = evaluate_model_txt(\n",
    "    \"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt\", \n",
    "    \"/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/data.yaml\", \n",
    "    subset=\"val\", img_size=2560)\n",
    "\n",
    "metrics_s_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "437f8509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>n_images</th>\n",
       "      <th>n_gt</th>\n",
       "      <th>n_pred</th>\n",
       "      <th>matched_gt</th>\n",
       "      <th>false_pos</th>\n",
       "      <th>duplicates</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>mAP50</th>\n",
       "      <th>mAP50_95</th>\n",
       "      <th>per_image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>2969</td>\n",
       "      <td>2856</td>\n",
       "      <td>113</td>\n",
       "      <td>64</td>\n",
       "      <td>97.109827</td>\n",
       "      <td>96.194005</td>\n",
       "      <td>96.450279</td>\n",
       "      <td>65.278979</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v04_trainingdata-1-mAP5095_0.61153-mAP50_0.97339.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>2844</td>\n",
       "      <td>2618</td>\n",
       "      <td>226</td>\n",
       "      <td>93</td>\n",
       "      <td>89.017341</td>\n",
       "      <td>92.053446</td>\n",
       "      <td>84.279721</td>\n",
       "      <td>47.789746</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.50636-mAP50_0.96336.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3087</td>\n",
       "      <td>2808</td>\n",
       "      <td>279</td>\n",
       "      <td>166</td>\n",
       "      <td>95.477729</td>\n",
       "      <td>90.962099</td>\n",
       "      <td>92.340565</td>\n",
       "      <td>45.012930</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.67422-mAP50_0.98795.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3043</td>\n",
       "      <td>2864</td>\n",
       "      <td>179</td>\n",
       "      <td>126</td>\n",
       "      <td>97.381843</td>\n",
       "      <td>94.117647</td>\n",
       "      <td>96.250639</td>\n",
       "      <td>63.349018</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yolo11_wolf_V047.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3190</td>\n",
       "      <td>2767</td>\n",
       "      <td>423</td>\n",
       "      <td>292</td>\n",
       "      <td>94.083645</td>\n",
       "      <td>86.739812</td>\n",
       "      <td>88.269467</td>\n",
       "      <td>52.455937</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3261</td>\n",
       "      <td>2823</td>\n",
       "      <td>438</td>\n",
       "      <td>352</td>\n",
       "      <td>95.987759</td>\n",
       "      <td>86.568537</td>\n",
       "      <td>94.524380</td>\n",
       "      <td>55.249025</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.60856-mAP50_0.97049.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3261</td>\n",
       "      <td>2850</td>\n",
       "      <td>411</td>\n",
       "      <td>342</td>\n",
       "      <td>96.905814</td>\n",
       "      <td>87.396504</td>\n",
       "      <td>96.109604</td>\n",
       "      <td>57.837505</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3166</td>\n",
       "      <td>2850</td>\n",
       "      <td>316</td>\n",
       "      <td>251</td>\n",
       "      <td>96.905814</td>\n",
       "      <td>90.018951</td>\n",
       "      <td>96.011370</td>\n",
       "      <td>60.111465</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   subset  n_images  n_gt  \\\n",
       "model                                                                       \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...    val       450  2941   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...    val       450  2941   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...    val       450  2941   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...    val       450  2941   \n",
       "yolo11_wolf_V047.pt                                   val       450  2941   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...    val       450  2941   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...    val       450  2941   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...    val       450  2941   \n",
       "\n",
       "                                                    n_pred  matched_gt  \\\n",
       "model                                                                    \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...    2969        2856   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...    2844        2618   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...    3087        2808   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...    3043        2864   \n",
       "yolo11_wolf_V047.pt                                   3190        2767   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...    3261        2823   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...    3261        2850   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...    3166        2850   \n",
       "\n",
       "                                                    false_pos  duplicates  \\\n",
       "model                                                                       \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...        113          64   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...        226          93   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...        279         166   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...        179         126   \n",
       "yolo11_wolf_V047.pt                                       423         292   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...        438         352   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...        411         342   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...        316         251   \n",
       "\n",
       "                                                       recall  precision  \\\n",
       "model                                                                      \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...  97.109827  96.194005   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...  89.017341  92.053446   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...  95.477729  90.962099   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...  97.381843  94.117647   \n",
       "yolo11_wolf_V047.pt                                 94.083645  86.739812   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...  95.987759  86.568537   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...  96.905814  87.396504   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...  96.905814  90.018951   \n",
       "\n",
       "                                                        mAP50   mAP50_95  \\\n",
       "model                                                                      \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...  96.450279  65.278979   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...  84.279721  47.789746   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...  92.340565  45.012930   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...  96.250639  63.349018   \n",
       "yolo11_wolf_V047.pt                                 88.269467  52.455937   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...  94.524380  55.249025   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...  96.109604  57.837505   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...  96.011370  60.111465   \n",
       "\n",
       "                                                                                            per_image  \n",
       "model                                                                                                  \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "yolo11_wolf_V047.pt                                 [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame([metrics_s, metrics_m, metrics_2560m, \n",
    "              metrics_mAP5095_067422mAP50_098795, metrics_adwait, metrics_3008, metrics_3008inter2, metrics_3008inter3]).set_index(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df757fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': '3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt', 'subset': 'val', 'n_images': 9, 'n_gt': 22, 'n_pred': 21, 'matched_gt': 11, 'false_pos': 10, 'duplicates': 3, 'recall': 50.0, 'precision': 52.38095238095238, 'mAP50': 42.99946265009228, 'mAP50_95': 13.702003111251301, 'per_image': [('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0369_MP4-100_jpg.rf.5725c8e40a61f4b2c52939905a2e5cb8.jpg', 1.0, 2), ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0369_MP4-117_jpg.rf.748308a3ab79a92920ebfdcb6d2b139e.jpg', 1.0, 2), ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0369_MP4-69_jpg.rf.9a655e5a41a544fb0b94e8932c6f7e6a.jpg', 1.0, 0), ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0370_MP4-114_jpg.rf.b9c94cb68ba80def50714b96b413fe3c.jpg', 0.0, 3), ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0370_MP4-192_jpg.rf.6cd2381146f8dd5052601cd8385ef147.jpg', 1.0, 2), ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0370_MP4-40_jpg.rf.37d87c01d2d00144d03143b47c6d58ab.jpg', 0.0, 3), ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0370_MP4-77_jpg.rf.83164fb91f498e9c41eea30da95a27c3.jpg', 0.5, 1), ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/DJI_0380_MP4-24_jpg.rf.27f349211443ca6edd70ccdde9de96bf.jpg', 0.2857142857142857, 10), ('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/images/youtube-42_jpg.rf.8d710a872eabc5b044d188a3feec255c.jpg', 0.0, 1)], 'data_yaml': '/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/data.yaml'}\n",
      "/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/data.yaml\n",
      "val\n",
      "subset = valid/images\n",
      "Detected root /Users/tristan/Downloads/wolf trial pose.v4i.yolov8 and labels in /Users/tristan/Downloads/wolf trial pose.v4i.yolov8/valid/labels\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462fe2f1c1db46de925b0d578a72a9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "overlaying:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  9 overlays written to /Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay/10_DJI_0380_MP4-24_jpg.rf.27f349211443ca6edd70ccdde9de96bf.jpg'),\n",
       " PosixPath('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay/3_DJI_0370_MP4-114_jpg.rf.b9c94cb68ba80def50714b96b413fe3c.jpg'),\n",
       " PosixPath('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay/3_DJI_0370_MP4-40_jpg.rf.37d87c01d2d00144d03143b47c6d58ab.jpg'),\n",
       " PosixPath('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay/2_DJI_0369_MP4-100_jpg.rf.5725c8e40a61f4b2c52939905a2e5cb8.jpg'),\n",
       " PosixPath('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay/2_DJI_0369_MP4-117_jpg.rf.748308a3ab79a92920ebfdcb6d2b139e.jpg'),\n",
       " PosixPath('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay/2_DJI_0370_MP4-192_jpg.rf.6cd2381146f8dd5052601cd8385ef147.jpg'),\n",
       " PosixPath('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay/1_DJI_0370_MP4-77_jpg.rf.83164fb91f498e9c41eea30da95a27c3.jpg'),\n",
       " PosixPath('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay/1_youtube-42_jpg.rf.8d710a872eabc5b044d188a3feec255c.jpg'),\n",
       " PosixPath('/Users/tristan/Downloads/wolf trial pose.v4i.yolov8/eval/hardness_3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt/val_overlay/0_DJI_0369_MP4-69_jpg.rf.9a655e5a41a544fb0b94e8932c6f7e6a.jpg')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ⇢ add this helper just above visualise_hardest_samples_txt ────────────────\n",
    "def draw_filled_box_alpha(frame, box_xyxy, color_bgr, alpha=0.35):\n",
    "    \"\"\"\n",
    "    Draw a filled box with transparency on `frame`.\n",
    "    \"\"\"\n",
    "    overlay = frame.copy()\n",
    "    x1, y1, x2, y2 = map(int, box_xyxy)\n",
    "    cv2.rectangle(overlay, (x1, y1), (x2, y2), color_bgr, thickness=-1)\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1-alpha, 0, dst=frame)  # in‑place\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Part 2 – overlay GT + predictions for hardest frames\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "def visualise_hardest_samples_txt(\n",
    "    model_weight: str,\n",
    "    data_yaml: str,\n",
    "    save_dir: Path,\n",
    "    hard_samples: list,\n",
    "    img_size: int = 640,\n",
    "    conf_thr: float = 0.25,\n",
    "    display_inline: bool = True,\n",
    "    subset: str = \"val\",\n",
    "):\n",
    "    save_dir = Path(save_dir)\n",
    "    out_dir  = save_dir / Path(subset+\"_overlay\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # dataset root for resolving paths when val set is a relative list\n",
    "    meta     = yaml.safe_load(open(data_yaml))\n",
    "    data_root = Path(data_yaml).resolve().parent\n",
    "    label_root = (data_root / meta[subset] / \"..\" / \"labels\").resolve()\n",
    "    print(f\"subset = {meta[subset]}\")\n",
    "    print(f\"Detected root {data_root} and labels in {label_root}\")\n",
    "    dataset_path = Path(meta.get(\"path\", \"\")) if meta.get(\"path\") else Path(\".\")\n",
    "\n",
    "    # drawing colours\n",
    "    C_GT  = (80, 240, 120)   # green\n",
    "    C_PR  = (0,0,255)        # red\n",
    "\n",
    "    def draw_box(img, box, color, thickness=2):\n",
    "        x1,y1,x2,y2 = map(int, box)\n",
    "        cv2.rectangle(img, (x1,y1), (x2,y2), color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    model = YOLO(model_weight)\n",
    "    model.to('mps')\n",
    "    overlay_paths = []\n",
    "\n",
    "    for p_str, recall, mistakes in tqdm(hard_samples, desc=\"overlaying\"):\n",
    "        img_path = (dataset_path / p_str).resolve()\n",
    "        img      = cv2.imread(str(img_path))\n",
    "        h, w     = img.shape[:2]\n",
    "\n",
    "        # --- draw GT boxes ---------------------------------------------\n",
    "        lbl_path = os.path.basename(img_path.with_suffix(\".txt\"))\n",
    "        lbl_path = label_root / lbl_path\n",
    "        if lbl_path.exists() and not lbl_path.stat().st_size == 0:\n",
    "            rows = np.loadtxt(lbl_path, ndmin=2, dtype=float)\n",
    "            for row in rows:\n",
    "                box = xywhn_to_xyxy(row[1:5], w, h)\n",
    "                draw_filled_box_alpha(img, box, C_GT, alpha=0.25)\n",
    "                draw_box(img, box, C_GT, thickness=1)\n",
    "        else:\n",
    "            print(f\"Label file {lbl_path} not found.\")\n",
    "            continue\n",
    "\n",
    "        # --- draw predictions ------------------------------------------\n",
    "        preds = model.predict(str(img_path),\n",
    "                              imgsz=img_size,\n",
    "                              conf=conf_thr,\n",
    "                              verbose=False)[0]\n",
    "        for box in preds.boxes.xyxy.cpu().numpy():\n",
    "            draw_box(img, box, C_PR)\n",
    "\n",
    "        # annotate hardness\n",
    "        cv2.putText(img, f\"recall={recall:.2f}\" + f\"  FP+DUP={mistakes}\",\n",
    "                    (5,18), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.6, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        outfile = out_dir / (str(mistakes)+\"_\"+img_path.stem + \".jpg\")\n",
    "        cv2.imwrite(str(outfile), img)\n",
    "        overlay_paths.append(outfile)\n",
    "\n",
    "        if display_inline:\n",
    "            display(Image(filename=str(outfile)))\n",
    "\n",
    "    print(f\"✔  {len(overlay_paths)} overlays written to {out_dir.resolve()}\")\n",
    "    return overlay_paths\n",
    "\n",
    "DISPLAY   = False                                # show in notebook?\n",
    "\n",
    "metrics = metrics_3008inter3.copy()\n",
    "\n",
    "if not \"data_yaml\" in metrics:\n",
    "    metrics[\"data_yaml\"] = DATA_YAML\n",
    "print(metrics)\n",
    "\n",
    "# sort metrics[\"per_image\"] by mistakes\n",
    "metrics[\"per_image\"].sort(key=lambda x: -x[2])  # sort by FP+DUP\n",
    "# top 10 hardest\n",
    "hardest_samples = metrics[\"per_image\"].copy()\n",
    "\n",
    "if len(hardest_samples) > 100:\n",
    "    hardest_samples = hardest_samples[:int(len(metrics[\"per_image\"])*0.1)]\n",
    "\n",
    "print(metrics[\"data_yaml\"])\n",
    "print(metrics[\"subset\"])\n",
    "\n",
    "visualise_hardest_samples_txt(\n",
    "    model_weight=\"/Users/tristan/Downloads/\"+metrics['model'],\n",
    "    data_yaml=metrics[\"data_yaml\"],\n",
    "    save_dir=Path(metrics[\"data_yaml\"]).parent / Path(\"eval\") / Path(\"hardness_\"+metrics[\"model\"]),\n",
    "    hard_samples=hardest_samples,  # top 10 hardest\n",
    "    img_size=IMG_SZ,\n",
    "    conf_thr=CONF_THR,\n",
    "    display_inline=DISPLAY,\n",
    "    subset=metrics[\"subset\"],                            # \"train\" or \"val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bf01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"6AIYp8La155paoPM9zuI\")\n",
    "project = rf.workspace(\"buffalo\").project(\"wolf-trial-pose\")\n",
    "version = project.version(26)\n",
    "dataset = version.download(\"yolov8\")\n",
    "dataset.location\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e551589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics_3008inter3[\"per_image\"][:int(len(metrics_3008inter3[\"per_image\"])*0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data.converter import convert_coco\n",
    "\n",
    "# point this at your COCO-style JSON folder,\n",
    "# set use_keypoints=True to include the 17 COCO keypoints\n",
    "convert_coco(\n",
    "    labels_dir=\"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8 (2)/train/\",\n",
    "    save_dir=\"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8_format/train/\",\n",
    "    use_keypoints=True\n",
    ")\n",
    "\n",
    "convert_coco(\n",
    "    labels_dir=\"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8 (2)/valid/\",\n",
    "    save_dir=\"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8_format/valid/\",\n",
    "    use_keypoints=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d70c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
