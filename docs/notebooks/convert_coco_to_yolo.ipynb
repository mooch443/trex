{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d09038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  hardest_validation_samples.py\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from os import name\n",
    "from pathlib import Path\n",
    "import math, json, numpy as np\n",
    "from ultralytics import YOLO\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def hardest_validation_samples(\n",
    "    model_weight: str,\n",
    "    data_yaml: str,\n",
    "    save_dir: str = \"runs/val_hardness\",\n",
    "    conf_thr: float = 0.25,\n",
    "    top_percent: float = 0.10,\n",
    "    iou_type: str = \"keypoints\",        # \"keypoints\"  |  \"bbox\"\n",
    "    oks_iou: float = 0.50               # IoU / OKS threshold that counts as “found”\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_weight : str\n",
    "        Path to the trained *.pt* checkpoint.\n",
    "    data_yaml : str\n",
    "        Your dataset YAML (must list train/val paths and key-point metadata).\n",
    "    save_dir : str, optional\n",
    "        Where intermediate JSON/TXT files will be written.\n",
    "    conf_thr : float, optional\n",
    "        Minimum confidence a prediction must have to be kept during validation.\n",
    "    top_percent : float, optional\n",
    "        Fraction of validation images to return as the “hardest” subset.\n",
    "    iou_type : {\"keypoints\",\"bbox\"}, optional\n",
    "        Metric family to evaluate with COCOeval.\n",
    "    oks_iou : float, optional\n",
    "        IoU/OKS level that defines a *successful* detection.\n",
    "    ---------------------------------------------------------------------------\n",
    "    Returns\n",
    "    -------\n",
    "    hard_samples : list[(file_name:str, score:float)]\n",
    "        Sorted hardest→easiest images and their AP/OKS scores.\n",
    "    detected_percent : float\n",
    "        Percentage of ground-truth objects that were correctly detected\n",
    "        (IoU/OKS ≥ `oks_iou` and confidence ≥ `conf_thr`).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Ultralytics validation (writes predictions.json & labels.json)\n",
    "    model = YOLO(model_weight)\n",
    "    model.to('mps')\n",
    "\n",
    "    if Path(save_dir+\"/val\").exists():\n",
    "        print(f\"Deleting previous validation results in {save_dir}/val\")\n",
    "        for f in Path(save_dir+\"/val\").glob(\"*\"):\n",
    "            f.unlink()\n",
    "        Path(save_dir+\"/val\").rmdir()\n",
    "    \n",
    "    val_res = model.val(data=data_yaml,\n",
    "                        save_json=True,\n",
    "                        save_txt=False,\n",
    "                        save_conf=True,\n",
    "                        conf=conf_thr,\n",
    "                        project=save_dir)\n",
    "\n",
    "    work = Path(val_res.save_dir)                 # ← folder created by YOLO\n",
    "    preds_json = work / \"predictions.json\"\n",
    "    gts_json   = work / \"labels.json\"             # auto-generated GT in COCO format\n",
    "\n",
    "    # 2) COCO-style evaluation, but we keep every per-image record\n",
    "    coco_gt = COCO(str(gts_json))\n",
    "    coco_dt = coco_gt.loadRes(str(preds_json))\n",
    "    ev      = COCOeval(coco_gt, coco_dt, iouType=iou_type)\n",
    "    ev.params.useCats = 1\n",
    "    ev.evaluate(); ev.accumulate()\n",
    "\n",
    "    thr_idx       = list(ev.params.iouThrs).index(oks_iou)\n",
    "    img_scores    = {}\n",
    "    gt_detected   = 0\n",
    "\n",
    "    for rec in tqdm(ev.evalImgs):                       # one dict per image×category\n",
    "        if rec is None:\n",
    "            continue\n",
    "        img_id      = rec[\"image_id\"]\n",
    "        # precision dims: [T×R×K×A×M]\n",
    "        precisions  = rec[\"precision\"][thr_idx, :, :, 0, 0]\n",
    "        score       = np.nanmean(precisions)      # mean over recalls & classes\n",
    "        img_scores.setdefault(img_id, []).append(score)\n",
    "\n",
    "        # how many GTs matched a prediction at this IoU/OKS?\n",
    "        matched     = (rec[\"matches\"][thr_idx] > 0).sum()\n",
    "        gt_detected += matched\n",
    "\n",
    "    # mean over classes for each image\n",
    "    img_scores = {k: float(np.nanmean(v)) for k, v in img_scores.items()}\n",
    "\n",
    "    # 3) hardest N %\n",
    "    n_imgs      = len(img_scores)\n",
    "    n_hard      = max(1, math.ceil(n_imgs * top_percent))\n",
    "    worst_first = sorted(img_scores.items(), key=lambda kv: kv[1])[:n_hard]\n",
    "    hard_samples = [(coco_gt.imgs[i][\"file_name\"], s) for i, s in worst_first]\n",
    "\n",
    "    # 4) overall detection percentage\n",
    "    n_gt             = sum(len(v) for v in coco_gt.imgToAnns.values())\n",
    "    detected_percent = 100.0 * gt_detected / n_gt if n_gt else 0.0\n",
    "\n",
    "    return hard_samples, detected_percent\n",
    "\n",
    "\n",
    "# ───────────────────────────── example usage ────────────────────────────────\n",
    "'''hardest, pct = hardest_validation_samples(\n",
    "    model_weight=\"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt\",\n",
    "    data_yaml=\"/Users/tristan/Downloads/yolo11_v04_trainingdata/data.yaml\",\n",
    "    conf_thr=0.25,           # same threshold you use in production\n",
    "    top_percent=0.10,\n",
    ")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Hard‑sample mining WITHOUT labels.json\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "from pathlib import Path\n",
    "import yaml, cv2, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Image\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import csv                      # ✱ FP/DUP\n",
    "from collections import Counter # ✱ FP/DUP\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  CONFIG  – EDIT THESE\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "MODEL_W   = \"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt\"     # your checkpoint\n",
    "#MODEL_W = \"/Users/tristan/Downloads/1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v04_trainingdata-1-mAP5095_0.61153-mAP50_0.97339.pt\"     \n",
    "DATA_YAML = \"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8_format/data.yaml\"  # your dataset yaml\n",
    "SAVE_DIR  = Path(\"val_hardness\")             # ← must match the folder\n",
    "IMG_SZ    = 2560                                   # same as your val size\n",
    "CONF_THR  = 0.25                                  # deployment conf\n",
    "TOP_PERC  = 1.0                                  # top‑10 % hardest\n",
    "IOU_THR   = 0.50\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  helper: IoU for two XYXY boxes\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "def xywhn_to_xyxy(box, w, h):\n",
    "    xc, yc, bw, bh = box\n",
    "    return np.array([(xc - bw/2)*w,\n",
    "                     (yc - bh/2)*h,\n",
    "                     (xc + bw/2)*w,\n",
    "                     (yc + bh/2)*h])\n",
    "\n",
    "def box_iou_matrix(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    boxes*: (N,4) or (M,4) arrays, xyxy in absolute pixels.\n",
    "    Returns IoU matrix shape (N, M).\n",
    "    \"\"\"\n",
    "    if boxes1.size == 0 or boxes2.size == 0:\n",
    "        return np.zeros((len(boxes1), len(boxes2)), dtype=float)\n",
    "    tl = np.maximum(boxes1[:, None, :2], boxes2[None, :, :2])  # top‑left\n",
    "    br = np.minimum(boxes1[:, None, 2:], boxes2[None, :, 2:])  # bottom‑right\n",
    "    wh = np.clip(br - tl, 0, None)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    a1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    a2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    return inter / (a1[:, None] + a2[None, :] - inter + 1e-6)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Part 1 – find hardest validation frames\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# … all previous imports & helpers stay exactly the same …\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Part 1 – find hardest frames  (now ranked by two metrics)\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "def hardest_validation_samples_txt(\n",
    "    model_weight: str,\n",
    "    data_yaml: str,\n",
    "    conf_thr: float = 0.25,\n",
    "    iou_thr: float = 0.50,\n",
    "    top_percent: float = 0.10,\n",
    "    img_size: int = 640,\n",
    "    subset: str = \"val\",\n",
    "):\n",
    "    meta      = yaml.safe_load(open(data_yaml))\n",
    "    data_root = Path(data_yaml).resolve().parent\n",
    "    val_root  = (data_root / meta[subset]).resolve()\n",
    "    is_txt_listing = val_root.suffix == \".txt\"\n",
    "\n",
    "    if is_txt_listing:\n",
    "        with open(val_root) as f:\n",
    "            img_paths = [Path(p.strip()) for p in f if p.strip()]\n",
    "    else:\n",
    "        img_paths = sorted([p for p in val_root.rglob(\"*\")\n",
    "                            if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}])\n",
    "\n",
    "    model = YOLO(model_weight).to(\"mps\")\n",
    "\n",
    "    per_image_stats = []      # ★ MULTI‑METRIC  (store all we need in 1 tuple)\n",
    "    csv_rows        = []\n",
    "    total_gt = total_found = fp_total = dup_total = 0\n",
    "\n",
    "    for p in tqdm(img_paths, desc=\"validating\"):\n",
    "        lbl_path = Path(f\"{data_root}/{meta[subset]}/../labels\") / p.with_suffix(\".txt\").name\n",
    "        gt_boxes = np.empty((0,4))\n",
    "        if lbl_path.exists() and lbl_path.stat().st_size:\n",
    "            h, w = cv2.imread(str(p)).shape[:2]\n",
    "            rows = np.loadtxt(lbl_path, ndmin=2, dtype=float)\n",
    "            gt_boxes = np.stack([xywhn_to_xyxy(r[1:5], w, h) for r in rows])\n",
    "        n_gt = len(gt_boxes)\n",
    "\n",
    "        pred = model.predict(str(p), imgsz=img_size, conf=conf_thr,\n",
    "                             verbose=False)[0]\n",
    "        pred_boxes = pred.boxes.xyxy.cpu().numpy() if pred.boxes else np.empty((0,4))\n",
    "        n_pred = len(pred_boxes)\n",
    "\n",
    "        false_pos = duplicates = found = 0\n",
    "        if n_pred:\n",
    "            if n_gt:\n",
    "                ious = box_iou_matrix(gt_boxes, pred_boxes)\n",
    "                best_gt   = ious.argmax(0)\n",
    "                best_iou  = ious[best_gt, range(n_pred)]\n",
    "                assigned  = np.where(best_iou >= iou_thr, best_gt, -1)\n",
    "                ctr       = Counter(assigned[assigned >= 0])\n",
    "                found      = len(ctr)\n",
    "                duplicates = sum(c-1 for c in ctr.values())\n",
    "                false_pos  = (assigned == -1).sum()\n",
    "            else:\n",
    "                false_pos = n_pred\n",
    "\n",
    "        recall = found / n_gt if n_gt else 1.0\n",
    "\n",
    "        # ★ MULTI‑METRIC — keep both recall & fp+dup\n",
    "        per_image_stats.append((str(p), recall, false_pos + duplicates))\n",
    "\n",
    "        csv_rows.append([str(p), n_gt, n_pred, found,\n",
    "                         false_pos, duplicates, recall])\n",
    "\n",
    "        total_gt    += n_gt\n",
    "        total_found += found\n",
    "        fp_total    += false_pos\n",
    "        dup_total   += duplicates\n",
    "\n",
    "    # ★ MULTI‑METRIC — sort by tuple:  (low recall  →  high fp+dup)\n",
    "    n_hard = max(1, int(len(per_image_stats) * top_percent))\n",
    "    hard_sorted = sorted(\n",
    "        per_image_stats,\n",
    "        key=lambda t: (-t[2], t[1])   # (recall ↑ , fp+dup ↓)  so we invert fp\n",
    "    )[:n_hard]\n",
    "\n",
    "    overall_recall = 100.0 * total_found / total_gt if total_gt else 0.0\n",
    "\n",
    "    # … CSV / TXT saving section stays unchanged …\n",
    "    # (use csv_rows as before)\n",
    "\n",
    "    return hard_sorted, overall_recall\n",
    "\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Run the pipeline\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "hardest, pct = hardest_validation_samples_txt(\n",
    "    model_weight=MODEL_W,\n",
    "    data_yaml=DATA_YAML,\n",
    "    conf_thr=CONF_THR,\n",
    "    iou_thr=IOU_THR,\n",
    "    top_percent=TOP_PERC,\n",
    "    img_size=IMG_SZ,\n",
    "    subset=\"train\",\n",
    ")\n",
    "print(f\"\\n📊  overall recall (IoU≥{IOU_THR}, conf≥{CONF_THR}) = {pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b102f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml, cv2, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Image\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import csv                      # ✱ FP/DUP\n",
    "from collections import Counter # ✱ FP/DUP\n",
    "\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  evaluate_model_txt  – now with mAP50‑95\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "def evaluate_model_txt(\n",
    "    model_weight: str,\n",
    "    data_yaml: str,\n",
    "    subset: str = \"val\",\n",
    "    conf_thr: float = 0.25,\n",
    "    iou_thr: float = 0.50,   # still used for recall / precision columns\n",
    "    img_size: int = 640,\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyse *every* image in `subset` (train/val) and return:\n",
    "\n",
    "        { model, subset, n_images, n_gt, n_pred,\n",
    "          matched_gt, false_pos, duplicates,\n",
    "          recall, precision,\n",
    "          mAP50, mAP50_95 }\n",
    "\n",
    "    * mAP50 is AP at IoU ≥ `iou_thr` (default 0.50)\n",
    "    * mAP50_95 is the mean AP over IoU = 0.50 … 0.95 (step 0.05)\n",
    "      – 10 thresholds, class‑agnostic.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------ setup\n",
    "    meta      = yaml.safe_load(open(data_yaml))\n",
    "    data_root = Path(data_yaml).resolve().parent\n",
    "    split_dir = (data_root / meta[subset]).resolve()\n",
    "\n",
    "    img_paths = ( [Path(p.strip()) for p in open(split_dir) if p.strip()]\n",
    "                  if split_dir.suffix == \".txt\"\n",
    "                  else sorted([p for p in split_dir.rglob(\"*\")\n",
    "                               if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}]) )\n",
    "\n",
    "    model = YOLO(model_weight).to(\"mps\")\n",
    "\n",
    "    # running tallies --------------------------------------------------------\n",
    "    total_gt = total_pred = matched_gt = fp_total = dup_total = 0\n",
    "\n",
    "    # AP data  ---------------------------------------------------------------\n",
    "    iou_thresholds = np.arange(0.50, 0.96, 0.05)         # 0.50 … 0.95\n",
    "    n_thr          = len(iou_thresholds)\n",
    "    scores_all     = [[] for _ in range(n_thr)]          # per‑thr score list\n",
    "    tp_flags_all   = [[] for _ in range(n_thr)]          # per‑thr TP mask\n",
    "\n",
    "\n",
    "    per_image_stats = []  \n",
    "\n",
    "    # ---------------------------------------------------------------- loop over images\n",
    "    for img_path in tqdm(img_paths, desc=f\"evaluating {subset}\"):\n",
    "        lbl_path = (data_root / meta[subset] / \"../labels\" /\n",
    "                    img_path.with_suffix(\".txt\").name).resolve()\n",
    "\n",
    "        gt_boxes = np.empty((0, 4))\n",
    "        if lbl_path.exists() and lbl_path.stat().st_size:\n",
    "            h, w = cv2.imread(str(img_path)).shape[:2]\n",
    "            rows = np.loadtxt(lbl_path, ndmin=2, dtype=float)\n",
    "            gt_boxes = np.stack([xywhn_to_xyxy(r[1:5], w, h) for r in rows])\n",
    "        n_gt = len(gt_boxes)\n",
    "        total_gt += n_gt\n",
    "\n",
    "        # make separate 'seen' flags for every IoU threshold\n",
    "        gt_seen = [np.zeros(n_gt, dtype=bool) for _ in range(n_thr)]\n",
    "\n",
    "        pred = model.predict(str(img_path), imgsz=img_size,\n",
    "                             conf=conf_thr, verbose=False)[0]\n",
    "        # ── handle case: NO predictions ─────────────────────────────────\n",
    "        if pred.boxes is None or len(pred.boxes) == 0:\n",
    "            matched_here = 0\n",
    "            fp_local     = dup_local = 0\n",
    "            recall_img   = 0.0 if n_gt else 1.0\n",
    "            missed_local = n_gt - matched_here                          # ★ add FN\n",
    "            per_image_stats.append(\n",
    "                (str(img_path), recall_img, fp_local + dup_local + missed_local)\n",
    "            )\n",
    "            matched_gt  += matched_here\n",
    "            fp_total    += fp_local\n",
    "            dup_total   += dup_local\n",
    "            continue\n",
    "\n",
    "        boxes   = pred.boxes.xyxy.cpu().numpy()\n",
    "        scores  = pred.boxes.conf.cpu().numpy()\n",
    "        order   = scores.argsort()[::-1]          # high → low\n",
    "        boxes, scores = boxes[order], scores[order]\n",
    "        total_pred += len(boxes)\n",
    "\n",
    "        # IoU matrix once per image\n",
    "        ious_img = ( box_iou_matrix(boxes, gt_boxes) if n_gt else\n",
    "                     np.zeros((len(boxes), 0)) )\n",
    "\n",
    "        # ---- per prediction, update every threshold -------------------\n",
    "        for j, (bx, sc) in enumerate(zip(boxes, scores)):\n",
    "            ious_pred = ious_img[j] if n_gt else []\n",
    "\n",
    "            for t_idx, thr in enumerate(iou_thresholds):\n",
    "                is_tp = False\n",
    "                if n_gt:\n",
    "                    best = ious_pred.argmax()\n",
    "                    if ious_pred[best] >= thr and not gt_seen[t_idx][best]:\n",
    "                        is_tp = True\n",
    "                        gt_seen[t_idx][best] = True\n",
    "                scores_all[t_idx].append(sc)\n",
    "                tp_flags_all[t_idx].append(int(is_tp))\n",
    "\n",
    "        # counts for recall/precision columns (use iou_thr = 0.50 by default)\n",
    "        # counts for recall / FP / dup at reference IoU\n",
    "        # counts for recall / FP / dup at reference IoU\n",
    "        if n_gt:\n",
    "            ref_idx      = iou_thresholds.tolist().index(iou_thr)\n",
    "            matched_here = gt_seen[ref_idx].sum()\n",
    "            fp_local     = len(boxes) - matched_here\n",
    "            dup_local    = max(0, fp_local - (n_gt - matched_here))\n",
    "            recall_img   = matched_here / n_gt\n",
    "            missed_local = n_gt - matched_here                          # ★ add FN\n",
    "        else:\n",
    "            matched_here = 0\n",
    "            fp_local     = len(boxes)\n",
    "            dup_local    = 0\n",
    "            recall_img   = 1.0\n",
    "            missed_local = 0\n",
    "\n",
    "        # ★ NEW ➌  – store (path, recall, FP+dup) for this image\n",
    "        per_image_stats.append(\n",
    "            (str(img_path), recall_img, fp_local + dup_local + missed_local)\n",
    "        )\n",
    "\n",
    "        matched_gt  += matched_here\n",
    "        fp_total    += fp_local\n",
    "        dup_total   += dup_local\n",
    "\n",
    "    # ---------------------------------------------------------------- AP calc\n",
    "    def ap_from_lists(scores, tps, total_gt):\n",
    "        s = np.asarray(scores)\n",
    "        t = np.asarray(tps)\n",
    "        order = s.argsort()[::-1]\n",
    "        t = t[order]\n",
    "        fp = 1 - t\n",
    "        tp_cum = np.cumsum(t)\n",
    "        fp_cum = np.cumsum(fp)\n",
    "        rec = tp_cum / (total_gt + 1e-6)\n",
    "        prec = tp_cum / (tp_cum + fp_cum + 1e-6)\n",
    "        return np.trapz(prec, rec)                    # trapezoidal AP\n",
    "\n",
    "    ap_list = [ap_from_lists(sc, tp, total_gt) for sc, tp in\n",
    "               zip(scores_all, tp_flags_all)]\n",
    "    mAP50     = ap_list[0]\n",
    "    mAP50_95  = sum(ap_list) / n_thr\n",
    "\n",
    "    # ---------------------------------------------------------------- bundle\n",
    "    metrics = dict(\n",
    "        model       = Path(model_weight).name,\n",
    "        subset      = subset,\n",
    "        n_images    = len(img_paths),\n",
    "        n_gt        = total_gt,\n",
    "        n_pred      = total_pred,\n",
    "        matched_gt  = matched_gt,\n",
    "        false_pos   = fp_total,\n",
    "        duplicates  = dup_total,\n",
    "        recall      = 100.0 * matched_gt / total_gt if total_gt else 0.0,\n",
    "        precision   = 100.0 * matched_gt / total_pred if total_pred else 0.0,\n",
    "        mAP50       = mAP50   * 100.0,\n",
    "        mAP50_95    = mAP50_95 * 100.0,\n",
    "        per_image   = per_image_stats,          # (optional) fill if you still need it\n",
    "    )\n",
    "\n",
    "    import torch\n",
    "    if torch.backends.mps.is_available():\n",
    "        # Release GPU memory on macOS\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8fab043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_W   = \"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt\"     # your checkpoint\n",
    "#MODEL_W = \"/Users/tristan/Downloads/1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v04_trainingdata-1-mAP5095_0.61153-mAP50_0.97339.pt\"     \n",
    "DATA_YAML = \"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8_format/data.yaml\"  # your dataset yaml\n",
    "SAVE_DIR  = Path(\"val_hardness\")             # ← must match the folder\n",
    "IMG_SZ    = 2560                                   # same as your val size\n",
    "CONF_THR  = 0.25                                  # deployment conf\n",
    "TOP_PERC  = 1.0                                  # top‑10 % hardest\n",
    "IOU_THR   = 0.50\n",
    "\n",
    "def xywhn_to_xyxy(box, w, h):\n",
    "    xc, yc, bw, bh = box\n",
    "    return np.array([(xc - bw/2)*w,\n",
    "                     (yc - bh/2)*h,\n",
    "                     (xc + bw/2)*w,\n",
    "                     (yc + bh/2)*h])\n",
    "\n",
    "def box_iou_matrix(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    boxes*: (N,4) or (M,4) arrays, xyxy in absolute pixels.\n",
    "    Returns IoU matrix shape (N, M).\n",
    "    \"\"\"\n",
    "    if boxes1.size == 0 or boxes2.size == 0:\n",
    "        return np.zeros((len(boxes1), len(boxes2)), dtype=float)\n",
    "    tl = np.maximum(boxes1[:, None, :2], boxes2[None, :, :2])  # top‑left\n",
    "    br = np.minimum(boxes1[:, None, 2:], boxes2[None, :, 2:])  # bottom‑right\n",
    "    wh = np.clip(br - tl, 0, None)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    a1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    a2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    return inter / (a1[:, None] + a2[None, :] - inter + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6776168e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e40338801045a3af71e43394bd981c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_s   = evaluate_model_txt(\n",
    "    \"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt\", \n",
    "    DATA_YAML, \n",
    "    subset=\"val\", img_size=2560)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "375f3c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f25629ca68d44a3afa5261f049a5227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_m   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v04_trainingdata-1-mAP5095_0.61153-mAP50_0.97339.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=1280)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e39c244e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c6ed1e8b2d4c95b22c5156fb249e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_2560m   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.50636-mAP50_0.96336.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=2560)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0819d147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8025dfda334eec995855d6b081db1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_mAP5095_067422mAP50_098795   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.67422-mAP50_0.98795.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=2560)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b46e41dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1307a16661548d48686c986af113155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_adwait   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/yolo11_wolf_V047.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2a6c762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6b734c09664745b95bda9a55bd482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_3008   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=3008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0d4560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608c7fbcd16843fa8d315148bbd8884e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_3008inter2   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.60856-mAP50_0.97049.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=3008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7076db09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674113a88b954ff7b3c1ca3086e74005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "evaluating val:   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics_3008inter3   = evaluate_model_txt(\n",
    "                 \"/Users/tristan/Downloads/3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt\"  , DATA_YAML,\n",
    "                 subset=\"val\",img_size=3008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88569437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "437f8509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>n_images</th>\n",
       "      <th>n_gt</th>\n",
       "      <th>n_pred</th>\n",
       "      <th>matched_gt</th>\n",
       "      <th>false_pos</th>\n",
       "      <th>duplicates</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>mAP50</th>\n",
       "      <th>mAP50_95</th>\n",
       "      <th>per_image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.69337-mAP50_0.98331.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>2969</td>\n",
       "      <td>2856</td>\n",
       "      <td>113</td>\n",
       "      <td>64</td>\n",
       "      <td>97.109827</td>\n",
       "      <td>96.194005</td>\n",
       "      <td>96.450279</td>\n",
       "      <td>65.278979</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v04_trainingdata-1-mAP5095_0.61153-mAP50_0.97339.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>2844</td>\n",
       "      <td>2618</td>\n",
       "      <td>226</td>\n",
       "      <td>93</td>\n",
       "      <td>89.017341</td>\n",
       "      <td>92.053446</td>\n",
       "      <td>84.279721</td>\n",
       "      <td>47.789746</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.50636-mAP50_0.96336.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3087</td>\n",
       "      <td>2808</td>\n",
       "      <td>279</td>\n",
       "      <td>166</td>\n",
       "      <td>95.477729</td>\n",
       "      <td>90.962099</td>\n",
       "      <td>92.340565</td>\n",
       "      <td>45.012930</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.67422-mAP50_0.98795.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3043</td>\n",
       "      <td>2864</td>\n",
       "      <td>179</td>\n",
       "      <td>126</td>\n",
       "      <td>97.381843</td>\n",
       "      <td>94.117647</td>\n",
       "      <td>96.250639</td>\n",
       "      <td>63.349018</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yolo11_wolf_V047.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3190</td>\n",
       "      <td>2767</td>\n",
       "      <td>423</td>\n",
       "      <td>292</td>\n",
       "      <td>94.083645</td>\n",
       "      <td>86.739812</td>\n",
       "      <td>88.269467</td>\n",
       "      <td>52.455937</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3261</td>\n",
       "      <td>2823</td>\n",
       "      <td>438</td>\n",
       "      <td>352</td>\n",
       "      <td>95.987759</td>\n",
       "      <td>86.568537</td>\n",
       "      <td>94.524380</td>\n",
       "      <td>55.249025</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.60856-mAP50_0.97049.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3261</td>\n",
       "      <td>2850</td>\n",
       "      <td>411</td>\n",
       "      <td>342</td>\n",
       "      <td>96.905814</td>\n",
       "      <td>87.396504</td>\n",
       "      <td>96.109604</td>\n",
       "      <td>57.837505</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.62367-mAP50_0.97582.pt</th>\n",
       "      <td>val</td>\n",
       "      <td>450</td>\n",
       "      <td>2941</td>\n",
       "      <td>3166</td>\n",
       "      <td>2850</td>\n",
       "      <td>316</td>\n",
       "      <td>251</td>\n",
       "      <td>96.905814</td>\n",
       "      <td>90.018951</td>\n",
       "      <td>96.011370</td>\n",
       "      <td>60.111465</td>\n",
       "      <td>[(/Users/tristan/Downloads/wolf_trial_pose_v4_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   subset  n_images  n_gt  \\\n",
       "model                                                                       \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...    val       450  2941   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...    val       450  2941   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...    val       450  2941   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...    val       450  2941   \n",
       "yolo11_wolf_V047.pt                                   val       450  2941   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...    val       450  2941   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...    val       450  2941   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...    val       450  2941   \n",
       "\n",
       "                                                    n_pred  matched_gt  \\\n",
       "model                                                                    \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...    2969        2856   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...    2844        2618   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...    3087        2808   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...    3043        2864   \n",
       "yolo11_wolf_V047.pt                                   3190        2767   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...    3261        2823   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...    3261        2850   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...    3166        2850   \n",
       "\n",
       "                                                    false_pos  duplicates  \\\n",
       "model                                                                       \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...        113          64   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...        226          93   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...        279         166   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...        179         126   \n",
       "yolo11_wolf_V047.pt                                       423         292   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...        438         352   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...        411         342   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...        316         251   \n",
       "\n",
       "                                                       recall  precision  \\\n",
       "model                                                                      \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...  97.109827  96.194005   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...  89.017341  92.053446   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...  95.477729  90.962099   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...  97.381843  94.117647   \n",
       "yolo11_wolf_V047.pt                                 94.083645  86.739812   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...  95.987759  86.568537   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...  96.905814  87.396504   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...  96.905814  90.018951   \n",
       "\n",
       "                                                        mAP50   mAP50_95  \\\n",
       "model                                                                      \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...  96.450279  65.278979   \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...  84.279721  47.789746   \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...  92.340565  45.012930   \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...  96.250639  63.349018   \n",
       "yolo11_wolf_V047.pt                                 88.269467  52.455937   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...  94.524380  55.249025   \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...  96.109604  57.837505   \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...  96.011370  60.111465   \n",
       "\n",
       "                                                                                            per_image  \n",
       "model                                                                                                  \n",
       "2560-yolo11s-pose-mosaic-2025-05-01-00_wolf_tri...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "1280-yolo11m-pose-mosaic-2025-04-29-11_yolo11_v...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "2560-yolo11m-pose-mosaic1-2025-05-03-13_wolf_tr...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "2560-yolo11s-pose-mosaic1-2025-05-02-23_wolf_tr...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "yolo11_wolf_V047.pt                                 [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "3008-yolo11n-pose-mosaic0.5-2025-05-05-07_wolf_...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  \n",
       "3008-yolo11n-pose-mosaic0.25-2025-05-05-16_wolf...  [(/Users/tristan/Downloads/wolf_trial_pose_v4_...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame([metrics_s, metrics_m, metrics_2560m, \n",
    "              metrics_mAP5095_067422mAP50_098795, metrics_adwait, metrics_3008, metrics_3008inter2, metrics_3008inter3]).set_index(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df757fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bc94287bd54d37af04b39ad0ea17d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "overlaying:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label file /Users/tristan/Downloads/wolf_trial_pose_v4_yolo8_format/valid/images/../labels/0509_113_PM3_Tract_AG_DJI_0630_NullFrame_png.rf.60f4d8b229c887734bffea9626a1a7ae.txt not found.\n",
      "✔  44 overlays written to /Users/tristan/trex/docs/notebooks/val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/4_0209_106_PM3_PGCP1_AD_DJI_0618_0209_106_PM3_PGCP1_AD_DJI_0618_P01_114_113513_png_jpg.rf.574675c5b6b4e640cc61b0b6906a0a47.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/12_0209_106_PM3_PGCP1_AD_DJI_0618_0209_106_PM3_PGCP1_AD_DJI_0618_P01_120_119779_png_jpg.rf.e7ec3606e1652774a28f188c1c7bdee9.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0209_106_PM3_PGCP1_AD_DJI_0618_0209_106_PM3_PGCP1_AD_DJI_0618_P01_13_12451_png_jpg.rf.e760e596217df95e945be209812b0f1c.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0209_106_PM3_PGCP1_AD_DJI_0618_0209_106_PM3_PGCP1_AD_DJI_0618_P01_15_14297_png_jpg.rf.7cb031701b65179914725c0e5be04cfb.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0209_106_PM3_PGCP1_AD_DJI_0618_0209_106_PM3_PGCP1_AD_DJI_0618_P01_16_15353_png_jpg.rf.f5ec9d59a8ad153370db216f0ac82d60.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/3_0209_106_PM3_PGCP1_AD_DJI_0618_0209_106_PM3_PGCP1_AD_DJI_0618_P01_49_48148_png_jpg.rf.e44d8bc67336519580d9b719d927d78c.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0209_106_PM3_PGCP1_AD_DJI_0618_0209_106_PM3_PGCP1_AD_DJI_0618_P01_4_3948_png_jpg.rf.00be71350f9ba477e0a5d0fd25a015a3.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_109_108873_png.rf.1419ad7577c53dcc6370a7811249da40.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_121_120108_png.rf.630996d14808fa9a65dde2c802ee943e.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_124_123377_png.rf.4ad502e09e33a8376e85d7c23fde8bde.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_132_131446_png.rf.c3b2b92fa89d062ad2dc2694f93b0cf3.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_137_136177_png.rf.2753ac5b3796ed738041e7a6e21badad.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_16_15904_png.rf.9621836134627bc67e8a294096fd043b.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_20_19120_png.rf.b0534413af29e18dfa379da244bbd191.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_28_27921_png.rf.1616a9c7c0e25b636b22d26e040b71d3.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_34_33188_png.rf.4bb164dd9551db3bbc580978693afe2f.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_47_46558_png.rf.d702a9c1e3addf5e2c89d113d66cd2c8.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_51_50755_png.rf.e382c392ee0d06056c65c1f414ec4acd.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_64_63945_png.rf.f89c189ba95b0e69e37ed8cc3a4c2510.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_69_68891_png.rf.843c871788402ae1ba4f7d54d0ebdd72.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_72_71333_png.rf.ad0cebc22dc8ceef28c594ee8de05ec7.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_79_78213_png.rf.978a7dcd3c0605b03eb59dc04525d733.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_85_84017_png.rf.056a77213209895ac14c156aabf4b5ef.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0629_0509_113_PM3_Tract_AG_DJI_0629_P01_90_89174_png.rf.d242a32f58bf8e4e6bf10db6dc8dd930.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0630_0509_113_PM3_Tract_AG_DJI_0630_P01_18_17427_png.rf.dd415d29131e69545cbfc4a54a7a909c.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0630_0509_113_PM3_Tract_AG_DJI_0630_P01_21_20761_png.rf.7881f072e41b461ed8412a354b25d445.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0630_0509_113_PM3_Tract_AG_DJI_0630_P01_23_22150_png.rf.29bf42c702e9e1f9ca3ffb3705800cd8.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0630_0509_113_PM3_Tract_AG_DJI_0630_P01_26_25470_png.rf.889566caef60ad2b79675dba127c5032.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0509_113_PM3_Tract_AG_DJI_0630_0509_113_PM3_Tract_AG_DJI_0630_P01_37_36280_png.rf.b464b6dde8754d9b2a943936fa15468d.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0630_0509_113_PM3_Tract_AG_DJI_0630_P01_44_43899_png.rf.5d7a322745ee60e48918a4bff546a733.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0630_0509_113_PM3_Tract_AG_DJI_0630_P01_62_61356_png.rf.95a07ba895bbfb2235785bd589d7e342.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0630_0509_113_PM3_Tract_AG_DJI_0630_P01_72_71690_png.rf.e71a6f66b2f0f31f028a08c046fffe10.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0632_0509_113_PM3_Tract_AG_DJI_0632_P01_10_9966_png.rf.16b6b6fb6f507b8af1f4e521d2965fd2.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0509_113_PM3_Tract_AG_DJI_0632_0509_113_PM3_Tract_AG_DJI_0632_P01_41_40899_png.rf.9bc9f2ff352a108cda20e45efe5947ca.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0709_122_PM3_GCP5_SH_DJI_0636_0709_122_PM3_GCP5_SH_DJI_0636_P01_114_113464_png_jpg.rf.f88b5d62b0feb983b9d4f545986e9bdc.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/4_0709_122_PM3_GCP5_SH_DJI_0636_0709_122_PM3_GCP5_SH_DJI_0636_P01_126_125666_png_jpg.rf.077a773d393801bdf99d0a4920fb9708.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0709_122_PM3_GCP5_SH_DJI_0636_0709_122_PM3_GCP5_SH_DJI_0636_P01_19_18431_png_jpg.rf.5a6b4c0128cfc57d4bb25afb86d8ada1.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0709_122_PM3_GCP5_SH_DJI_0636_0709_122_PM3_GCP5_SH_DJI_0636_P01_38_37815_png_jpg.rf.d6d12a74ac0c3e05cd17484f599adfbb.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/4_0709_122_PM3_GCP5_SH_DJI_0636_0709_122_PM3_GCP5_SH_DJI_0636_P01_39_38048_png_jpg.rf.50d5fe9df6d584f39ca54544e9c9005c.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/2_0709_122_PM3_GCP5_SH_DJI_0636_0709_122_PM3_GCP5_SH_DJI_0636_P01_57_56857_png_jpg.rf.1b3957573d9778a9c5ea407fa5683cd8.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0709_122_PM3_GCP5_SH_DJI_0638_0709_122_PM3_GCP5_SH_DJI_0638_P01_124_123428_png_jpg.rf.991c16f0a52e48ead57292909ae5fd66.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0709_122_PM3_GCP5_SH_DJI_0638_0709_122_PM3_GCP5_SH_DJI_0638_P01_125_124986_png_jpg.rf.7b871713e26edfec70f3012f388f7ab5.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/0_0709_122_PM3_GCP5_SH_DJI_0638_0709_122_PM3_GCP5_SH_DJI_0638_P01_141_140462_png_jpg.rf.ae40b9d9cedcfdb2f91d640e8c769802.jpg'),\n",
       " PosixPath('val_hardness_3008-yolo11n-pose-mosaic0.5-2025-05-04-14_wolf_trial_pose_v4_yolo8_format-1-mAP5095_0.58645-mAP50_0.96939.pt/val_overlay/4_0709_122_PM3_GCP5_SH_DJI_0638_0709_122_PM3_GCP5_SH_DJI_0638_P01_32_31618_png_jpg.rf.230c353ac9f8c6f12dd685bc6db4141f.jpg')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ⇢ add this helper just above visualise_hardest_samples_txt ────────────────\n",
    "def draw_filled_box_alpha(frame, box_xyxy, color_bgr, alpha=0.35):\n",
    "    \"\"\"\n",
    "    Draw a filled box with transparency on `frame`.\n",
    "    \"\"\"\n",
    "    overlay = frame.copy()\n",
    "    x1, y1, x2, y2 = map(int, box_xyxy)\n",
    "    cv2.rectangle(overlay, (x1, y1), (x2, y2), color_bgr, thickness=-1)\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1-alpha, 0, dst=frame)  # in‑place\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "#  Part 2 – overlay GT + predictions for hardest frames\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "def visualise_hardest_samples_txt(\n",
    "    model_weight: str,\n",
    "    data_yaml: str,\n",
    "    save_dir: Path,\n",
    "    hard_samples: list,\n",
    "    img_size: int = 640,\n",
    "    conf_thr: float = 0.25,\n",
    "    display_inline: bool = True,\n",
    "    subset: str = \"val\",\n",
    "):\n",
    "    save_dir = Path(save_dir)\n",
    "    out_dir  = save_dir / Path(subset+\"_overlay\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # dataset root for resolving paths when val set is a relative list\n",
    "    meta     = yaml.safe_load(open(data_yaml))\n",
    "    data_root = Path(data_yaml).resolve().parent\n",
    "    dataset_path = Path(meta.get(\"path\", \"\")) if meta.get(\"path\") else Path(\".\")\n",
    "\n",
    "    # drawing colours\n",
    "    C_GT  = (80, 240, 120)   # green\n",
    "    C_PR  = (0,0,255)        # red\n",
    "\n",
    "    def draw_box(img, box, color, thickness=2):\n",
    "        x1,y1,x2,y2 = map(int, box)\n",
    "        cv2.rectangle(img, (x1,y1), (x2,y2), color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    model = YOLO(model_weight)\n",
    "    model.to('mps')\n",
    "    overlay_paths = []\n",
    "\n",
    "    for p_str, recall, mistakes in tqdm(hard_samples, desc=\"overlaying\"):\n",
    "        img_path = (dataset_path / p_str).resolve()\n",
    "        img      = cv2.imread(str(img_path))\n",
    "        h, w     = img.shape[:2]\n",
    "\n",
    "        # --- draw GT boxes ---------------------------------------------\n",
    "        lbl_path = os.path.basename(img_path.with_suffix(\".txt\"))\n",
    "        lbl_path = Path(str(data_root)+\"/\"+meta[subset]+\"/../labels\") / lbl_path\n",
    "        if lbl_path.exists():\n",
    "            rows = np.loadtxt(lbl_path, ndmin=2, dtype=float)\n",
    "            for row in rows:\n",
    "                box = xywhn_to_xyxy(row[1:5], w, h)\n",
    "                draw_filled_box_alpha(img, box, C_GT, alpha=0.25)\n",
    "                draw_box(img, box, C_GT, thickness=1)\n",
    "        else:\n",
    "            print(f\"Label file {lbl_path} not found.\")\n",
    "            continue\n",
    "\n",
    "        # --- draw predictions ------------------------------------------\n",
    "        preds = model.predict(str(img_path),\n",
    "                              imgsz=img_size,\n",
    "                              conf=conf_thr,\n",
    "                              verbose=False)[0]\n",
    "        for box in preds.boxes.xyxy.cpu().numpy():\n",
    "            draw_box(img, box, C_PR)\n",
    "\n",
    "        # annotate hardness\n",
    "        cv2.putText(img, f\"recall={recall:.2f}\" + f\"  FP+DUP={mistakes}\",\n",
    "                    (5,18), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.6, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        outfile = out_dir / (str(mistakes)+\"_\"+img_path.stem + \".jpg\")\n",
    "        cv2.imwrite(str(outfile), img)\n",
    "        overlay_paths.append(outfile)\n",
    "\n",
    "        if display_inline:\n",
    "            display(Image(filename=str(outfile)))\n",
    "\n",
    "    print(f\"✔  {len(overlay_paths)} overlays written to {out_dir.resolve()}\")\n",
    "    return overlay_paths\n",
    "\n",
    "DISPLAY   = False                                # show in notebook?\n",
    "\n",
    "metrics = metrics_3008\n",
    "visualise_hardest_samples_txt(\n",
    "    model_weight=\"/Users/tristan/Downloads/\"+metrics['model'],\n",
    "    data_yaml=DATA_YAML,\n",
    "    save_dir=Path(\"val_hardness_\"+metrics[\"model\"]),\n",
    "    hard_samples=metrics[\"per_image\"][:int(len(metrics[\"per_image\"])*0.1)],  # top 10 hardest\n",
    "    img_size=IMG_SZ,\n",
    "    conf_thr=CONF_THR,\n",
    "    display_inline=DISPLAY,\n",
    "    subset=\"val\",                            # \"train\" or \"val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e551589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics_3008inter3[\"per_image\"][:int(len(metrics_3008inter3[\"per_image\"])*0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data.converter import convert_coco\n",
    "\n",
    "# point this at your COCO-style JSON folder,\n",
    "# set use_keypoints=True to include the 17 COCO keypoints\n",
    "convert_coco(\n",
    "    labels_dir=\"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8 (2)/train/\",\n",
    "    save_dir=\"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8_format/train/\",\n",
    "    use_keypoints=True\n",
    ")\n",
    "\n",
    "convert_coco(\n",
    "    labels_dir=\"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8 (2)/valid/\",\n",
    "    save_dir=\"/Users/tristan/Downloads/wolf_trial_pose_v4_yolo8_format/valid/\",\n",
    "    use_keypoints=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d70c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
